{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, torch\n",
    "from align_test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- Attn bias + residual ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "--- MLP ---\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "--- LM head ---\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "--- Final Norm ---\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "qProjSize = 64\n",
    "num_heads = 12\n",
    "num_tokens = 25\n",
    "for i in range(tot_num_layers):\n",
    "    hf_base = os.path.join(hf_path, f\"fwd_step_0_decoder.layers.{i}.\")\n",
    "    ff_base = os.path.join(ff_path, f\"fwd_step_0_layers_{i}_layers_{i}_\")\n",
    "    \n",
    "    # LayerNorm\n",
    "    hf_tensor = hf_base + \"self_attn_layer_norm.input_0\"\n",
    "    ff_tensor = ff_base + \"attention_layer_norm_shard_0_output_0\"\n",
    "    compare_tensors(hf_tensor, ff_tensor)\n",
    "    hf_tensor = hf_base + \"self_attn_layer_norm.output_0\"\n",
    "    ff_tensor = ff_base + \"attention_layer_norm_shard_0_output_1\"\n",
    "    compare_tensors(hf_tensor, ff_tensor)\n",
    "\n",
    "    # # Attention QKV proj\n",
    "    # print(\"---Attn---\")\n",
    "    # ff_tensor = ff_base + \"attention_shard_0_qkv_proj_output\"\n",
    "    # ff_tensor = load_ff_tensor(ff_tensor, [qProjSize, num_heads, 3, num_tokens])\n",
    "    # ff_q_proj = ff_tensor[:,:,0,:]\n",
    "    # ff_k_proj = ff_tensor[:,:,1,:]\n",
    "    # ff_v_proj = ff_tensor[:,:,2,:]\n",
    "    # hf_q_proj = hf_base + \"self_attn.q_proj.output_0\"\n",
    "    # hf_q_proj = load_hf_tensor(hf_q_proj).squeeze().T\n",
    "    # hf_q_proj = hf_q_proj.reshape(12,64,25)\n",
    "    # hf_q_proj = np.transpose(hf_q_proj, (1,0,2))\n",
    "    # hf_k_proj = hf_base + \"self_attn.k_proj.output_0\"\n",
    "    # hf_k_proj = load_hf_tensor(hf_k_proj).squeeze().T\n",
    "    # hf_k_proj = hf_k_proj.reshape(12,64,25)\n",
    "    # hf_k_proj = np.transpose(hf_k_proj, (1,0,2))\n",
    "    # hf_v_proj = hf_base + \"self_attn.v_proj.output_0\"\n",
    "    # hf_v_proj = load_hf_tensor(hf_v_proj).squeeze().T\n",
    "    # hf_v_proj = hf_v_proj.reshape(12,64,25)\n",
    "    # hf_v_proj = np.transpose(hf_v_proj, (1,0,2))\n",
    "    # compare_loaded_tensors(hf_q_proj/np.sqrt(qProjSize), ff_q_proj)\n",
    "    # compare_loaded_tensors(hf_k_proj, ff_k_proj)\n",
    "    # compare_loaded_tensors(hf_v_proj, ff_v_proj)\n",
    "\n",
    "    # Compare attn bias, residuals\n",
    "    print(\"--- Attn bias + residual ---\")\n",
    "    ff_residual1 = ff_path + f\"/fwd_step_0_layers_{i}_AddBiasResidualLayerNorm_shard_0_input_1\"\n",
    "    ff_residual2 = ff_base + \"attention_layer_norm_shard_0_output_0\"\n",
    "    compare_flexflow_tensors(ff_residual1, ff_residual2)\n",
    "    hf_tensor = hf_base + \"self_attn_layer_norm.input_0\"\n",
    "    compare_tensors(hf_tensor, ff_residual2)\n",
    "    ff_tensor = ff_path + f\"/fwd_step_0_layers_{i}_AddBiasResidualLayerNorm_shard_0_output_0\"\n",
    "    hf_tensor = hf_base + \"final_layer_norm.input_0\"\n",
    "    compare_tensors(hf_tensor, ff_tensor)\n",
    "    \n",
    "    print(\"--- MLP ---\")\n",
    "    hf_tensor = hf_base + \"fc1.input_0\"\n",
    "    ff_tensor = ff_base + \"fc1_shard_0_input_0\"\n",
    "    compare_tensors(hf_tensor, ff_tensor)\n",
    "    hf_tensor = hf_base + \"fc2.input_0\"\n",
    "    ff_tensor = ff_base + \"fc2_shard_0_input_0\"\n",
    "    compare_tensors(hf_tensor, ff_tensor)\n",
    "# LM head\n",
    "print(\"\\n--- LM head ---\")\n",
    "hf_tensor = hf_path + \"/fwd_step_0_base_model.model.lm_head.input_0\"\n",
    "ff_tensor = ff_path + \"/fwd_step_0_layers_11_embed_tokens_weight_lm_head_shard_0_input_0\"\n",
    "compare_tensors(hf_tensor, ff_tensor)\n",
    "hf_tensor = hf_path + \"/fwd_step_0_base_model.model.lm_head.output_0\"\n",
    "ff_tensor = ff_path + \"/fwd_step_0_layers_11_embed_tokens_weight_lm_head_shard_0_output_0\"\n",
    "compare_tensors(hf_tensor, ff_tensor)\n",
    "# Final layer norm\n",
    "print(\"\\n--- Final Norm ---\")\n",
    "hf_tensor = hf_path + \"/fwd_step_0_decoder.final_layer_norm.input_0\"\n",
    "ff_tensor = ff_path + \"/fwd_step_0_layers_11_final_layer_norm_shard_0_output_0\"\n",
    "compare_tensors(hf_tensor, ff_tensor)\n",
    "ff_tensor1 = ff_path + \"/fwd_step_0_layers_11_final_layer_norm_shard_0_input_activation\"\n",
    "# compare_flexflow_tensors_shortest(ff_tensor, ff_tensor1)\n",
    "hf_tensor = hf_path + \"/fwd_step_0_decoder.final_layer_norm.output_0\"\n",
    "ff_tensor = ff_path + \"/fwd_step_0_layers_11_final_layer_norm_shard_0_output_1\"\n",
    "compare_tensors(hf_tensor, ff_tensor)\n",
    "hf_tensor = hf_path + \"/fwd_step_0_decoder.final_layer_norm.saved_result_1\"\n",
    "ff_tensor = ff_path + \"/fwd_step_0_layers_11_final_layer_norm_shard_0_mean\"\n",
    "compare_tensors(hf_tensor, ff_tensor)\n",
    "hf_tensor = hf_path + \"/fwd_step_0_decoder.final_layer_norm.saved_result_2\"\n",
    "ff_tensor = ff_path + \"/fwd_step_0_layers_11_final_layer_norm_shard_0_rstd\"\n",
    "compare_tensors(hf_tensor, ff_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m compare_flexflow_tensors(ff_tensor, ff_tensor1)\n\u001b[1;32m     20\u001b[0m compare_tensors(hf_tensor, ff_tensor) \u001b[38;5;66;03m# fails\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compare fwd input/output of layernorm\u001b[39;00m\n\u001b[1;32m     25\u001b[0m hf_FWD_norm_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fwd_step_0_decoder.final_layer_norm.input_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compare backward pass\n",
    "hf_tensor = hf_path + \"/bwd_step_0_base_model.model.lm_head.go_0\"\n",
    "ff_tensor = ff_path + \"/bwd_step_0_layers_11_embed_tokens_weight_lm_head_shard_0_output_0\"\n",
    "compare_tensors(hf_tensor, ff_tensor, tolerance=1e-5)\n",
    "hf_tensor = hf_path + \"/bwd_step_0_base_model.model.lm_head.gi_0\"\n",
    "ff_tensor = ff_path + \"/bwd_step_0_layers_11_embed_tokens_weight_lm_head_shard_0_input_0\"\n",
    "compare_tensors(hf_tensor, ff_tensor, tolerance=1e-5)\n",
    "\n",
    "hf_tensor1 = hf_path + \"/bwd_step_0_decoder.final_layer_norm.go_0\"\n",
    "compare_hf_tensors(hf_tensor, hf_tensor1)\n",
    "ff_tensor = ff_path + \"/bwd_step_0_layers_11_final_layer_norm_shard_0_output_0\"\n",
    "compare_tensors(hf_tensor1, ff_tensor)\n",
    "\n",
    "hf_tensor = hf_path + \"/bwd_step_0_decoder.final_layer_norm.gi_0\"\n",
    "ff_tensor = ff_path + \"/bwd_step_0_layers_11_final_layer_norm_shard_0_input_0\"\n",
    "ff_tensor1 = ff_path + \"/bwd_step_0_layers_11_final_layer_norm_shard_0_input_1\"\n",
    "compare_flexflow_tensors(ff_tensor, ff_tensor1)\n",
    "compare_tensors(hf_tensor, ff_tensor) # fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_decoder.layers.0.fc1.input_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_decoding-step_0_layer-num_0_layer-name_layers_0_fc1_shard-id_0_input_0\n",
      "HF: [ 0.0193019  -1.0467215   0.21579844 ...  0.04534929 -0.25642633\n",
      "  0.10879952]\n",
      "FF:[ 0.01458706 -1.02212262  0.20589906 ...  0.04446212 -0.25625792\n",
      "  0.108039  ]\n",
      "[ True False  True ...  True  True  True]\n",
      "[    1     3     7 ... 19170 19174 19188]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m hf_fc1_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_decoder.layers.0.fc1.input_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m ff_fc1_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_decoding-step_0_layer-num_0_layer-name_layers_0_fc1_shard-id_0_input_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcompare_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_fc1_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_fc1_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# LORA input\u001b[39;00m\n\u001b[1;32m     20\u001b[0m hf_lora_A_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_weight_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fwd_step_0_layers.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mlp.down_proj.lora_A.default.input_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/FlexFlow/tests/peft/align_test_utils.py:32\u001b[0m, in \u001b[0;36mcompare_tensors\u001b[0;34m(hf_tensor_filepath, ff_tensor_filepath, tolerance)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mismatches)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#print(np.nonzero(hf_tensor)[0])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# print(ff_tensor[36], hf_tensor[36])\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(mismatches) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.05\u001b[39m\u001b[38;5;241m*\u001b[39mlen_hf_tensor)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOk!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "for layer_num in range(tot_num_layers):\n",
    "    hf_input_ln_out = f\"{hf_path}/fwd_step_0_decoder.layers.{layer_num}.self_attn_layer_norm.output_0\"\n",
    "    ff_input_ln_out = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_layer_norm_shard-id_0_output_1\"\n",
    "    compare_tensors(hf_input_ln_out, ff_input_ln_out)\n",
    "   \n",
    "    hf_ffn_norm_in = f\"{hf_path}/fwd_step_0_decoder.layers.{layer_num}.final_layer_norm.input_0\"\n",
    "    ff_ffn_norm_in = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_AddBiasResidualLayerNorm_shard-id_0_output_0\"\n",
    "    # compare_tensors(hf_ffn_norm_in, ff_ffn_norm_in)\n",
    "    \n",
    "    hf_ffn_norm_out = f\"{hf_path}/fwd_step_0_decoder.layers.{layer_num}.final_layer_norm.output_0\"\n",
    "    ff_ffn_norm_out = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_AddBiasResidualLayerNorm_shard-id_0_output_1\"\n",
    "    # compare_tensors(hf_ffn_norm_out, ff_ffn_norm_out)\n",
    "    hf_fc1_in = \"/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_decoder.layers.0.fc1.input_0\"\n",
    "    ff_fc1_in = \"/usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_decoding-step_0_layer-num_0_layer-name_layers_0_fc1_shard-id_0_input_0\"\n",
    "    compare_tensors(hf_fc1_in, ff_fc1_in)\n",
    "\n",
    "\n",
    "    # LORA input\n",
    "    hf_lora_A_in = f\"{hf_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.input_0\"\n",
    "    ff_lora_A_in = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_input_0\"\n",
    "    compare_hf_tensors(hf_down_proj_in, hf_lora_A_in)\n",
    "    compare_tensors(hf_lora_A_in, ff_lora_A_in)\n",
    "    # LORA weights\n",
    "    hf_lora_A_weight_fp = f\"{hf_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp)\n",
    "    hf_lora_B_weight_fp = f\"{hf_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp)\n",
    "    # LORA intermediate hf\n",
    "    hf_lora_A_out = f\"{hf_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.output_0\"\n",
    "    hf_lora_B_in = f\"{hf_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.input_0\"\n",
    "    compare_hf_tensors(hf_lora_A_out, hf_lora_B_in)\n",
    "    # LORA output\n",
    "    hf_lora_out = f\"{hf_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.output_0\"\n",
    "    ff_lora_out = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_output_0\"\n",
    "    # compare_tensors(hf_lora_out, ff_lora_out)\n",
    "    # compare_flexflow_tensors(ff_down_proj_out, ff_lora_out)\n",
    "    # compare_tensors(hf_down_proj_out, ff_lora_out)\n",
    "    compare_tensors_difference(hf_lora_out, ff_lora_out, ff_down_proj_out)\n",
    "    \n",
    "\n",
    "# After last layer only\n",
    "hf_norm_out = f\"{hf_path}/fwd_step_0_norm.output_0\"\n",
    "ff_norm_out = f\"{ff_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_output_1\"\n",
    "compare_tensors(hf_norm_out, ff_norm_out)\n",
    "hf_lm_head_out = f\"{hf_path}/fwd_step_0_base_model.model.lm_head.output_0\"\n",
    "ff_lm_head_out = f\"{ff_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_output_0\"\n",
    "compare_tensors(hf_lm_head_out, ff_lm_head_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_decoder.final_layer_norm.input_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_decoding-step_0_layer-num_11_layer-name_final_layer_norm_shard-id_0_output_0\n",
      "HF: [-0.00542103 -1.781267    0.16552497 ... -0.77217525 -0.5760026\n",
      "  0.04363118]\n",
      "FF:[ 0.03817766 -1.5644939   0.22477378 ... -0.94569921 -0.43960798\n",
      " -0.06447437]\n",
      "[False False False ... False False False]\n",
      "[    0     1     2 ... 19197 19198 19199]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m ff_FWD_norm_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mff_weight_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_0_decoding-step_0_layer-num_11_layer-name_final_layer_norm_shard-id_0_output_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m ff_FWD_norm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mff_weight_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_0_decoding-step_0_layer-num_11_layer-name_final_layer_norm_shard-id_0_output_1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mcompare_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_FWD_norm_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_FWD_norm_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m compare_tensors(hf_FWD_norm_out, ff_FWD_norm_out)\n\u001b[1;32m     25\u001b[0m hf_BWD_norm_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_weight_base_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bwd_step_0_decoder.final_layer_norm.gi_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/FlexFlow/tests/peft/align_test_utils.py:29\u001b[0m, in \u001b[0;36mcompare_tensors\u001b[0;34m(hf_tensor_filepath, ff_tensor_filepath, tolerance)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mismatches)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#print(np.nonzero(hf_tensor)[0])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# print(ff_tensor[36], hf_tensor[36])\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(mismatches) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.05\u001b[39m\u001b[38;5;241m*\u001b[39mlen_hf_tensor)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOk!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "\n",
    "ff_BWD_softmax_in = f\"{ff_path}/model_0_bwd-step_0_layer-num_100_layer-name_Softmax_shard-id_0_input_0\"\n",
    "\n",
    "hf_BWD_lm_head_out = f\"{hf_path}/bwd_step_0_base_model.model.lm_head.go_0\"\n",
    "ff_BWD_lm_head_out = f\"{ff_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_embed_tokens_weight_lm_head_shard-id_0_output_0\"\n",
    "compare_tensors(hf_BWD_lm_head_out, ff_BWD_lm_head_out, tolerance=1e-5)\n",
    "hf_BWD_lm_head_in = f\"{hf_path}/bwd_step_0_base_model.model.lm_head.gi_0\"\n",
    "ff_BWD_lm_head_in = f\"{ff_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_embed_tokens_weight_lm_head_shard-id_0_input_0\"\n",
    "compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in, tolerance=1e-5)\n",
    "\n",
    "hf_BWD_norm_out = f\"{hf_path}/bwd_step_0_decoder.final_layer_norm.go_0\"\n",
    "ff_BWD_norm_out = f\"{ff_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_final_layer_norm_shard-id_0_output_0\"\n",
    "compare_hf_tensors(hf_BWD_lm_head_in, hf_BWD_norm_out)\n",
    "compare_tensors(hf_BWD_norm_out, ff_BWD_norm_out)\n",
    "\n",
    "# Compare fwd input/output of layernorm\n",
    "hf_FWD_norm_in = f\"{hf_path}/fwd_step_0_decoder.final_layer_norm.input_0\"\n",
    "hf_FWD_norm_out = f\"{hf_path}/fwd_step_0_decoder.final_layer_norm.output_0\"\n",
    "ff_FWD_norm_in = f\"{ff_path}/model_0_decoding-step_0_layer-num_11_layer-name_final_layer_norm_shard-id_0_output_0\"\n",
    "ff_FWD_norm_out = f\"{ff_path}/model_0_decoding-step_0_layer-num_11_layer-name_final_layer_norm_shard-id_0_output_1\"\n",
    "compare_tensors(hf_FWD_norm_in, ff_FWD_norm_in)\n",
    "compare_tensors(hf_FWD_norm_out, ff_FWD_norm_out)\n",
    "\n",
    "hf_BWD_norm_in = f\"{hf_path}/bwd_step_0_decoder.final_layer_norm.gi_0\"\n",
    "ff_BWD_norm_in = f\"{ff_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_final_layer_norm_shard-id_0_input_1\"\n",
    "compare_tensors(hf_BWD_norm_in, ff_BWD_norm_in, tolerance=1e-5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
