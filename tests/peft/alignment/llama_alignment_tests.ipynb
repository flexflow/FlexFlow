{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, torch\n",
    "from align_test_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/FlexFlow/tests/peft/hf_peft_tensors /usr/FlexFlow/build/inference_tensors\n"
     ]
    }
   ],
   "source": [
    "print(hf_path, ff_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check weights (semi-automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "def convert_hf_filename_to_ff_filename(f, num_layers=12):\n",
    "    if f.endswith(\".lm_head.weight\"):\n",
    "        f_version = f\"fwd_step_0_layers_{num_layers-1}_lm_head_shard_0_weight_0\"\n",
    "    elif f == \"norm.weight\":\n",
    "        f_version = f\"fwd_step_0_layers_{num_layers-1}_norm_shard_0_weight_0\"\n",
    "    else:\n",
    "        f_version = \"fwd_step_0_\"\n",
    "        if f.startswith(\"layers.\"):\n",
    "            layernum = f.split(\"layers.\")[1].split(\".\")[0]\n",
    "            f_version += f\"layers_{layernum}_\"\n",
    "        f_version += f.split(\".weight\")[0].replace(\".base_layer\", \"\").replace(\".default\", \"\")\n",
    "        weight_index=\"0\"\n",
    "        if \"lora_A\" in f_version:\n",
    "            weight_index=\"A\"\n",
    "        elif \"lora_B\" in f_version:\n",
    "            weight_index=\"B\"\n",
    "        f_version = f_version.replace(\"lora_A\", \"lora\").replace(\"lora_B\", \"lora\")\n",
    "        f_version += f\"_shard_0_weight_{weight_index}\"\n",
    "    return f_version\n",
    "\n",
    "files_list = os.listdir(hf_path)\n",
    "num_layers=12\n",
    "for f in sorted(files_list):\n",
    "    if f.endswith(\".weight\"):\n",
    "        if \"self_attn\" in f:\n",
    "            continue\n",
    "        f_version = convert_hf_filename_to_ff_filename(f, num_layers=num_layers)\n",
    "        # print(f, f_version)\n",
    "        hf_w_path = os.path.join(hf_path, f)\n",
    "        ff_w_path = os.path.join(ff_path, f_version)\n",
    "        assert(os.path.isfile(hf_w_path))\n",
    "        assert(os.path.isfile(ff_w_path))\n",
    "        # print(\"\\t\", os.path.isfile(hf_w_path), os.path.isfile(ff_w_path))\n",
    "        # print(\"\\t\", ff_w_path)\n",
    "\n",
    "        # check equivalence\n",
    "        compare_tensors(hf_w_path, ff_w_path, tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model for automatic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "use_full_precision=True\n",
    "peft_model_id=\"goliaro/llama-160m-lora\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "if peft_config.peft_type != \"LORA\":\n",
    "    raise ValueError(f\"PEFT type {peft_config.peft_type} not supported yet\")\n",
    "\n",
    "peft_config.init_lora_weights = (\n",
    "    False\n",
    ")  # prevent HF from re-inizialing the weights randomly\n",
    "model_name = peft_config.base_model_name_or_path\n",
    "# Load base model, and apply the PEFT layer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32 if use_full_precision else torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, config=peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens True True\n",
      "layers.0.self_attn.q_proj True True\n",
      "layers.0.self_attn.k_proj True True\n",
      "layers.0.self_attn.v_proj True True\n",
      "layers.0.self_attn.o_proj True True\n",
      "layers.0.self_attn.rotary_emb True True\n",
      "layers.0.mlp.gate_proj True True\n",
      "layers.0.mlp.up_proj True True\n",
      "layers.0.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.0.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.0.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.0.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.0.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.0.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.0.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_0_layers.0.mlp.act_fn_shard_0_output_0\n",
      "layers.0.input_layernorm True True\n",
      "layers.0.post_attention_layernorm True True\n",
      "layers.1.self_attn.q_proj True True\n",
      "layers.1.self_attn.k_proj True True\n",
      "layers.1.self_attn.v_proj True True\n",
      "layers.1.self_attn.o_proj True True\n",
      "layers.1.self_attn.rotary_emb True True\n",
      "layers.1.mlp.gate_proj True True\n",
      "layers.1.mlp.up_proj True True\n",
      "layers.1.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.1.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.1.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.1.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.1.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.1.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.1.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_1_layers.1.mlp.act_fn_shard_0_output_0\n",
      "layers.1.input_layernorm True True\n",
      "layers.1.post_attention_layernorm True True\n",
      "layers.2.self_attn.q_proj True True\n",
      "layers.2.self_attn.k_proj True True\n",
      "layers.2.self_attn.v_proj True True\n",
      "layers.2.self_attn.o_proj True True\n",
      "layers.2.self_attn.rotary_emb True True\n",
      "layers.2.mlp.gate_proj True True\n",
      "layers.2.mlp.up_proj True True\n",
      "layers.2.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.2.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.2.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.2.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.2.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.2.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.2.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_2_layers.2.mlp.act_fn_shard_0_output_0\n",
      "layers.2.input_layernorm True True\n",
      "layers.2.post_attention_layernorm True True\n",
      "layers.3.self_attn.q_proj True True\n",
      "layers.3.self_attn.k_proj True True\n",
      "layers.3.self_attn.v_proj True True\n",
      "layers.3.self_attn.o_proj True True\n",
      "layers.3.self_attn.rotary_emb True True\n",
      "layers.3.mlp.gate_proj True True\n",
      "layers.3.mlp.up_proj True True\n",
      "layers.3.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.3.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.3.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.3.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.3.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.3.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.3.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_3_layers.3.mlp.act_fn_shard_0_output_0\n",
      "layers.3.input_layernorm True True\n",
      "layers.3.post_attention_layernorm True True\n",
      "layers.4.self_attn.q_proj True True\n",
      "layers.4.self_attn.k_proj True True\n",
      "layers.4.self_attn.v_proj True True\n",
      "layers.4.self_attn.o_proj True True\n",
      "layers.4.self_attn.rotary_emb True True\n",
      "layers.4.mlp.gate_proj True True\n",
      "layers.4.mlp.up_proj True True\n",
      "layers.4.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.4.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.4.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.4.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.4.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.4.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.4.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_4_layers.4.mlp.act_fn_shard_0_output_0\n",
      "layers.4.input_layernorm True True\n",
      "layers.4.post_attention_layernorm True True\n",
      "layers.5.self_attn.q_proj True True\n",
      "layers.5.self_attn.k_proj True True\n",
      "layers.5.self_attn.v_proj True True\n",
      "layers.5.self_attn.o_proj True True\n",
      "layers.5.self_attn.rotary_emb True True\n",
      "layers.5.mlp.gate_proj True True\n",
      "layers.5.mlp.up_proj True True\n",
      "layers.5.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.5.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.5.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.5.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.5.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.5.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.5.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_5_layers.5.mlp.act_fn_shard_0_output_0\n",
      "layers.5.input_layernorm True True\n",
      "layers.5.post_attention_layernorm True True\n",
      "layers.6.self_attn.q_proj True True\n",
      "layers.6.self_attn.k_proj True True\n",
      "layers.6.self_attn.v_proj True True\n",
      "layers.6.self_attn.o_proj True True\n",
      "layers.6.self_attn.rotary_emb True True\n",
      "layers.6.mlp.gate_proj True True\n",
      "layers.6.mlp.up_proj True True\n",
      "layers.6.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.6.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.6.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.6.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.6.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.6.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.6.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_6_layers.6.mlp.act_fn_shard_0_output_0\n",
      "layers.6.input_layernorm True True\n",
      "layers.6.post_attention_layernorm True True\n",
      "layers.7.self_attn.q_proj True True\n",
      "layers.7.self_attn.k_proj True True\n",
      "layers.7.self_attn.v_proj True True\n",
      "layers.7.self_attn.o_proj True True\n",
      "layers.7.self_attn.rotary_emb True True\n",
      "layers.7.mlp.gate_proj True True\n",
      "layers.7.mlp.up_proj True True\n",
      "layers.7.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.7.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.7.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.7.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.7.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.7.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.7.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_7_layers.7.mlp.act_fn_shard_0_output_0\n",
      "layers.7.input_layernorm True True\n",
      "layers.7.post_attention_layernorm True True\n",
      "layers.8.self_attn.q_proj True True\n",
      "layers.8.self_attn.k_proj True True\n",
      "layers.8.self_attn.v_proj True True\n",
      "layers.8.self_attn.o_proj True True\n",
      "layers.8.self_attn.rotary_emb True True\n",
      "layers.8.mlp.gate_proj True True\n",
      "layers.8.mlp.up_proj True True\n",
      "layers.8.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.8.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.8.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.8.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.8.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.8.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.8.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_8_layers.8.mlp.act_fn_shard_0_output_0\n",
      "layers.8.input_layernorm True True\n",
      "layers.8.post_attention_layernorm True True\n",
      "layers.9.self_attn.q_proj True True\n",
      "layers.9.self_attn.k_proj True True\n",
      "layers.9.self_attn.v_proj True True\n",
      "layers.9.self_attn.o_proj True True\n",
      "layers.9.self_attn.rotary_emb True True\n",
      "layers.9.mlp.gate_proj True True\n",
      "layers.9.mlp.up_proj True True\n",
      "layers.9.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.9.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.9.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.9.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.9.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.9.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.9.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_9_layers.9.mlp.act_fn_shard_0_output_0\n",
      "layers.9.input_layernorm True True\n",
      "layers.9.post_attention_layernorm True True\n",
      "layers.10.self_attn.q_proj True True\n",
      "layers.10.self_attn.k_proj True True\n",
      "layers.10.self_attn.v_proj True True\n",
      "layers.10.self_attn.o_proj True True\n",
      "layers.10.self_attn.rotary_emb True True\n",
      "layers.10.mlp.gate_proj True True\n",
      "layers.10.mlp.up_proj True True\n",
      "layers.10.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.10.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.10.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.10.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.10.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.10.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.10.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.mlp.act_fn_shard_0_output_0\n",
      "layers.10.input_layernorm True True\n",
      "layers.10.post_attention_layernorm True True\n",
      "layers.11.self_attn.q_proj True True\n",
      "layers.11.self_attn.k_proj True True\n",
      "layers.11.self_attn.v_proj True True\n",
      "layers.11.self_attn.o_proj True True\n",
      "layers.11.self_attn.rotary_emb True True\n",
      "layers.11.mlp.gate_proj True True\n",
      "layers.11.mlp.up_proj True True\n",
      "layers.11.mlp.down_proj.base_layer True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.base_layer_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.base_layer_shard_0_output_0\n",
      "layers.11.mlp.down_proj.lora_dropout.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_dropout.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_dropout.default_shard_0_output_0\n",
      "layers.11.mlp.down_proj.lora_A.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_A.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_A.default_shard_0_output_0\n",
      "layers.11.mlp.down_proj.lora_B.default True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_B.default_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_B.default_shard_0_output_0\n",
      "layers.11.mlp.down_proj.lora_embedding_A False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_embedding_A_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_embedding_A_shard_0_output_0\n",
      "layers.11.mlp.down_proj.lora_embedding_B False False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_embedding_B_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.down_proj.lora_embedding_B_shard_0_output_0\n",
      "layers.11.mlp.act_fn True False\n",
      "\t /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.act_fn_shard_0_input_0 /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.mlp.act_fn_shard_0_output_0\n",
      "layers.11.input_layernorm True True\n",
      "layers.11.post_attention_layernorm True True\n",
      "norm True True\n",
      "lm_head True True\n"
     ]
    }
   ],
   "source": [
    "named_modules_ = [\n",
    "    name.replace(\"base_model.model.model.\", \"\").replace(\"base_model.model.model\", \"\").replace(\"base_model.model.\", \"\").replace(\"base_model.model\", \"\").replace(\"base_model.\", \"\").replace(\"base_model\", \"\")\n",
    "    for name, _ in model.named_modules()\n",
    "]\n",
    "\n",
    "def remove_prefixes(named_modules):\n",
    "    i = 0\n",
    "    while i < len(named_modules) - 1:\n",
    "        if named_modules[i + 1].startswith(named_modules[i]):\n",
    "            named_modules.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    return named_modules\n",
    "named_modules = remove_prefixes(named_modules_)\n",
    "\n",
    "def convert_hf_module_name_to_ff_filenames(n, num_layers=12):\n",
    "    if n == \"embed_tokens\":\n",
    "        ff_in_name = \"fwd_step_0_layers_0_embed_tokens_shard_0_input_0\"\n",
    "        ff_out_name = \"fwd_step_0_layers_0_embed_tokens_shard_0_output_0\"\n",
    "    elif n == \"lm_head\" or n == \"norm\":\n",
    "        ff_in_name = f\"fwd_step_0_layers_{num_layers-1}_{n}_shard_0_input_0\"\n",
    "        ff_out_name = f\"fwd_step_0_layers_{num_layers-1}_{n}_shard_0_output_0\"\n",
    "    elif n.startswith(\"layers.\"):\n",
    "        layernum = n.split(\"layers.\")[1].split(\".\")[0]\n",
    "        ff_in_name = f\"fwd_step_0_layers_{layernum}_{n}_shard_0_input_0\"\n",
    "        ff_out_name = f\"fwd_step_0_layers_{layernum}_{n}_shard_0_output_0\"\n",
    "    else:\n",
    "        assert False, f\"Module {n} not supported yet\"\n",
    "    return os.path.join(ff_path, ff_in_name), os.path.join(ff_path, ff_out_name)\n",
    "\n",
    "# Compute the hf path, check if the input and output are there\n",
    "for n in named_modules:\n",
    "    in_name = f\"fwd_step_0_{n}.input_0\"\n",
    "    out_name = f\"fwd_step_0_{n}.output_0\"\n",
    "    if n == \"lm_head\":\n",
    "        in_name = f\"fwd_step_0_base_model.model.{n}.input_0\"\n",
    "        out_name = f\"fwd_step_0_base_model.model.{n}.output_0\"\n",
    "    hf_mod_in = os.path.join(hf_path, in_name)\n",
    "    hf_mod_out = os.path.join(hf_path, out_name)\n",
    "    check = os.path.exists(hf_mod_in) and os.path.exists(hf_mod_out)\n",
    "    \n",
    "    check2=True\n",
    "    if \"self_attn\" not in n:\n",
    "        ff_mod_in, ff_mod_out = convert_hf_module_name_to_ff_filenames(n, num_layers=num_layers)\n",
    "        check2 = os.path.exists(ff_mod_in) and os.path.exists(ff_mod_out)\n",
    "    print(n, check, check2)\n",
    "    if not check2:\n",
    "        print(\"\\t\", ff_mod_in, ff_mod_out)\n",
    "    # print(n, check)\n",
    "    # print(\"\\t\", )\n",
    "    \n",
    "\n",
    "# Compute the corresponding ff path, check if the input and output are there\n",
    "\n",
    "# for x in named_modules:\n",
    "#     print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down_proj'}\n"
     ]
    }
   ],
   "source": [
    "print(model.peft_config['default'].target_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "hf_embed_input= \"/usr/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_embed_tokens.input_0\"\n",
    "ff_embed_input=\"/usr/FlexFlow/tests/peft/inference_tensors/fwd_step_0_layers_0_embed_tokens_shard_0_input_0\"\n",
    "compare_tensors(hf_embed_input, ff_embed_input)\n",
    "hf_embed_output=\"/usr/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_embed_tokens.output_0\"\n",
    "ff_embed_output=\"/usr/FlexFlow/tests/peft/inference_tensors/fwd_step_0_layers_0_embed_tokens_shard_0_output_0\"\n",
    "compare_tensors(hf_embed_output, ff_embed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_layers.10.input_layernorm.input_0 and /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_10_layers.10.input_layernorm_shard_0_output_0\n",
      "HF: [ 0.          0.          0.         ...  0.06630182  6.3429456\n",
      " -0.21220279]\n",
      "FF:[ 0.          0.          0.         ...  0.06630275  6.34293985\n",
      " -0.21219885]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[15889]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_layers.11.input_layernorm.input_0 and /usr/FlexFlow/build/inference_tensors/fwd_step_0_layers_11_layers.11.input_layernorm_shard_0_output_0\n",
      "HF: [ 0.          0.          0.         ...  0.14172177  9.79423\n",
      " -6.2940273 ]\n",
      "FF:[ 0.          0.          0.         ...  0.14172006  9.79421902\n",
      " -6.29402065]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 2878  3206  3367  3607  5183  5346  6257  6544  7466  7679  7805  8119\n",
      "  8159  8911  9450  9897 13696 13938 14058 14599 15126 15839 16128 16195]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "for i in range(tot_num_layers):\n",
    "    hf_input_ln_in = f\"{hf_path}/fwd_step_0_layers.{i}.input_layernorm.input_0\"\n",
    "    ff_input_ln_in = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.input_layernorm_shard_0_input_0\"\n",
    "    if i > 0:\n",
    "        ff_input_ln_in = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.input_layernorm_shard_0_output_0\"\n",
    "    compare_tensors(hf_input_ln_in, ff_input_ln_in, tolerance=1e-5)\n",
    "    hf_input_ln_out = f\"{hf_path}/fwd_step_0_layers.{i}.input_layernorm.output_0\"\n",
    "    ff_input_ln_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.input_layernorm_shard_0_output_0\"\n",
    "    if i > 0:\n",
    "        ff_input_ln_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.input_layernorm_shard_0_output_1\"\n",
    "    compare_tensors(hf_input_ln_out, ff_input_ln_out, tolerance=1e-5)\n",
    "    hf_attn_out = f\"{hf_path}/fwd_step_0_layers.{i}.self_attn.o_proj.output_0\"\n",
    "    ff_attn_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.self_attn_shard_0_output_0\"\n",
    "    compare_tensors(hf_attn_out, ff_attn_out, tolerance=1e-5)\n",
    "    hf_ffn_norm_out = f\"{hf_path}/fwd_step_0_layers.{i}.post_attention_layernorm.output_0\"\n",
    "    ff_ffn_norm_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.post_attention_layernorm_shard_0_output_1\"\n",
    "    compare_tensors(hf_ffn_norm_out, ff_ffn_norm_out, tolerance=1e-5)\n",
    "    # w1\n",
    "    hf_gate_proj_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.gate_proj.output_0\"\n",
    "    ff_gate_proj_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.gate_proj_shard_0_output_0\"\n",
    "    compare_tensors(hf_gate_proj_out, ff_gate_proj_out, tolerance=1e-5)\n",
    "    # w3\n",
    "    hf_up_proj_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.up_proj.output_0\" \n",
    "    ff_up_proj_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.up_proj_shard_0_output_0\"\n",
    "    compare_tensors(hf_up_proj_out, ff_up_proj_out, tolerance=1e-5)\n",
    "    # w2\n",
    "    hf_down_proj_in = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.down_proj.input_0\"\n",
    "    hf_down_proj_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.down_proj.output_0\"\n",
    "    ff_down_proj_in = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.down_proj_shard_0_input_0\"\n",
    "    ff_down_proj_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.down_proj_shard_0_output_0\"\n",
    "    compare_tensors(hf_down_proj_in, ff_down_proj_in)\n",
    "    # compare_tensors(hf_down_proj_out, ff_down_proj_out)\n",
    "    # LORA input\n",
    "    hf_lora_A_in = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.down_proj.lora_A.default.input_0\"\n",
    "    ff_lora_A_in = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.down_proj.lora_shard_0_input_0\"\n",
    "    compare_hf_tensors(hf_down_proj_in, hf_lora_A_in)\n",
    "    compare_tensors(hf_lora_A_in, ff_lora_A_in)\n",
    "    # LORA weights\n",
    "    hf_lora_A_weight_fp = f\"{hf_path}/layers.{i}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.down_proj.lora_shard_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp)\n",
    "    hf_lora_B_weight_fp = f\"{hf_path}/layers.{i}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.down_proj.lora_shard_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp)\n",
    "    # LORA intermediate hf\n",
    "    hf_lora_A_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.down_proj.lora_A.default.output_0\"\n",
    "    hf_lora_B_in = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.down_proj.lora_B.default.input_0\"\n",
    "    compare_hf_tensors(hf_lora_A_out, hf_lora_B_in)\n",
    "    # LORA output\n",
    "    hf_lora_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.down_proj.lora_B.default.output_0\"\n",
    "    ff_lora_out = f\"{ff_path}/fwd_step_0_layers_{i}_layers.{i}.mlp.down_proj.lora_shard_0_output_0\"\n",
    "    # compare_tensors(hf_lora_out, ff_lora_out)\n",
    "    # compare_flexflow_tensors(ff_down_proj_out, ff_lora_out)\n",
    "    # compare_tensors(hf_down_proj_out, ff_lora_out)\n",
    "    compare_tensors_difference(hf_lora_out, ff_lora_out, ff_down_proj_out)\n",
    "    \n",
    "\n",
    "# After last layer only\n",
    "hf_norm_out = f\"{hf_path}/fwd_step_0_norm.output_0\"\n",
    "ff_norm_out = f\"{ff_path}/fwd_step_0_layers_{tot_num_layers-1}_norm_shard_0_output_1\"\n",
    "compare_tensors(hf_norm_out, ff_norm_out, tolerance=1e-5)\n",
    "hf_lm_head_out = f\"{hf_path}/fwd_step_0_base_model.model.lm_head.output_0\"\n",
    "ff_lm_head_out = f\"{ff_path}/fwd_step_0_layers_{tot_num_layers-1}_lm_head_shard_0_output_0\"\n",
    "compare_tensors(hf_lm_head_out, ff_lm_head_out, tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- LM head --\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "-- Final Norm --\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "\n",
    "# ff_BWD_softmax_in = f\"{ff_path}/model_0_bwd-step_0_layer-num_100_layer-name_Softmax_shard-id_0_input_0\"\n",
    "print(\"-- LM head --\")\n",
    "hf_BWD_lm_head_out = f\"{hf_path}/bwd_step_0_base_model.model.lm_head.go_0\"\n",
    "ff_BWD_lm_head_out = f\"{ff_path}/bwd_step_0_layers_{tot_num_layers-1}_output_shard_0_output_0\"\n",
    "compare_tensors(hf_BWD_lm_head_out, ff_BWD_lm_head_out, tolerance=1e-5)\n",
    "# compare weights\n",
    "hf_lm_head_weight = f\"{hf_path}/base_model.model.lm_head.weight\"\n",
    "ff_lm_head_weight = f\"{ff_path}/fwd_step_0_layers_{tot_num_layers-1}_output_shard_0_weight_0\"\n",
    "compare_tensors(hf_lm_head_weight, ff_lm_head_weight, tolerance=1e-5)\n",
    "hf_BWD_lm_head_in = f\"{hf_path}/bwd_step_0_base_model.model.lm_head.gi_0\"\n",
    "ff_BWD_lm_head_in = f\"{ff_path}/bwd_step_0_layers_{tot_num_layers-1}_output_shard_0_input_0\"\n",
    "compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in, tolerance=1e-5)\n",
    "# # Manually check the matmul\n",
    "# ff_tensor_out = np.loadtxt(ff_BWD_lm_head_out, delimiter=',')\n",
    "# ff_weight = np.loadtxt(ff_lm_head_weight, delimiter=',').reshape((4096,32000), order='F')\n",
    "# ff_tensor_out = ff_tensor_out[:32000*24].reshape((32000,24), order='F')\n",
    "# print(ff_tensor_out.shape)\n",
    "# print(ff_weight.shape)\n",
    "# print(np.matmul(ff_weight, ff_tensor_out))\n",
    "# compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in)\n",
    "# ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "print(\"-- Final Norm --\")\n",
    "hf_BWD_norm_out = f\"{hf_path}/bwd_step_0_norm.go_0\"\n",
    "ff_BWD_norm_out = f\"{ff_path}/bwd_step_0_layers_{tot_num_layers-1}_norm_shard_0_output_0\"\n",
    "compare_hf_tensors(hf_BWD_lm_head_in, hf_BWD_norm_out)\n",
    "compare_tensors(hf_BWD_norm_out, ff_BWD_norm_out)\n",
    "ff_BWD_norm_weight = f\"{ff_path}/fwd_step_0_layers_{tot_num_layers-1}_norm_shard_0_weight_0\"\n",
    "hf_FWD_norm_weight = f\"{hf_path}/norm.weight\"\n",
    "compare_tensors(hf_FWD_norm_weight, ff_BWD_norm_weight, tolerance=1e-5)\n",
    "hf_BWD_norm_in = f\"{hf_path}/bwd_step_0_norm.gi_0\"\n",
    "ff_BWD_norm_in = f\"{ff_path}/bwd_step_0_layers_{tot_num_layers-1}_norm_shard_0_input_1\"\n",
    "compare_tensors(hf_BWD_norm_in, ff_BWD_norm_in, tolerance=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2] # first half\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # second half\n",
    "    return torch.cat((x2, -x1), dim=-1)\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "head_dim = 64\n",
    "max_position_embeddings = 2048\n",
    "rope_theta=10_000\n",
    "kv_seq_len = 24\n",
    "rotary_emb = LlamaRotaryEmbedding(\n",
    "    head_dim,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    base=rope_theta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "-- W2 --\n",
      "Ok!\n",
      "Ok!\n",
      "-- Lora --\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "-- W2/W1/W3 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_11_SigmoidSiluMulti_shard_0_output_0\n",
      "HF: [ 6.4350547e+03 -6.4898600e+05  1.1761116e+05 ...  2.1410337e+01\n",
      "  1.2096541e+01  3.6424692e+00]\n",
      "FF:[ 6.43525000e+03 -6.48986062e+05  1.17611250e+05 ...  2.14103413e+01\n",
      "  1.20965385e+01  3.64246368e+00]\n",
      "[False  True  True ...  True  True  True]\n",
      "[   0  162  185  308  339  745  747  820  830  909  933  968 1008 1156\n",
      " 1160 1190 1212 1296 1304 1311 1323 1353 1395 1421 1523 1578 1689 1717\n",
      " 1736 1748 1836 2074 2124 2192 2221 2313 2394 2515 2518 2693 2758 2825\n",
      " 2888 2894 2937 3024]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_11_layers_11_feed_forward_w2_shard_0_input_0\n",
      "HF: [ 6.4350547e+03 -6.4898600e+05  1.1761116e+05 ...  2.1410337e+01\n",
      "  1.2096541e+01  3.6424692e+00]\n",
      "FF:[ 6.43525000e+03 -6.48986062e+05  1.17611250e+05 ...  2.14103413e+01\n",
      "  1.20965385e+01  3.64246368e+00]\n",
      "[False  True  True ...  True  True  True]\n",
      "[   0  162  185  308  339  745  747  820  830  909  933  968 1008 1156\n",
      " 1160 1190 1212 1296 1304 1311 1323 1353 1395 1421 1523 1578 1689 1717\n",
      " 1736 1748 1836 2074 2124 2192 2221 2313 2394 2515 2518 2693 2758 2825\n",
      " 2888 2894 2937 3024]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "-- Attention --\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.self_attn.o_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_11_layers_11_attention_shard_0_o_proj_in_grad\n",
      "HF: [ 1.2223595e+06 -2.6348565e+06 -5.0760525e+05 ...  6.8275871e+01\n",
      " -5.8116108e+01  9.5347488e+01]\n",
      "FF:[ 1.22235925e+06 -2.63485625e+06 -5.07605000e+05 ...  6.82758865e+01\n",
      " -5.81161423e+01  9.53475494e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 51  77  95 168 175 232 725]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[ 1.22235950e+06  9.93645859e+01 -2.82157593e+01 ... -3.94578514e+01\n",
      "  -1.98409653e+01 -1.33438044e+01]\n",
      " [-2.63485650e+06 -1.13461929e+02  1.14223976e+02 ...  7.52578735e+01\n",
      "   1.33362747e+02  6.78501587e+01]\n",
      " [-5.07605250e+05  4.34111862e+01  8.10619354e+01 ...  4.70537224e+01\n",
      "   4.02149696e+01  6.98045502e+01]\n",
      " ...\n",
      " [ 3.02792250e+06  3.31295319e+02  9.98417091e+00 ...  4.90895653e+01\n",
      "   9.71413574e+01  6.82758713e+01]\n",
      " [-3.64456375e+06 -2.43692596e+02 -6.85474396e+00 ... -3.71503868e+01\n",
      "  -1.34136658e+01 -5.81161079e+01]\n",
      " [ 3.31921500e+06  2.24193970e+02 -6.64005566e+00 ...  2.11662292e+00\n",
      "   3.37400856e+01  9.53474884e+01]]\n",
      "FF:[[ 1.22235925e+06  9.93645630e+01 -2.82157211e+01 ... -3.94577713e+01\n",
      "  -1.98408775e+01 -1.33438234e+01]\n",
      " [-2.63485625e+06 -1.13461960e+02  1.14224037e+02 ...  7.52577744e+01\n",
      "   1.33362701e+02  6.78501205e+01]\n",
      " [-5.07605000e+05  4.34111404e+01  8.10619278e+01 ...  4.70536804e+01\n",
      "   4.02149124e+01  6.98045578e+01]\n",
      " ...\n",
      " [ 3.02792250e+06  3.31295227e+02  9.98412323e+00 ...  4.90895386e+01\n",
      "   9.71413727e+01  6.82758865e+01]\n",
      " [-3.64456400e+06 -2.43692627e+02 -6.85472488e+00 ... -3.71504822e+01\n",
      "  -1.34137001e+01 -5.81161423e+01]\n",
      " [ 3.31921500e+06  2.24193970e+02 -6.64004517e+00 ...  2.11670875e+00\n",
      "   3.37400322e+01  9.53475494e+01]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[ 51  77  95 168 175 232 725]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[ 1.2223588e+06 -2.6348530e+06 -5.0760291e+05 ...  3.0279325e+06\n",
      "  -3.6445672e+06  3.3192180e+06]\n",
      " [-4.2496326e+02  1.1576636e+03  9.8397858e+02 ...  1.6480791e+03\n",
      "  -5.9697235e+02  6.2627173e+02]\n",
      " [-2.2012039e+01  6.6097900e+01  3.9933994e+01 ...  5.7103355e+01\n",
      "  -1.5968766e+01  3.6536639e+00]\n",
      " ...\n",
      " [-1.2302110e+00  5.3052688e+00  2.1982718e+00 ...  1.3990868e+00\n",
      "  -5.5132383e-01  4.8985812e-01]\n",
      " [-1.0771493e+00  6.9571300e+00  2.7373023e+00 ...  4.9663010e+00\n",
      "  -9.9705428e-01  2.1829298e+00]\n",
      " [-5.9534687e-01  3.0272012e+00  3.1143982e+00 ...  2.4072502e+00\n",
      "  -2.0490403e+00  3.3617332e+00]]\n",
      "FF:[[ 1.22235850e+06 -2.63485275e+06 -5.07602656e+05 ...  3.02793250e+06\n",
      "  -3.64456750e+06  3.31921800e+06]\n",
      " [-4.24962585e+02  1.15766296e+03  9.83978577e+02 ...  1.64807898e+03\n",
      "  -5.96972351e+02  6.26271790e+02]\n",
      " [-2.20120354e+01  6.60979462e+01  3.99340210e+01 ...  5.71033745e+01\n",
      "  -1.59687757e+01  3.65366316e+00]\n",
      " ...\n",
      " [-1.23020661e+00  5.30526114e+00  2.19826817e+00 ...  1.39908671e+00\n",
      "  -5.51325083e-01  4.89858717e-01]\n",
      " [-1.07714510e+00  6.95712519e+00  2.73729825e+00 ...  4.96630049e+00\n",
      "  -9.97055829e-01  2.18292713e+00]\n",
      " [-5.95347941e-01  3.02720070e+00  3.11439991e+00 ...  2.40725493e+00\n",
      "  -2.04904509e+00  3.36174107e+00]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[0 0 0 0 0 0 0]\n",
      "Ok!\n",
      "7.4363425925925934% mismatch in QK prods softmax out grad\n",
      "Ok!\n",
      "hf_attn_in:  (768, 24)\n",
      "[[-7.52523500e+06 -1.27625415e+03 -4.39338150e+01 ... -3.34414902e+01\n",
      "   2.38160934e+01  3.15938339e+01]\n",
      " [-9.55138900e+06  6.71377197e+02  2.06871887e+02 ... -3.86393509e+01\n",
      "   2.14816055e+01 -6.58599396e+01]\n",
      " [ 1.14522670e+07  2.19898975e+03 -6.89673233e+00 ...  9.51593590e+00\n",
      "  -1.68612709e+01  6.02474251e+01]\n",
      " ...\n",
      " [ 2.10891925e+06  3.78648706e+03  1.02701221e+03 ...  3.59794388e+01\n",
      "   5.03902206e+01  4.19777756e+01]\n",
      " [ 2.11695300e+06 -2.36283508e+02 -1.08002625e+02 ...  9.36443710e+00\n",
      "   3.84094887e+01 -7.51948738e+00]\n",
      " [ 7.39155050e+06  1.11731885e+03  3.38369843e+02 ...  3.70399475e+01\n",
      "   1.77629051e+01  9.76780853e+01]]\n",
      "ff_attn_in:  (768, 24)\n",
      "[[-7.52523600e+06 -1.27625293e+03 -4.39336700e+01 ... -3.34414597e+01\n",
      "   2.38162422e+01  3.15938187e+01]\n",
      " [-9.55138900e+06  6.71377319e+02  2.06871674e+02 ... -3.86393127e+01\n",
      "   2.14817867e+01 -6.58600464e+01]\n",
      " [ 1.14522660e+07  2.19898950e+03 -6.89660644e+00 ...  9.51594448e+00\n",
      "  -1.68611774e+01  6.02474518e+01]\n",
      " ...\n",
      " [ 2.10891850e+06  3.78648633e+03  1.02701196e+03 ...  3.59794846e+01\n",
      "   5.03901253e+01  4.19777679e+01]\n",
      " [ 2.11695400e+06 -2.36282440e+02 -1.08002762e+02 ...  9.36448860e+00\n",
      "   3.84096107e+01 -7.51954842e+00]\n",
      " [ 7.39155000e+06  1.11731921e+03  3.38370087e+02 ...  3.70398293e+01\n",
      "   1.77627277e+01  9.76782227e+01]]\n",
      "6.011284722222222% mismatch in attention input grads\n",
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "-- W2 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.mlp.down_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_layers_10_feed_forward_w2_shard_0_output_0\n",
      "HF: [-9.4779546e+09 -1.2174155e+10  1.4899113e+10 ...  4.9057606e+01\n",
      "  4.7770348e+01  5.8564331e+01]\n",
      "FF:[-9.47795558e+09 -1.21741548e+10  1.48991119e+10 ...  4.90575981e+01\n",
      "  4.77703362e+01  5.85643845e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   88   138   187   203   232   242   493   657   750   900  1198  1249\n",
      "  1287  1305  1414  1428  1490  1588  1600  1612  1625  1657  1676  1677\n",
      "  1692  1694  1724  1730  1772  1822  1825  1838  1853  1910  2035  2043\n",
      "  2053  2059  2073  2078  2123  2145  2214  2238  2241  2285  2292  2389\n",
      "  2542  2582  2589  2599  2674  2688  2711  2840  2856  2961  2963  2980\n",
      "  3064  3176  3192  3255  3262  3278  3338  3341  3412  3419  3492  3590\n",
      "  3624  3646  3657  3807  3840  3842  3846  3883  3887  4005  4049  4071\n",
      "  4076  4077  4079  4137  4142  4192  4193  4202  4218  4224  4273  4355\n",
      "  4358  4381  4401  4435  4469  4499  4514  4546  4598  4619  4747  4846\n",
      "  4872  4916  4952  4966  5016  5067  5107  5112  5116  5194  5225  5350\n",
      "  5364  5403  5515  5537  5550  5578  5650  5653  5654  5736  5751  5837\n",
      "  5870  5881  5972  5998  6006  6051  6061  6107  6129  6204  6236  6292\n",
      "  6296  6327  6382  6393  6403  6420  6424  6436  6468  6542  6599  6675\n",
      "  6681  6711  6723  6767  6823  6914  6983  7047  7064  7133  7167  7197\n",
      "  7198  7209  7528  7537  7538  7686  7850  7855  7889  7910  7919  7927\n",
      "  7937  7939  8089  8101  8157  8169  8175  8223  8292  8304  8306  8342\n",
      "  8351  8414  8475  8500  8543  8558  8609  8656  8687  8704  8724  8726\n",
      "  8777  8816  8826  8871  8904  8934  8983  9012  9033  9043  9068  9093\n",
      "  9125  9133  9144  9151  9154  9217  9222  9320  9335  9367  9398  9421\n",
      "  9434  9521  9547  9633  9702  9726  9763  9949 10018 10053 10062 10079\n",
      " 10137 10149 10203 10261 10269 10292 10312 10332 10471 10478 10514 10596\n",
      " 10645 10676 10678 10781 10795 10810 10833 10891 10904 10935 10957 10977\n",
      " 10982 11028 11095 11172 11223 11251 11283 11303 11319 11374 11392 11437\n",
      " 11486 11627 11678 11750 11759 11979 11996 12019 12126 12237 12262 12288\n",
      " 12303 12309 12315 12387 12543 12569 12613 12648 12786 12852 12866 12879\n",
      " 12947 12963 13037 13058 13261 13284 13312 13394 13399 13427 13526 13527\n",
      " 13592 13695 13741 13752 13775 13803 13812 13866 13902 14049 14170 14241\n",
      " 14354 14382 14426 14451 14455 14486 14502 14582 14820 14934 14961 14976\n",
      " 15000 15003 15014 15077 15096 15108 15135 15148 15165 15219 15232 15290\n",
      " 15339 15345 15819 15945 15994 16077 16135 16218 16231 16233 16239 16243\n",
      " 16295 16311 16339 16356 16366 16417 16456 16498 16502 16503 16506 16547\n",
      " 16585 16603 16611 16633 16661 16683 16704 16710 16723 16724 16745 16754\n",
      " 16773 16787 16789 16818 16829 16833 16913 16933 17025 17033 17037 17055\n",
      " 17084 17098 17109 17176 17225 17240 17292 17294 17339 17390 17427 17437\n",
      " 17579 17626 17630 17654 17719 17902 17912 18023 18025 18124 18203 18339\n",
      " 18344]\n",
      "Ok!\n",
      "Ok!\n",
      "-- Lora --\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.mlp.down_proj.lora_B.default.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_layers_10_feed_forward_w2_lora_shard_0_output_0\n",
      "HF: [-9.4779546e+09 -1.2174155e+10  1.4899113e+10 ...  4.9057606e+01\n",
      "  4.7770348e+01  5.8564331e+01]\n",
      "FF:[-9.47795558e+09 -1.21741548e+10  1.48991119e+10 ...  4.90575981e+01\n",
      "  4.77703362e+01  5.85643845e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 88 138 187 203 232 242 493 657 750]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.mlp.down_proj.lora_A.default.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_layers_10_feed_forward_w2_lora_shard_0_input_0\n",
      "HF: [ 4.7819588e+07  3.8833264e+07  4.7789860e+07 ...  1.0804405e+00\n",
      "  2.7186510e-01 -2.9918199e+00]\n",
      "FF:[ 4.78195960e+07  3.88332640e+07  4.77898600e+07 ...  1.08044124e+00\n",
      "  2.71864563e-01 -2.99182224e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 109  211  312  422  590  832  835 1016 1053 1076 1268 1353 1374 1693\n",
      " 1701 1710 1722 1832 1954 1965 1997 2076 2124 2146 2378 2520 2605 2624\n",
      " 2967 3007 3015]\n",
      "Ok!\n",
      "-- W2/W1/W3 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_SigmoidSiluMulti_shard_0_output_0\n",
      "HF: [ 3.3558659e+09  1.3409817e+10 -1.4671958e+10 ...  7.2100967e+01\n",
      "  6.5979071e+00 -2.1230124e+01]\n",
      "FF:[ 3.35586406e+09  1.34098166e+10 -1.46719611e+10 ...  7.21009750e+01\n",
      "  6.59790993e+00 -2.12301121e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   4   95  111  163  179  191  279  305  363  406  447  487  489  494\n",
      "  517  617  703  713  735  796  805  819  826  858  882  959  964  967\n",
      "  986 1020 1035 1054 1067 1070 1077 1081 1095 1097 1123 1139 1181 1238\n",
      " 1296 1342 1369 1489 1550 1557 1623 1669 1752 1757 1783 1819 1876 1949\n",
      " 1963 1993 2034 2047 2091 2115 2153 2170 2306 2381 2419 2431 2456 2501\n",
      " 2503 2591 2653 2768 2778 2791 2970 2980 3053 3067]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_layers_10_feed_forward_w2_shard_0_input_0\n",
      "HF: [ 3.3558659e+09  1.3409817e+10 -1.4671958e+10 ...  7.2100967e+01\n",
      "  6.5979071e+00 -2.1230124e+01]\n",
      "FF:[ 3.35586406e+09  1.34098166e+10 -1.46719611e+10 ...  7.21009750e+01\n",
      "  6.59790993e+00 -2.12301121e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   4   95  111  163  179  191  279  305  363  406  447  487  489  494\n",
      "  517  617  703  713  735  796  805  819  826  858  882  959  964  967\n",
      "  986 1020 1035 1054 1067 1070 1077 1081 1095 1097 1123 1139 1181 1238\n",
      " 1296 1342 1369 1489 1550 1557 1623 1669 1752 1757 1783 1819 1876 1949\n",
      " 1963 1993 2034 2047 2091 2115 2153 2170 2306 2381 2419 2431 2456 2501\n",
      " 2503 2591 2653 2768 2778 2791 2970 2980 3053 3067]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "-- Attention --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.self_attn.o_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_layers_10_attention_shard_0_output_0\n",
      "HF: [-9.4779546e+09 -1.2174155e+10  1.4899113e+10 ...  9.3464905e+01\n",
      "  7.5613129e+01  7.6598846e+01]\n",
      "FF:[-9.47795558e+09 -1.21741548e+10  1.48991119e+10 ...  9.34649200e+01\n",
      "  7.56131058e+01  7.65989227e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 88 138 187 203 232 242 493 657 750]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.10.self_attn.o_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_10_layers_10_attention_shard_0_o_proj_in_grad\n",
      "HF: [-9.4470595e+09 -7.3870331e+09  1.2659395e+10 ... -2.8149616e+01\n",
      "  1.7019112e+02 -7.7236428e+00]\n",
      "FF:[-9.44706150e+09 -7.38703309e+09  1.26593966e+10 ... -2.81496239e+01\n",
      "  1.70191177e+02 -7.72364044e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 11  98 109 134 262 266 274 309 310 327 328 364 398 409 429 605 645]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[-9.44705946e+09  2.28078384e+01  3.18554016e+02 ...  1.17267204e+02\n",
      "   2.06791725e+01  1.13138672e+02]\n",
      " [-7.38703309e+09 -7.36898804e+00  7.93705673e+01 ...  2.04039650e+01\n",
      "   3.18331490e+01  5.44241562e+01]\n",
      " [ 1.26593946e+10  1.77534424e+02 -2.97175941e+01 ...  1.16716766e+01\n",
      "   7.70214081e+01  2.81902496e+02]\n",
      " ...\n",
      " [ 4.51210445e+10  3.63867615e+02 -8.04915466e+01 ... -1.34332123e+02\n",
      "  -1.22151840e+02 -2.81496162e+01]\n",
      " [-1.39591885e+10  1.59216873e+02  6.11343079e+01 ...  1.56675262e+02\n",
      "   9.68551483e+01  1.70191116e+02]\n",
      " [-1.29442345e+10 -2.39441833e+02  2.73647644e+02 ... -4.41197014e+01\n",
      "  -9.48526230e+01 -7.72364283e+00]]\n",
      "FF:[[-9.44706150e+09  2.28079376e+01  3.18553864e+02 ...  1.17267227e+02\n",
      "   2.06791859e+01  1.13138741e+02]\n",
      " [-7.38703309e+09 -7.36921692e+00  7.93703690e+01 ...  2.04038925e+01\n",
      "   3.18332825e+01  5.44241333e+01]\n",
      " [ 1.26593966e+10  1.77534454e+02 -2.97174206e+01 ...  1.16717224e+01\n",
      "   7.70213699e+01  2.81902618e+02]\n",
      " ...\n",
      " [ 4.51210527e+10  3.63867554e+02 -8.04915695e+01 ... -1.34332092e+02\n",
      "  -1.22151901e+02 -2.81496239e+01]\n",
      " [-1.39591834e+10  1.59216995e+02  6.11343040e+01 ...  1.56675293e+02\n",
      "   9.68551559e+01  1.70191177e+02]\n",
      " [-1.29442304e+10 -2.39441772e+02  2.73647644e+02 ... -4.41196594e+01\n",
      "  -9.48526916e+01 -7.72364044e+00]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[ 11  98 109 134 262 266 274 309 310 327 328 364 398 409 429 605 645]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[-9.44705946e+09 -7.38703309e+09  1.26593946e+10 ...  4.51210445e+10\n",
      "  -1.39591885e+10 -1.29442345e+10]\n",
      " [ 1.14852783e+03  4.39543152e+02  1.07877356e+03 ... -2.42416113e+03\n",
      "   2.64504834e+03  4.68633453e+02]\n",
      " [ 5.72417107e+01  4.12602806e+01 -2.27319489e+01 ... -3.40788422e+01\n",
      "   4.86237946e+01  1.25752163e+01]\n",
      " ...\n",
      " [ 6.76848269e+00  8.23165894e+00  2.10253639e+01 ... -3.19590777e-01\n",
      "   3.68098617e-01 -1.95310101e-01]\n",
      " [ 4.08574820e+00  5.33035660e+00  1.41003275e+01 ... -1.35607815e+00\n",
      "   4.06074905e+00 -7.67630756e-01]\n",
      " [ 2.03186665e+01  9.77407932e+00  5.06271019e+01 ... -6.80029154e-01\n",
      "   4.11142111e+00 -1.86585218e-01]]\n",
      "FF:[[-9.44706150e+09 -7.38703309e+09  1.26593966e+10 ...  4.51210527e+10\n",
      "  -1.39591834e+10 -1.29442304e+10]\n",
      " [ 1.14852808e+03  4.39542755e+02  1.07877344e+03 ... -2.42416138e+03\n",
      "   2.64504932e+03  4.68633698e+02]\n",
      " [ 5.72415771e+01  4.12602005e+01 -2.27318707e+01 ... -3.40787392e+01\n",
      "   4.86236725e+01  1.25752039e+01]\n",
      " ...\n",
      " [ 6.76847696e+00  8.23167515e+00  2.10253181e+01 ... -3.19590837e-01\n",
      "   3.68098557e-01 -1.95310280e-01]\n",
      " [ 4.08574867e+00  5.33037567e+00  1.41003180e+01 ... -1.35607564e+00\n",
      "   4.06074095e+00 -7.67629445e-01]\n",
      " [ 2.03186874e+01  9.77407932e+00  5.06271439e+01 ... -6.80029511e-01\n",
      "   4.11142349e+00 -1.86585203e-01]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Ok!\n",
      "6.640625% mismatch in QK prods softmax out grad\n",
      "Ok!\n",
      "hf_attn_in:  (768, 24)\n",
      "[[-5.1505955e+10 -4.7166772e+03 -1.3288132e+02 ... -3.0123844e+00\n",
      "  -5.5234032e+01  6.0299168e+00]\n",
      " [-3.5960029e+10 -5.3263096e+03 -1.9434322e+02 ... -5.6601189e+01\n",
      "  -1.0787462e+02 -6.0718418e+01]\n",
      " [ 4.8131662e+10  1.1578307e+04  1.7744476e+02 ... -5.6970375e+01\n",
      "  -1.7497168e+01 -7.2297249e+00]\n",
      " ...\n",
      " [-9.0346426e+08  6.4752144e+03  3.2408417e+02 ...  6.1075470e+01\n",
      "   8.5356834e+01  8.3221588e+01]\n",
      " [-5.0754217e+09 -2.2929268e+03 -1.4913528e+02 ...  8.6639397e+01\n",
      "   1.1156468e+02  1.0695674e+02]\n",
      " [ 5.5844772e+09  3.0225920e+03 -6.3137859e+01 ... -6.5270996e+01\n",
      "   8.2730171e+01 -1.0107367e+02]]\n",
      "ff_attn_in:  (768, 24)\n",
      "[[-5.15059548e+10 -4.71667773e+03 -1.32881012e+02 ... -3.01225996e+00\n",
      "  -5.52339973e+01  6.02991867e+00]\n",
      " [-3.59600292e+10 -5.32630957e+03 -1.94343079e+02 ... -5.66010437e+01\n",
      "  -1.07874649e+02 -6.07182846e+01]\n",
      " [ 4.81316659e+10  1.15783076e+04  1.77444519e+02 ... -5.69703102e+01\n",
      "  -1.74972763e+01 -7.22990799e+00]\n",
      " ...\n",
      " [-9.03455232e+08  6.47521484e+03  3.24083832e+02 ...  6.10753632e+01\n",
      "   8.53567886e+01  8.32217255e+01]\n",
      " [-5.07543654e+09 -2.29292749e+03 -1.49135025e+02 ...  8.66392517e+01\n",
      "   1.11564789e+02  1.06956917e+02]\n",
      " [ 5.58446592e+09  3.02259229e+03 -6.31376152e+01 ... -6.52709351e+01\n",
      "   8.27302551e+01 -1.01073837e+02]]\n",
      "7.025824652777778% mismatch in attention input grads\n",
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "-- W2 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.mlp.down_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_layers_9_feed_forward_w2_shard_0_output_0\n",
      "HF: [-6.33203254e+13 -4.43651289e+13  6.35509366e+13 ...  1.08435585e+02\n",
      "  9.42303467e+01  5.89958420e+01]\n",
      "FF:[-6.33203296e+13 -4.43651289e+13  6.35509408e+13 ...  1.08435623e+02\n",
      "  9.42303467e+01  5.89958954e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   26    51    66    85   259   262   272   296   298   329   392   415\n",
      "   428   482   492   514   526   531   671   731   763   777   893   927\n",
      "   984  1105  1184  1206  1418  1541  1548  1572  1577  1613  1619  1643\n",
      "  1658  1661  1691  1701  1706  1726  1757  1784  1815  1833  1849  1856\n",
      "  1880  1891  1921  1956  1969  2012  2021  2028  2030  2059  2065  2144\n",
      "  2149  2183  2210  2238  2292  2342  2357  2384  2414  2495  2531  2565\n",
      "  2597  2662  2713  2781  2821  2829  2877  2904  2921  2927  2962  2973\n",
      "  3044  3066  3094  3100  3106  3159  3193  3251  3377  3389  3397  3427\n",
      "  3436  3570  3594  3703  3729  3770  3772  3780  3811  3840  3842  3860\n",
      "  3907  3920  3929  3946  3955  3969  4005  4009  4034  4048  4077  4089\n",
      "  4104  4129  4134  4178  4202  4212  4219  4239  4245  4256  4273  4373\n",
      "  4407  4463  4464  4465  4481  4511  4537  4541  4543  4549  4597  4599\n",
      "  4633  4759  4760  4789  4846  4884  4901  4930  4954  4971  4993  5024\n",
      "  5030  5041  5050  5116  5130  5163  5207  5224  5282  5313  5322  5349\n",
      "  5363  5403  5410  5412  5454  5543  5581  5590  5654  5673  5784  5821\n",
      "  5849  5880  5911  5917  5982  6000  6062  6165  6178  6193  6200  6272\n",
      "  6322  6351  6366  6376  6380  6382  6393  6412  6420  6430  6433  6446\n",
      "  6476  6482  6488  6490  6519  6527  6540  6556  6563  6567  6577  6600\n",
      "  6619  6680  6709  6735  6768  6777  6780  6823  6825  6826  6830  6863\n",
      "  6880  6912  6988  7006  7030  7071  7077  7102  7123  7244  7264  7367\n",
      "  7389  7390  7434  7451  7452  7455  7505  7532  7539  7589  7598  7620\n",
      "  7651  7653  7659  7709  7714  7740  7751  7759  7803  7808  7820  7917\n",
      "  7923  7926  7949  7962  7966  7978  8002  8004  8040  8050  8052  8068\n",
      "  8180  8223  8250  8253  8265  8341  8344  8375  8376  8386  8449  8468\n",
      "  8501  8509  8522  8535  8585  8590  8593  8642  8657  8674  8687  8707\n",
      "  8714  8726  8729  8737  8756  8769  8801  8846  8850  8865  8907  8998\n",
      "  9018  9043  9059  9066  9083  9093  9098  9130  9131  9165  9189  9216\n",
      "  9285  9337  9368  9526  9539  9563  9620  9659  9723  9793  9804  9817\n",
      "  9820  9827  9908  9995 10053 10128 10135 10143 10205 10253 10274 10292\n",
      " 10300 10311 10327 10356 10406 10441 10491 10494 10551 10562 10563 10634\n",
      " 10649 10674 10710 10734 10821 10831 10833 10838 10845 10911 10966 10981\n",
      " 10988 10990 10998 11008 11044 11049 11100 11127 11141 11197 11250 11269\n",
      " 11285 11308 11361 11383 11437 11460 11494 11502 11511 11522 11546 11557\n",
      " 11564 11588 11649 11658 11671 11674 11703 11729 11749 11759 11832 11892\n",
      " 11979 11988 12000 12038 12063 12078 12107 12119 12165 12259 12269 12270\n",
      " 12347 12369 12386 12415 12475 12518 12566 12569 12574 12652 12693 12792\n",
      " 12833 12834 12852 12872 12900 12946 13117 13121 13124 13321 13345 13357\n",
      " 13427 13431 13446 13473 13526 13635 13638 13662 13706 13733 13803 13807\n",
      " 13852 13882 13912 13924 13962 13969 13986 14023 14036 14046 14085 14110\n",
      " 14130 14141 14175 14183 14191 14220 14222 14223 14285 14310 14331 14336\n",
      " 14354 14375 14425 14427 14451 14482 14493 14516 14560 14563 14581 14623\n",
      " 14671 14677 14679 14680 14685 14688 14742 14799 14860 14868 14870 14872\n",
      " 14900 14909 14916 14940 14964 14991 15003 15023 15027 15033 15038 15051\n",
      " 15086 15100 15184 15214 15232 15290 15352 15363 15365 15407 15433 15451\n",
      " 15522 15577 15707 15720 15725 15739 15830 15837 15875 15937 15965 15985\n",
      " 16017 16054 16113 16136 16142 16169 16191 16232 16238 16250 16268 16282\n",
      " 16285 16290 16295 16304 16327 16334 16353 16356 16363 16382 16403 16407\n",
      " 16408 16409 16458 16459 16495 16497 16499 16500 16516 16532 16595 16603\n",
      " 16611 16657 16678 16680 16695 16701 16704 16754 16768 16807 16818 16856\n",
      " 16870 16951 16971 16986 16989 16992 17048 17134 17181 17208 17217 17236\n",
      " 17243 17319 17363 17398 17448 17471 17497 17557 17646 17654 17659 17692\n",
      " 17754 17947 17957 17969 17975 18029 18128 18146 18196 18206 18207 18250\n",
      " 18265 18313 18406]\n",
      "Ok!\n",
      "Ok!\n",
      "-- Lora --\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.mlp.down_proj.lora_B.default.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_layers_9_feed_forward_w2_lora_shard_0_output_0\n",
      "HF: [-6.33203254e+13 -4.43651289e+13  6.35509366e+13 ...  1.08435585e+02\n",
      "  9.42303467e+01  5.89958420e+01]\n",
      "FF:[-6.33203296e+13 -4.43651289e+13  6.35509408e+13 ...  1.08435623e+02\n",
      "  9.42303467e+01  5.89958954e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 26  51  66  85 259 262 272 296 298 329 392 415 428 482 492 514 526 531\n",
      " 671 731 763]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.mlp.down_proj.lora_A.default.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_layers_9_feed_forward_w2_lora_shard_0_input_0\n",
      "HF: [ 5.0590863e+10  3.7823513e+11 -5.0394451e+11 ... -5.5814421e-01\n",
      "  2.2970559e-01 -1.2293311e+00]\n",
      "FF:[ 5.05906831e+10  3.78235290e+11 -5.03944544e+11 ... -5.58144033e-01\n",
      "  2.29705781e-01 -1.22933090e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 189  254  317  418  515  546  577  634  636  675  712  808 1011 1030\n",
      " 1080 1091 1132 1168 1254 1265 1285 1287 1354 1381 1427 1459 1506 1620\n",
      " 1654 1752 1887 1897 1900 1937 1981 1985 1986 2003 2029 2152 2181 2295\n",
      " 2395 2426 2445 2673 2687 2859 2947 2977 3037]\n",
      "Ok!\n",
      "-- W2/W1/W3 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_SigmoidSiluMulti_shard_0_output_0\n",
      "HF: [ 2.5211001e+13 -5.6630301e+13 -2.3639437e+13 ... -4.6000423e+01\n",
      "  1.2655228e+01  7.1020460e+00]\n",
      "FF:[ 2.52109673e+13 -5.66302930e+13 -2.36394182e+13 ... -4.60003510e+01\n",
      "  1.26551876e+01  7.10206795e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   9   49  113  174  243  267  271  288  323  335  397  399  438  439\n",
      "  457  475  506  568  569  652  680  689  715  735  739  758  766  777\n",
      "  785  837  842  852  865  884  893  919  930  932  936  939  957 1018\n",
      " 1095 1105 1112 1114 1129 1168 1217 1220 1229 1230 1233 1237 1283 1304\n",
      " 1354 1453 1532 1542 1547 1550 1592 1597 1603 1615 1647 1679 1698 1699\n",
      " 1712 1770 1819 1835 1875 1977 2007 2016 2039 2066 2078 2102 2153 2245\n",
      " 2403 2447 2621 2698 2704 2728 2736 2743 2774 2792 2836 2858 2870 2881\n",
      " 2932 2948 3018 3034 3066]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_layers_9_feed_forward_w2_shard_0_input_0\n",
      "HF: [ 2.5211001e+13 -5.6630301e+13 -2.3639437e+13 ... -4.6000423e+01\n",
      "  1.2655228e+01  7.1020460e+00]\n",
      "FF:[ 2.52109673e+13 -5.66302930e+13 -2.36394182e+13 ... -4.60003510e+01\n",
      "  1.26551876e+01  7.10206795e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   9   49  113  174  243  267  271  288  323  335  397  399  438  439\n",
      "  457  475  506  568  569  652  680  689  715  735  739  758  766  777\n",
      "  785  837  842  852  865  884  893  919  930  932  936  939  957 1018\n",
      " 1095 1105 1112 1114 1129 1168 1217 1220 1229 1230 1233 1237 1283 1304\n",
      " 1354 1453 1532 1542 1547 1550 1592 1597 1603 1615 1647 1679 1698 1699\n",
      " 1712 1770 1819 1835 1875 1977 2007 2016 2039 2066 2078 2102 2153 2245\n",
      " 2403 2447 2621 2698 2704 2728 2736 2743 2774 2792 2836 2858 2870 2881\n",
      " 2932 2948 3018 3034 3066]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "-- Attention --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.self_attn.o_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_layers_9_attention_shard_0_output_0\n",
      "HF: [-6.3320325e+13 -4.4365129e+13  6.3550937e+13 ...  7.2449814e+01\n",
      "  8.6617142e+01  8.3981407e+01]\n",
      "FF:[-6.33203296e+13 -4.43651289e+13  6.35509408e+13 ...  7.24498901e+01\n",
      "  8.66170959e+01  8.39814606e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 26  51  66  85 259 262 272 296 298 329 392 415 428 482 492 514 526 531\n",
      " 671 731 763]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.9.self_attn.o_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_9_layers_9_attention_shard_0_o_proj_in_grad\n",
      "HF: [ 7.2885461e+13 -6.0835821e+13 -7.9732612e+13 ...  2.5297220e+02\n",
      " -8.1722275e+01 -7.0014725e+01]\n",
      "FF:[ 7.28854608e+13 -6.08357832e+13 -7.97326201e+13 ...  2.52972260e+02\n",
      " -8.17222137e+01 -7.00146637e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[  6  36  43  55  60  82 101 110 117 217 221 229 236 256 289 392 421 429\n",
      " 433 454 486 518 523 565 568 629 639 648 707 725 744]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[ 7.28854608e+13  6.37500977e+02  2.96775421e+02 ...  8.35403061e+01\n",
      "   1.72460327e+02  2.90482426e+01]\n",
      " [-6.08358210e+13 -5.23222847e+01 -2.34542664e+02 ... -1.87500763e+01\n",
      "  -8.99429398e+01  8.64021378e+01]\n",
      " [-7.97326117e+13 -4.24736328e+02 -1.82208099e+02 ...  3.21808720e+00\n",
      "  -5.87415466e+01 -2.08511108e+02]\n",
      " ...\n",
      " [-1.13411917e+14 -3.48418640e+02  1.52205795e+02 ...  1.51519928e+02\n",
      "   2.45651031e+02  2.52972198e+02]\n",
      " [-3.75985275e+12  2.39696625e+02  1.51989685e+02 ... -2.85605354e+01\n",
      "  -1.79121232e+00 -8.17222748e+01]\n",
      " [ 1.11016038e+14 -1.96372967e+01 -1.27668396e+02 ...  3.35008011e+01\n",
      "  -7.46116943e+01 -7.00147247e+01]]\n",
      "FF:[[ 7.28854608e+13  6.37500977e+02  2.96775513e+02 ...  8.35403976e+01\n",
      "   1.72460068e+02  2.90483646e+01]\n",
      " [-6.08357832e+13 -5.23225098e+01 -2.34542755e+02 ... -1.87501526e+01\n",
      "  -8.99431992e+01  8.64022217e+01]\n",
      " [-7.97326201e+13 -4.24736572e+02 -1.82207733e+02 ...  3.21793270e+00\n",
      "  -5.87416573e+01 -2.08511139e+02]\n",
      " ...\n",
      " [-1.13411925e+14 -3.48418640e+02  1.52205902e+02 ...  1.51519714e+02\n",
      "   2.45650864e+02  2.52972260e+02]\n",
      " [-3.75988630e+12  2.39696686e+02  1.51989319e+02 ... -2.85606136e+01\n",
      "  -1.79138493e+00 -8.17222137e+01]\n",
      " [ 1.11016046e+14 -1.96372318e+01 -1.27668480e+02 ...  3.35009079e+01\n",
      "  -7.46116791e+01 -7.00146637e+01]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[  6  36  43  55  60  82 101 110 117 217 221 229 236 256 289 392 421 429\n",
      " 433 454 486 518 523 565 568 629 639 648 707 725 744]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[ 7.2885461e+13 -6.0835821e+13 -7.9732612e+13 ... -1.1341192e+14\n",
      "  -3.7598527e+12  1.1101604e+14]\n",
      " [ 3.3241980e+03 -6.3044128e+02 -3.0447307e+03 ...  3.0137921e+02\n",
      "   3.8262988e+02 -4.2889914e+02]\n",
      " [ 3.5639046e+01 -1.6155790e+01 -2.4461178e+01 ...  2.7450909e+02\n",
      "   1.6181946e+02 -2.5407137e+02]\n",
      " ...\n",
      " [ 4.6487908e+00 -9.6633381e-01 -2.7078497e-01 ...  3.6374569e+01\n",
      "  -1.7563061e+00 -7.1206141e+00]\n",
      " [ 1.8901447e+00  8.9006472e-01 -4.3125896e+00 ...  2.6014965e+01\n",
      "  -3.7720141e-01 -7.8855257e+00]\n",
      " [ 1.9513500e+00  5.8041654e+00 -1.4006979e+01 ...  7.2743622e+01\n",
      "  -2.3499712e+01 -2.0133139e+01]]\n",
      "FF:[[ 7.28854608e+13 -6.08357832e+13 -7.97326201e+13 ... -1.13411925e+14\n",
      "  -3.75988630e+12  1.11016046e+14]\n",
      " [ 3.32419922e+03 -6.30442505e+02 -3.04472998e+03 ...  3.01379364e+02\n",
      "   3.82629669e+02 -4.28898712e+02]\n",
      " [ 3.56390572e+01 -1.61558037e+01 -2.44611683e+01 ...  2.74509308e+02\n",
      "   1.61819229e+02 -2.54071594e+02]\n",
      " ...\n",
      " [ 4.64879847e+00 -9.66338813e-01 -2.70792574e-01 ...  3.63745117e+01\n",
      "  -1.75632846e+00 -7.12060070e+00]\n",
      " [ 1.89013767e+00  8.90062451e-01 -4.31257772e+00 ...  2.60149212e+01\n",
      "  -3.77217919e-01 -7.88551569e+00]\n",
      " [ 1.95135939e+00  5.80417490e+00 -1.40069904e+01 ...  7.27435226e+01\n",
      "  -2.34996586e+01 -2.01330910e+01]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Ok!\n",
      "7.609953703703703% mismatch in QK prods softmax out grad\n",
      "Ok!\n",
      "hf_attn_in:  (768, 24)\n",
      "[[-1.17282076e+14 -2.12461621e+03  8.80099030e+01 ...  4.34470520e+01\n",
      "   7.55885468e+01 -2.88791332e+01]\n",
      " [-2.07757936e+14 -3.81796265e+02 -2.33774780e+02 ...  8.11984329e+01\n",
      "  -4.41825638e+01  7.35064125e+00]\n",
      " [ 4.11484165e+13  2.50572113e+02  1.91601822e+02 ...  1.00269365e+01\n",
      "  -3.41638985e+01  1.20433075e+02]\n",
      " ...\n",
      " [ 7.95562329e+13  1.55007373e+03  1.70351212e+02 ... -1.80320053e+01\n",
      "   8.77533417e+01  2.14678173e+01]\n",
      " [-1.86546485e+14 -5.18847070e+03 -3.34331085e+02 ...  2.51586838e+01\n",
      "  -4.06135368e+01 -6.27860641e+00]\n",
      " [ 1.89751705e+14 -3.09853809e+03 -1.18278351e+01 ... -1.24640663e+02\n",
      "   1.59719009e+01 -6.47173615e+01]]\n",
      "ff_attn_in:  (768, 24)\n",
      "[[-1.17282034e+14 -2.12461694e+03  8.80101547e+01 ...  4.34468918e+01\n",
      "   7.55886002e+01 -2.88791542e+01]\n",
      " [-2.07757920e+14 -3.81795776e+02 -2.33774765e+02 ...  8.11985397e+01\n",
      "  -4.41825829e+01  7.35066986e+00]\n",
      " [ 4.11484543e+13  2.50570099e+02  1.91601196e+02 ...  1.00270777e+01\n",
      "  -3.41638451e+01  1.20433121e+02]\n",
      " ...\n",
      " [ 7.95562413e+13  1.55007288e+03  1.70350784e+02 ... -1.80321960e+01\n",
      "   8.77533112e+01  2.14678249e+01]\n",
      " [-1.86546469e+14 -5.18847070e+03 -3.34331268e+02 ...  2.51588135e+01\n",
      "  -4.06132622e+01 -6.27861023e+00]\n",
      " [ 1.89751521e+14 -3.09853711e+03 -1.18275299e+01 ... -1.24640862e+02\n",
      "   1.59719791e+01 -6.47173767e+01]]\n",
      "7.530381944444445% mismatch in attention input grads\n",
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "-- W2 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.mlp.down_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_layers_8_feed_forward_w2_shard_0_output_0\n",
      "HF: [-1.3223293e+17 -2.3794983e+17  4.7027590e+16 ...  7.7873253e+01\n",
      "  8.6085976e+01  6.8200005e+01]\n",
      "FF:[-1.32232886e+17 -2.37949812e+17  4.70276284e+16 ...  7.78733292e+01\n",
      "  8.60859299e+01  6.82000580e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[    3    24    66    71    94    95   124   134   141   150   163   181\n",
      "   226   261   284   318   320   378   382   385   391   395   403   422\n",
      "   434   495   515   523   524   549   579   610   644   710   764   772\n",
      "   870   984   987  1045  1249  1330  1362  1489  1517  1550  1556  1588\n",
      "  1595  1659  1672  1684  1689  1768  1792  1799  1808  1818  1842  1871\n",
      "  1889  1899  1910  1915  1925  1936  1993  1997  2033  2041  2059  2062\n",
      "  2066  2098  2111  2124  2129  2130  2146  2153  2159  2166  2197  2206\n",
      "  2210  2212  2222  2234  2237  2320  2321  2357  2359  2362  2385  2428\n",
      "  2518  2539  2553  2568  2598  2683  2689  2694  2711  2714  2733  2787\n",
      "  2788  2795  2811  2815  2853  2881  2890  2917  2981  2997  3021  3037\n",
      "  3089  3149  3163  3191  3196  3217  3225  3248  3277  3287  3292  3305\n",
      "  3327  3361  3385  3402  3417  3425  3456  3479  3516  3521  3528  3555\n",
      "  3587  3599  3608  3684  3702  3733  3770  3779  3819  3822  3823  3898\n",
      "  3921  3942  3950  4012  4053  4077  4086  4091  4139  4185  4198  4225\n",
      "  4241  4296  4347  4349  4368  4403  4407  4418  4453  4471  4472  4473\n",
      "  4494  4537  4549  4555  4558  4598  4623  4648  4666  4698  4729  4782\n",
      "  4848  4866  4886  4943  4959  5008  5010  5012  5057  5079  5177  5178\n",
      "  5186  5211  5271  5281  5296  5313  5328  5356  5364  5409  5429  5440\n",
      "  5453  5455  5457  5476  5529  5563  5591  5621  5625  5631  5654  5661\n",
      "  5692  5705  5720  5740  5751  5758  5787  5799  5813  5835  5836  5867\n",
      "  5872  5893  5953  5974  5980  5982  6000  6055  6082  6086  6102  6107\n",
      "  6123  6159  6172  6193  6220  6230  6231  6263  6286  6297  6362  6396\n",
      "  6401  6430  6436  6485  6497  6499  6502  6510  6537  6554  6555  6563\n",
      "  6564  6579  6586  6598  6615  6625  6626  6649  6651  6661  6754  6764\n",
      "  6776  6852  6863  6874  6883  6892  6913  6945  6969  7036  7057  7066\n",
      "  7082  7138  7147  7150  7157  7197  7202  7231  7234  7235  7240  7270\n",
      "  7278  7287  7322  7327  7345  7348  7361  7390  7402  7490  7539  7573\n",
      "  7610  7714  7721  7758  7794  7812  7827  7829  7837  7839  7882  7894\n",
      "  7943  7948  7952  7969  7975  7996  8024  8027  8037  8043  8055  8078\n",
      "  8079  8088  8090  8095  8154  8258  8264  8283  8297  8313  8329  8336\n",
      "  8359  8361  8376  8383  8416  8421  8428  8454  8475  8502  8521  8613\n",
      "  8642  8653  8696  8756  8764  8777  8791  8837  8849  8859  8878  8955\n",
      "  8991  8997  9006  9012  9040  9066  9093  9097  9098  9131  9158  9162\n",
      "  9165  9214  9216  9280  9297  9301  9316  9355  9371  9412  9421  9475\n",
      "  9510  9580  9620  9645  9696  9713  9732  9768  9802  9817  9819  9826\n",
      "  9839  9846  9947 10004 10062 10065 10072 10103 10107 10108 10138 10167\n",
      " 10173 10228 10262 10292 10326 10356 10360 10372 10421 10446 10466 10468\n",
      " 10499 10505 10513 10517 10589 10606 10612 10645 10664 10669 10726 10777\n",
      " 10835 10838 10839 10848 10855 10877 10897 10941 10963 10971 10977 10997\n",
      " 11030 11060 11065 11076 11088 11140 11167 11174 11231 11252 11257 11259\n",
      " 11275 11297 11302 11319 11331 11333 11357 11358 11380 11382 11402 11423\n",
      " 11446 11447 11500 11501 11522 11585 11623 11670 11728 11736 11759 11761\n",
      " 11772 11785 11839 11894 11916 11924 11936 11962 11968 11969 11977 11984\n",
      " 12008 12030 12054 12074 12123 12175 12182 12194 12237 12262 12282 12285\n",
      " 12341 12348 12351 12370 12376 12386 12399 12449 12507 12513 12518 12522\n",
      " 12549 12572 12643 12648 12663 12689 12696 12710 12769 12780 12788 12792\n",
      " 12793 12852 12864 12879 12884 12985 13018 13041 13057 13176 13264 13272\n",
      " 13274 13275 13292 13303 13333 13379 13427 13428 13442 13451 13454 13500\n",
      " 13510 13533 13564 13588 13607 13640 13655 13686 13687 13688 13732 13747\n",
      " 13786 13801 13803 13826 13841 13846 13850 13892 13909 13946 14036 14040\n",
      " 14046 14060 14080 14152 14161 14183 14195 14210 14240 14278 14331 14354\n",
      " 14370 14372 14386 14395 14409 14432 14434 14497 14506 14531 14559 14589\n",
      " 14648 14663 14686 14698 14715 14743 14757 14799 14808 14810 14849 14893\n",
      " 14902 14929 14937 14947 14953 14958 15005 15012 15018 15036 15066 15069\n",
      " 15083 15152 15154 15196 15197 15212 15292 15309 15323 15340 15343 15375\n",
      " 15389 15396 15408 15410 15454 15499 15532 15557 15605 15647 15677 15736\n",
      " 15745 15756 15769 15809 15824 15876 15882 15900 15906 15941 16027 16030\n",
      " 16040 16116 16190 16192 16205 16207 16239 16279 16285 16295 16348 16358\n",
      " 16367 16384 16386 16394 16399 16455 16457 16458 16471 16495 16500 16502\n",
      " 16520 16541 16542 16598 16623 16643 16651 16665 16673 16679 16713 16725\n",
      " 16734 16736 16739 16751 16756 16768 16861 16870 16939 16976 17007 17028\n",
      " 17040 17069 17087 17108 17125 17139 17151 17158 17174 17175 17178 17182\n",
      " 17189 17221 17258 17341 17360 17370 17381 17395 17396 17415 17432 17450\n",
      " 17463 17470 17472 17473 17496 17507 17536 17608 17626 17627 17649 17653\n",
      " 17664 17771 17815 17822 17831 17864 17883 17931 17994 17999 18035 18174\n",
      " 18209 18250 18274 18307 18327 18403 18423]\n",
      "Ok!\n",
      "Ok!\n",
      "-- Lora --\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.mlp.down_proj.lora_B.default.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_layers_8_feed_forward_w2_lora_shard_0_output_0\n",
      "HF: [-1.3223293e+17 -2.3794983e+17  4.7027590e+16 ...  7.7873253e+01\n",
      "  8.6085976e+01  6.8200005e+01]\n",
      "FF:[-1.32232886e+17 -2.37949812e+17  4.70276284e+16 ...  7.78733292e+01\n",
      "  8.60859299e+01  6.82000580e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[  3  24  66  71  94  95 124 134 141 150 163 181 226 261 284 318 320 378\n",
      " 382 385 391 395 403 422 434 495 515 523 524 549 579 610 644 710 764]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.mlp.down_proj.lora_A.default.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_layers_8_feed_forward_w2_lora_shard_0_input_0\n",
      "HF: [ 6.5550952e+14  4.9376585e+14  3.8510841e+14 ...  1.6802770e+00\n",
      " -1.1248941e+00 -1.1701980e+00]\n",
      "FF:[ 6.55509317e+14  4.93765882e+14  3.85108377e+14 ...  1.68027747e+00\n",
      " -1.12489426e+00 -1.17019880e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[   6   79  111  149  155  168  187  195  220  223  252  261  329  343\n",
      "  347  369  386  392  403  438  439  450  461  524  535  643  656  659\n",
      "  661  668  722  727  732  742  754  801  816  820  835  837  849  850\n",
      "  978  993  997 1012 1019 1034 1044 1071 1088 1094 1114 1135 1151 1170\n",
      " 1190 1212 1273 1275 1277 1289 1290 1308 1311 1337 1364 1379 1394 1430\n",
      " 1454 1460 1469 1474 1703 1725 1728 1732 1733 1741 1754 1757 1804 1806\n",
      " 1856 1862 1932 1945 1996 2030 2044 2045 2065 2071 2075 2094 2149 2152\n",
      " 2163 2180 2182 2215 2254 2357 2362 2370 2392 2398 2428 2484 2519 2521\n",
      " 2524 2582 2618 2641 2645 2664 2674 2681 2691 2735 2747 2779 2872 2899\n",
      " 2909 2935 2957 3000 3033]\n",
      "Ok!\n",
      "-- W2/W1/W3 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_SigmoidSiluMulti_shard_0_output_0\n",
      "HF: [-1.3871785e+17 -8.3164397e+16  4.9509505e+16 ...  4.3806694e+01\n",
      "  9.4386072e+00 -2.4460859e+01]\n",
      "FF:[-1.38717840e+17 -8.31644654e+16  4.95094495e+16 ...  4.38065948e+01\n",
      "  9.43864822e+00 -2.44608364e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[  80   83  172  173  176  184  215  285  329  338  341  395  403  465\n",
      "  468  565  572  601  614  636  639  651  660  749  750  806  828  844\n",
      "  873  952  971  988  992 1014 1082 1083 1085 1123 1152 1195 1200 1227\n",
      " 1391 1397 1462 1546 1548 1563 1584 1629 1704 1706 1759 1764 1820 1833\n",
      " 1851 1857 1864 1899 1929 1943 1958 1967 1980 1985 2002 2030 2069 2076\n",
      " 2120 2127 2130 2157 2180 2187 2195 2212 2243 2249 2256 2299 2393 2505\n",
      " 2516 2525 2546 2562 2604 2702 2712 2731 2745 2764 2789 2821 2873 2915\n",
      " 2936 2945 2951 3013 3016]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_layers_8_feed_forward_w2_shard_0_input_0\n",
      "HF: [-1.3871785e+17 -8.3164397e+16  4.9509505e+16 ...  4.3806694e+01\n",
      "  9.4386072e+00 -2.4460859e+01]\n",
      "FF:[-1.38717840e+17 -8.31644654e+16  4.95094495e+16 ...  4.38065948e+01\n",
      "  9.43864822e+00 -2.44608364e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[  80   83  172  173  176  184  215  285  329  338  341  395  403  465\n",
      "  468  565  572  601  614  636  639  651  660  749  750  806  828  844\n",
      "  873  952  971  988  992 1014 1082 1083 1085 1123 1152 1195 1200 1227\n",
      " 1391 1397 1462 1546 1548 1563 1584 1629 1704 1706 1759 1764 1820 1833\n",
      " 1851 1857 1864 1899 1929 1943 1958 1967 1980 1985 2002 2030 2069 2076\n",
      " 2120 2127 2130 2157 2180 2187 2195 2212 2243 2249 2256 2299 2393 2505\n",
      " 2516 2525 2546 2562 2604 2702 2712 2731 2745 2764 2789 2821 2873 2915\n",
      " 2936 2945 2951 3013 3016]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "-- Attention --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.self_attn.o_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_layers_8_attention_shard_0_output_0\n",
      "HF: [-1.3223293e+17 -2.3794983e+17  4.7027590e+16 ...  3.5121140e+01\n",
      " -3.5587997e+00  9.5641022e+01]\n",
      "FF:[-1.32232886e+17 -2.37949812e+17  4.70276284e+16 ...  3.51211472e+01\n",
      " -3.55898285e+00  9.56410980e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[  3  24  66  71  94  95 124 134 141 150 163 181 226 261 284 318 320 378\n",
      " 382 385 391 395 403 422 434 495 515 523 524 549 579 610 644 710 764]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.8.self_attn.o_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_8_layers_8_attention_shard_0_o_proj_in_grad\n",
      "HF: [-1.6186993e+17 -3.5698813e+17  3.4442975e+16 ... -2.5844165e+02\n",
      "  2.0677340e+01 -2.4573349e+01]\n",
      "FF:[-1.61869621e+17 -3.56988336e+17  3.44430865e+16 ... -2.58441467e+02\n",
      "  2.06775093e+01 -2.45735531e+01]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ 93  99 114 137 141 142 160 193 235 259 269 299 307 316 350 364 400 523\n",
      " 608 702 720 731 759]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[-1.6186993e+17 -2.1968115e+02  8.5754425e+01 ... -6.9909119e+01\n",
      "  -2.6478451e+01 -7.4195160e+01]\n",
      " [-3.5698813e+17  3.9582391e+02  5.5431940e+02 ...  1.9529277e+02\n",
      "   1.2558211e+02  6.7965935e+01]\n",
      " [ 3.4442975e+16  2.8310864e+02 -8.1522171e+01 ... -2.3606525e+01\n",
      "  -2.0410315e+01 -1.5228156e+02]\n",
      " ...\n",
      " [ 4.0923264e+16 -2.4507169e+02 -8.2614380e+02 ... -2.6583340e+02\n",
      "  -1.9878247e+02 -2.5844165e+02]\n",
      " [ 6.9156258e+17  1.3969666e+02 -7.5639044e+02 ... -1.5231053e+02\n",
      "  -3.3650037e+02  2.0677340e+01]\n",
      " [ 9.9511712e+16 -3.2348724e+01  3.0624988e+02 ...  1.0391423e+02\n",
      "   6.0626881e+01 -2.4573349e+01]]\n",
      "FF:[[-1.61869621e+17 -2.19681122e+02  8.57541504e+01 ... -6.99092026e+01\n",
      "  -2.64783611e+01 -7.41952515e+01]\n",
      " [-3.56988336e+17  3.95823853e+02  5.54319275e+02 ...  1.95292725e+02\n",
      "   1.25582062e+02  6.79659348e+01]\n",
      " [ 3.44430865e+16  2.83108551e+02 -8.15224686e+01 ... -2.36064014e+01\n",
      "  -2.04101429e+01 -1.52281570e+02]\n",
      " ...\n",
      " [ 4.09233933e+16 -2.45071564e+02 -8.26143555e+02 ... -2.65833405e+02\n",
      "  -1.98782272e+02 -2.58441467e+02]\n",
      " [ 6.91562577e+17  1.39696579e+02 -7.56390808e+02 ... -1.52310455e+02\n",
      "  -3.36500092e+02  2.06775093e+01]\n",
      " [ 9.95114373e+16 -3.23486938e+01  3.06250122e+02 ...  1.03914482e+02\n",
      "   6.06264191e+01 -2.45735531e+01]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[ 93  99 114 137 141 142 160 193 235 259 269 299 307 316 350 364 400 523\n",
      " 608 702 720 731 759]\n",
      "Ok!\n",
      "mismatch between hf_tensor and ff_tensor\n",
      "HF: [[-1.6186993e+17 -3.5698813e+17  3.4442975e+16 ...  4.0923264e+16\n",
      "   6.9156258e+17  9.9511712e+16]\n",
      " [-5.3483575e+02  2.6249797e+03 -6.7268573e+02 ... -6.1204077e+03\n",
      "  -4.3047915e+03 -9.5139771e+01]\n",
      " [-1.2200641e+01  1.0347147e+02 -2.6777636e+01 ... -1.4766699e+02\n",
      "  -9.8514114e+01  1.2616925e+01]\n",
      " ...\n",
      " [-3.2097631e+00  9.1431990e+00 -1.6333975e+00 ... -6.9996667e+00\n",
      "  -6.4008064e+00  1.9126304e+00]\n",
      " [-3.0982289e+00  1.2355285e+01 -3.1715555e+00 ... -4.6754313e+00\n",
      "  -6.2553053e+00  1.0515085e+00]\n",
      " [-2.9516125e+00  2.7038031e+00 -6.0580249e+00 ... -1.6555168e+01\n",
      "   1.3245420e+00 -1.5741113e+00]]\n",
      "FF:[[-1.61869621e+17 -3.56988336e+17  3.44430865e+16 ...  4.09233933e+16\n",
      "   6.91562577e+17  9.95114373e+16]\n",
      " [-5.34834961e+02  2.62497900e+03 -6.72686401e+02 ... -6.12040576e+03\n",
      "  -4.30479297e+03 -9.51402283e+01]\n",
      " [-1.22006664e+01  1.03471611e+02 -2.67777309e+01 ... -1.47666946e+02\n",
      "  -9.85141525e+01  1.26169167e+01]\n",
      " ...\n",
      " [-3.20977211e+00  9.14321709e+00 -1.63339353e+00 ... -6.99966621e+00\n",
      "  -6.40081263e+00  1.91262615e+00]\n",
      " [-3.09821057e+00  1.23552399e+01 -3.17152786e+00 ... -4.67541933e+00\n",
      "  -6.25528765e+00  1.05149710e+00]\n",
      " [-2.95161533e+00  2.70380235e+00 -6.05802393e+00 ... -1.65551491e+01\n",
      "   1.32455230e+00 -1.57412362e+00]]\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Ok!\n",
      "8.101851851851851% mismatch in QK prods softmax out grad\n",
      "Ok!\n",
      "hf_attn_in:  (768, 24)\n",
      "[[-7.3778828e+16  1.0956941e+03  1.1773144e+02 ... -4.0466427e+01\n",
      "  -3.1198654e+01 -1.7603550e+01]\n",
      " [-1.2087128e+18  6.9384756e+03  6.1327003e+01 ...  1.5329468e+01\n",
      "   7.6757736e+00 -4.5589094e+00]\n",
      " [-6.7892266e+17  5.4895034e+03  7.6927376e+01 ...  9.1396770e+00\n",
      "   2.3195824e+01 -6.1995559e+00]\n",
      " ...\n",
      " [ 2.6452032e+17  9.9761787e+03  2.2349066e+02 ...  5.7504387e+01\n",
      "  -8.6791611e-01  4.6890911e+01]\n",
      " [-6.7528534e+16  3.3856902e+03  2.5189743e+02 ...  2.2824722e+01\n",
      "   8.7917282e+01 -2.1569672e+01]\n",
      " [-2.1779064e+17  5.2511855e+03  6.6282043e+01 ...  9.9689598e+00\n",
      "  -5.5022659e+00 -3.2573143e+01]]\n",
      "ff_attn_in:  (768, 24)\n",
      "[[-7.37791458e+16  1.09569678e+03  1.17731285e+02 ... -4.04664154e+01\n",
      "  -3.11988506e+01 -1.76035423e+01]\n",
      " [-1.20871251e+18  6.93847900e+03  6.13275528e+01 ...  1.53295393e+01\n",
      "   7.67594433e+00 -4.55900288e+00]\n",
      " [-6.78922523e+17  5.48950342e+03  7.69272308e+01 ...  9.13961220e+00\n",
      "   2.31957569e+01 -6.19959354e+00]\n",
      " ...\n",
      " [ 2.64520284e+17  9.97617871e+03  2.23490509e+02 ...  5.75044785e+01\n",
      "  -8.67943764e-01  4.68908234e+01]\n",
      " [-6.75287400e+16  3.38569165e+03  2.51897339e+02 ...  2.28247147e+01\n",
      "   8.79171448e+01 -2.15696106e+01]\n",
      " [-2.17790679e+17  5.25118652e+03  6.62821960e+01 ...  9.96885872e+00\n",
      "  -5.50213098e+00 -3.25731125e+01]]\n",
      "9.809027777777777% mismatch in attention input grads\n",
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "-- W2 --\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.7.mlp.down_proj.go_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/bwd_step_0_layers_7_layers_7_feed_forward_w2_shard_0_output_0\n",
      "HF: [-7.5522525e+19 -1.3283726e+21 -7.2549753e+20 ...  4.9017162e+01\n",
      " -9.7436657e+00  8.5870697e+01]\n",
      "FF:[-7.55228501e+19 -1.32837218e+21 -7.25497390e+20 ...  4.90171394e+01\n",
      " -9.74382782e+00  8.58707886e+01]\n",
      "[ True  True  True ...  True False  True]\n",
      "[   19    64    75 ... 18418 18428 18430]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHuggingface-FlexFlow checks:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- W2 --\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mcompare_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_BWD_w2_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_BWD_w2_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m compare_tensors(hf_w2_weight, ff_w2_weight, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- Lora --\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/FlexFlow/tests/peft/align_test_utils.py:47\u001b[0m, in \u001b[0;36mcompare_tensors\u001b[0;34m(hf_tensor_filepath, ff_tensor_filepath, tolerance)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mismatches)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#print(np.nonzero(hf_tensor)[0])\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(ff_tensor[36], hf_tensor[36])\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(mismatches) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.05\u001b[39m\u001b[38;5;241m*\u001b[39mlen_hf_tensor)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOk!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "attention_tests=True\n",
    "for i in range(tot_num_layers-1, -1, -1):\n",
    "    # HuggingFace filepaths\n",
    "    hf_BWD_norm_in = f\"{hf_path}/bwd_step_0_norm.gi_0\"\n",
    "    hf_BWD_loraB_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.lora_B.default.go_0\"\n",
    "    hf_BWD_loraB_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.lora_B.default.gi_0\"\n",
    "    hf_BWD_loraA_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.lora_A.default.go_0\"\n",
    "    hf_BWD_loraA_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.lora_A.default.gi_0\"\n",
    "    hf_loraA_weight = f\"{hf_path}/layers.{i}.mlp.down_proj.lora_A.default.weight\"\n",
    "    hf_loraB_weight = f\"{hf_path}/layers.{i}.mlp.down_proj.lora_B.default.weight\"\n",
    "    hf_BWD_lora_dropout_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.lora_dropout.default.go_0\"\n",
    "    hf_BWD_lora_dropout_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.lora_dropout.default.gi_0\"\n",
    "    hf_BWD_w2_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.go_0\"\n",
    "    hf_BWD_w2_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.down_proj.gi_0\"\n",
    "    hf_w2_weight = f\"{hf_path}/layers.{i}.mlp.down_proj.weight\"\n",
    "    hf_BWD_w3_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.up_proj.go_0\"\n",
    "    hf_BWD_w3_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.up_proj.gi_0\"\n",
    "    hf_BWD_w1_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.gate_proj.go_0\"\n",
    "    hf_BWD_w1_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.gate_proj.gi_0\"\n",
    "    hf_BWD_act_fn_in = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.act_fn.gi_0\"\n",
    "    hf_BWD_act_fn_out = f\"{hf_path}/bwd_step_0_layers.{i}.mlp.act_fn.go_0\"\n",
    "    hf_BWD_ffn_norm_out = f\"{hf_path}/bwd_step_0_layers.{i}.post_attention_layernorm.go_0\"\n",
    "    hf_BWD_ffn_norm_in = f\"{hf_path}/bwd_step_0_layers.{i}.post_attention_layernorm.gi_0\"\n",
    "    hf_BWD_attn_out_out = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.o_proj.go_0\"\n",
    "    hf_BWD_attn_q_in = f\"{hf_path}/bwd_step_0_layers.11.self_attn.q_proj.gi_0\"\n",
    "    hf_FWD_w1_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.gate_proj.output_0\"\n",
    "    hf_FWD_w3_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.up_proj.output_0\"\n",
    "    hf_FWD_act_fn_out = f\"{hf_path}/fwd_step_0_layers.{i}.mlp.act_fn.output_0\"\n",
    "    hf_BWD_attn_oproj_in = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.o_proj.gi_0\"\n",
    "    hf_attn_qproj_weight = f\"{hf_path}/layers.{i}.self_attn.q_proj.weight\"\n",
    "    hf_attn_kproj_weight = f\"{hf_path}/layers.{i}.self_attn.k_proj.weight\"\n",
    "    hf_attn_vproj_weight = f\"{hf_path}/layers.{i}.self_attn.v_proj.weight\"\n",
    "    hf_attn_oproj_weight = f\"{hf_path}/layers.{i}.self_attn.o_proj.weight\"\n",
    "    \n",
    "    # FlexFlow filepaths\n",
    "    ff_BWD_w2_out = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_shard_0_output_0\"\n",
    "    ff_BWD_w2_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_shard_0_input_0\"\n",
    "    ff_BWD_w2_in_pre = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_shard_0_pre_input_0\"\n",
    "    ff_w2_weight = f\"{ff_path}/fwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_shard_0_weight_0\"\n",
    "    ff_BWD_ssm_out = f\"{ff_path}/bwd_step_0_layers_{i}_SigmoidSiluMulti_shard_0_output_0\"\n",
    "    ff_BWD_ssm_in1 = f\"{ff_path}/bwd_step_0_layers_{i}_SigmoidSiluMulti_shard_0_input_0\"\n",
    "    ff_BWD_ssm_in2 = f\"{ff_path}/bwd_step_0_layers_{i}_SigmoidSiluMulti_shard_0_input_1\"\n",
    "    ff_BWD_w3_out = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w3_shard_0_output_0\"\n",
    "    ff_BWD_w3_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w3_shard_0_input_0\"\n",
    "    ff_BWD_lora_A_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_lora_shard_0_input_0\"\n",
    "    ff_BWD_lora_B_out = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_lora_shard_0_output_0\"\n",
    "    ff_lora_A_weight = f\"{ff_path}/fwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_lora_shard_0_weight_A\"\n",
    "    ff_lora_B_weight = f\"{ff_path}/fwd_step_0_layers_{i}_layers_{i}_feed_forward_w2_lora_shard_0_weight_B\"\n",
    "    ff_BWD_w1_out = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w1_shard_0_output_0\"\n",
    "    ff_BWD_w1_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w1_shard_0_input_0\"\n",
    "    ff_BWD_w1_in_pre = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w1_shard_0_pre_input_0\"\n",
    "    ff_w1_weight = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_feed_forward_w1_shard_0_weight_0\"\n",
    "    ff_BWD_ffn_norm_in1 = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_ffn_norm_shard_0_input_0\"\n",
    "    ff_BWD_ffn_norm_in2 = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_ffn_norm_shard_0_input_1\"\n",
    "    ff_BWD_ffn_norm_out = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_ffn_norm_shard_0_output_0\"\n",
    "    ff_BWD_attn_out = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_output_0\"\n",
    "    ff_BWD_attn_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_input_0\"\n",
    "    ff_BWD_ssm_cached_w1_input = f\"{ff_path}/bwd_step_0_layers_{i}_SigmoidSiluMulti_shard_0_cached_w1_output\"\n",
    "    ff_BWD_ssm_cached_w3_input = f\"{ff_path}/bwd_step_0_layers_{i}_SigmoidSiluMulti_shard_0_cached_w3_output\"\n",
    "    ff_FWD_w1_out = f\"{ff_path}/fwd_step_0_layers_0_layers_0_feed_forward_w1_shard_0_output_0\"\n",
    "    ff_FWD_w3_out = f\"{ff_path}/fwd_step_0_layers_0_layers_0_feed_forward_w3_shard_0_output_0\"\n",
    "    ff_FWD_act_fnc_out = f\"{ff_path}/bwd_step_0_layers_{i}_SigmoidSiluMulti_shard_0_act_fn_output\"\n",
    "    ff_BWD_attn_o_proj_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_o_proj_in_grad\"\n",
    "    ff_attn_oproj_weight = f\"{ff_path}/fwd_step_0_layers_{i}_layers_{i}_attention_shard_0_weight_0\"\n",
    "    \n",
    "    \n",
    "    # HuggingFace checks\n",
    "    print(\"\\nHuggingface checks:\")\n",
    "    if i == tot_num_layers-1:\n",
    "        compare_hf_tensors(hf_BWD_norm_in, hf_BWD_loraB_out)\n",
    "        compare_hf_tensors(hf_BWD_norm_in, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_loraB_out, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_loraB_in, hf_BWD_loraA_out)\n",
    "\n",
    "    compare_hf_tensors(hf_BWD_act_fn_in, hf_BWD_w1_out)\n",
    "    check_hf_sum_tensors(hf_BWD_ffn_norm_out, hf_BWD_w1_in, hf_BWD_w3_in)\n",
    "    if i == tot_num_layers-1:\n",
    "        check_hf_sum_tensors(hf_BWD_attn_out_out, hf_BWD_ffn_norm_in, hf_BWD_norm_in)\n",
    "\n",
    "    # FlexFlow checks\n",
    "    print(\"\\nFlexFlow checks:\")\n",
    "    compare_flexflow_tensors(ff_BWD_w2_out, ff_BWD_lora_B_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w2_in_pre, ff_BWD_lora_A_in)\n",
    "    compare_flexflow_tensors(ff_BWD_w2_in, ff_BWD_ssm_out)\n",
    "    compare_flexflow_tensors(ff_BWD_ssm_in2, ff_BWD_w3_out)\n",
    "    compare_flexflow_tensors(ff_BWD_ssm_in1, ff_BWD_w1_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w1_in, ff_BWD_ffn_norm_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w1_in_pre, ff_BWD_w3_in)\n",
    "    compare_flexflow_tensors(ff_BWD_ffn_norm_in1, ff_BWD_ffn_norm_in2, max_len=24*768)\n",
    "    \n",
    "    # HF-FlexFlow checks\n",
    "    print(\"\\nHuggingface-FlexFlow checks:\")\n",
    "    print(\"-- W2 --\")\n",
    "    compare_tensors(hf_BWD_w2_out, ff_BWD_w2_out, tolerance=1e-5)\n",
    "    compare_tensors(hf_w2_weight, ff_w2_weight, tolerance=1e-5)\n",
    "    \n",
    "    print(\"-- Lora --\")\n",
    "    compare_tensors(hf_loraA_weight, ff_lora_A_weight, tolerance=1e-5)\n",
    "    compare_tensors(hf_loraB_weight, ff_lora_B_weight, tolerance=1e-5)\n",
    "\n",
    "    compare_tensors(hf_BWD_loraB_out, ff_BWD_lora_B_out)\n",
    "    compare_tensors(hf_BWD_loraA_in, ff_BWD_lora_A_in)\n",
    "    \n",
    "    print(\"-- W2/W1/W3 --\")\n",
    "    compare_tensors(hf_BWD_w2_in, ff_BWD_ssm_out)\n",
    "    compare_tensors(hf_BWD_w2_in, ff_BWD_w2_in)\n",
    "    compare_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "    compare_tensors_difference(hf_BWD_w1_in, ff_BWD_w1_in, ff_BWD_w1_in_pre)\n",
    "    compare_tensors(hf_BWD_w3_out, ff_BWD_w3_out)\n",
    "    compare_tensors(hf_BWD_w3_in, ff_BWD_w3_in)\n",
    "    compare_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "    \n",
    "    print(\"-- Attention --\")\n",
    "    compare_tensors(hf_BWD_attn_out_out, ff_BWD_attn_out)\n",
    "    hidden_size = 768\n",
    "    qProjSize = 64\n",
    "    num_heads = 12\n",
    "    num_new_tokens = num_tokens = 24\n",
    "    if attention_tests:\n",
    "        # compare attn weight tensors\n",
    "        ff_attn_weight_tensor = np.loadtxt(ff_attn_oproj_weight, delimiter=',')\n",
    "        ff_attn_qproj_weight_tensor = ff_attn_weight_tensor[:hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "        ff_attn_kproj_weight_tensor = ff_attn_weight_tensor[hidden_size*qProjSize*num_heads:2*hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "        ff_attn_vproj_weight_tensor = ff_attn_weight_tensor[2*hidden_size*qProjSize*num_heads:3*hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "        ff_attn_oproj_weight_tensor = ff_attn_weight_tensor[3*hidden_size*qProjSize*num_heads:].reshape((qProjSize*num_heads,hidden_size), order='F')\n",
    "        \n",
    "        hf_attn_qproj_weight_tensor = torch.load(hf_attn_qproj_weight).T.detach().cpu().numpy()\n",
    "        hf_attn_kproj_weight_tensor = torch.load(hf_attn_kproj_weight).T.detach().cpu().numpy()\n",
    "        hf_attn_vproj_weight_tensor = torch.load(hf_attn_vproj_weight).T.detach().cpu().numpy()\n",
    "        hf_attn_oproj_weight_tensor = torch.load(hf_attn_oproj_weight).T.detach().cpu().numpy()\n",
    "        \n",
    "        assert(np.allclose(ff_attn_qproj_weight_tensor, hf_attn_qproj_weight_tensor, atol=1e-5))\n",
    "        assert(np.allclose(ff_attn_kproj_weight_tensor, hf_attn_kproj_weight_tensor, atol=1e-5))\n",
    "        assert(np.allclose(ff_attn_vproj_weight_tensor, hf_attn_vproj_weight_tensor, atol=1e-5))\n",
    "        assert(np.allclose(ff_attn_oproj_weight_tensor, hf_attn_oproj_weight_tensor, atol=1e-5))\n",
    "        \n",
    "        # Compare attn outproj grad in tensors\n",
    "        compare_tensors(hf_BWD_attn_oproj_in, ff_BWD_attn_o_proj_in)\n",
    "        \n",
    "        ########### Compare value projs grads ######################\n",
    "        # 1. compare qk prods softmax\n",
    "        hf_qk_prods_softmax = f\"{hf_path}/fwd_step_0_layers.{i}.self_attn.qk_prods_softmax.output_0\"\n",
    "        ff_attn_qk_prods_softmax = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_qk_prods_softmax\"\n",
    "        \n",
    "        hf_qk_prods_softmax = torch.load(hf_qk_prods_softmax)\n",
    "        ff_qk_prods_softmax = np.loadtxt(ff_attn_qk_prods_softmax, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "\n",
    "        for head_idx in range(num_heads):\n",
    "            hf_qkps = hf_qk_prods_softmax.squeeze()[head_idx, :, :].detach().cpu().numpy()\n",
    "            ff_qkps = ff_qk_prods_softmax[:,:,head_idx]\n",
    "            assert(np.allclose(ff_qkps, hf_qkps, atol=1e-5))\n",
    "        \n",
    "        # 2. compare attn heads grads\n",
    "        hf_attn_heads_grads = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.o_proj.gi_0\"\n",
    "        ff_attn_heads_grads = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_o_proj_in_grad\"\n",
    "\n",
    "        hf_attn_heads_grads = torch.load(hf_attn_heads_grads).T.squeeze().detach().cpu().numpy()\n",
    "        ff_attn_heads_grads = np.loadtxt(ff_attn_heads_grads, delimiter=',').reshape((qProjSize*num_heads, num_new_tokens), order = 'F')\n",
    "        # NEED TO VISUALLY INSPECT\n",
    "        compare_loaded_tensors(hf_attn_heads_grads, ff_attn_heads_grads)\n",
    "\n",
    "        # 3. vproj grads\n",
    "        hf_vproj_grads = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.v_proj.go_0\"\n",
    "        ff_vproj_grads = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_v_proj_in_grad\"\n",
    "\n",
    "        hf_vproj_grads = torch.load(hf_vproj_grads).squeeze().detach().cpu().numpy()\n",
    "        ff_vproj_grads = np.loadtxt(ff_vproj_grads, delimiter=',').reshape((num_tokens, qProjSize*num_heads), order='F')\n",
    "        compare_loaded_tensors(hf_vproj_grads, ff_vproj_grads)\n",
    "        \n",
    "        \n",
    "        ##############################\n",
    "        hf_value_states = f\"{hf_path}/fwd_step_0_layers.{i}.self_attn.value_states.output_0\"\n",
    "        hf_value_states = torch.load(hf_value_states).squeeze().permute(2,0,1).detach().cpu().numpy()\n",
    "        # print(hf_value_states.shape)\n",
    "        ff_value_states = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_vcache\"\n",
    "        ff_value_states = np.loadtxt(ff_value_states, delimiter=',').reshape((qProjSize, num_heads, num_tokens), order='F')\n",
    "        # print(ff_value_states.shape)\n",
    "        assert(np.allclose(hf_value_states, ff_value_states, atol=1e-2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ########## Compare key and query projs grads ##################\n",
    "        ff_devQKVPRojArray = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_devQKVPRojArray\"\n",
    "        ff_devQKVPRojArray = np.loadtxt(ff_devQKVPRojArray, delimiter=',').reshape((num_tokens, qProjSize*num_heads, 3), order = 'F')\n",
    "        ff_qProjGrads = ff_devQKVPRojArray[:,:,0]\n",
    "        ff_kProjGrads = ff_devQKVPRojArray[:,:,1]\n",
    "        ff_vProjGrads = ff_devQKVPRojArray[:,:,2]\n",
    "        assert(np.allclose(ff_vProjGrads, ff_vproj_grads, atol=1e-5))\n",
    "\n",
    "        # simulate qk_prods_softmax\n",
    "        ff_attn_heads_grads = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_o_proj_in_grad\"\n",
    "        ff_attn_heads_grads = np.loadtxt(ff_attn_heads_grads, delimiter=',').reshape((qProjSize,num_heads, num_new_tokens), order = 'F')\n",
    "        ff_attn_heads_grads = torch.from_numpy(ff_attn_heads_grads)\n",
    "        ff_attn_heads_grads = ff_attn_heads_grads.permute(1,2,0)\n",
    "        ff_value_states = torch.from_numpy(ff_value_states)\n",
    "        ff_value_states = ff_value_states.permute(1,0,2)\n",
    "        # print(ff_attn_heads_grads.shape)\n",
    "        # print(ff_value_states.shape)\n",
    "        simulated_qk_prods_softmax_grads = torch.matmul(ff_attn_heads_grads, ff_value_states)\n",
    "        #simulated_qk_prods_softmax_grads = simulated_qk_prods_softmax_grads\n",
    "        #print(\"Simulated QK prods grads:\")\n",
    "        #print(simulated_qk_prods_softmax_grads[0,:,:])\n",
    "\n",
    "        # qk prods softmax right before softmax\n",
    "        hf_qk_prods_softmax2 = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.qk_prods_softmax.go_0\"\n",
    "        hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "        ff_qk_prods_softmax2 = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_qk_prods_softmax_grad\"\n",
    "        ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "        hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "        hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "        \n",
    "        mismatches = np.where(~np.isclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2))\n",
    "        mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "        pct_mismatch = len(mismatches) / (hf_qk_prods_softmax2.shape[0] * hf_qk_prods_softmax2.shape[1] * hf_qk_prods_softmax2.shape[2])\n",
    "        print(f\"{pct_mismatch*100}% mismatch in QK prods softmax out grad\")\n",
    "        # print(hf_qk_prods_softmax2[:2,:,0])\n",
    "        # print(ff_qk_prods_softmax2[:2,:,0])\n",
    "        assert(pct_mismatch <= 0.1)\n",
    "\n",
    "        # qk prods softmax right after softmax\n",
    "        hf_qk_prods_softmax2 = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.pre_softmax.gi_0\"\n",
    "        hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "        ff_qk_prods_softmax2 = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_qk_prods_softmax_grad_in\"\n",
    "        ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "        hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "        hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "        compare_loaded_tensors(hf_qk_prods_softmax2, ff_qk_prods_softmax2)\n",
    "        \n",
    "        # qk prods softmax after mask\n",
    "        hf_qk_prods_softmax2 = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.matmul_op.go_0\"\n",
    "        hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "        ff_qk_prods_softmax2 = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_qk_prods_softmax_grad_in_masked\"\n",
    "        ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "        hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "        hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "        assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "\n",
    "        # Compare query activation\n",
    "        hf_query_activation = hf_path + f\"/fwd_step_0_layers.11.self_attn.query_activation.output_0\"\n",
    "        hf_query_activation = torch.load(hf_query_activation)\n",
    "        ff_query_activation = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_query_activation\"\n",
    "        ff_query_activation = np.loadtxt(ff_query_activation, delimiter=',').reshape((qProjSize, num_heads, num_new_tokens), order = 'F')\n",
    "        hf_query_activation = hf_query_activation.squeeze().permute(2,0,1).detach().cpu().numpy()\n",
    "        # assert(np.allclose(ff_query_activation, hf_query_activation, atol=1e-2))\n",
    "        # print(hf_query_activation[:,0,:])\n",
    "        # print()\n",
    "        # print(ff_query_activation[:,0,:])\n",
    "        # assert False\n",
    "        # compare_loaded_tensors(hf_query_activation, ff_query_activation)\n",
    "        check_rope = False\n",
    "        if check_rope:\n",
    "        ########################################## ROPE and Kproj ##########################################\n",
    "\n",
    "            # Compare FF kproj with intermediate kproj data from HF\n",
    "            hf_kproj_grads_post_rotary = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.identity_kv_post_rotary.go_0\"\n",
    "            hf_kproj_grads_post_rotary = torch.load(hf_kproj_grads_post_rotary)\n",
    "            hf_kproj_grads_post_rotary_copy = hf_kproj_grads_post_rotary.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "            # print(\"hf_kproj_grads_post_rotary: \", hf_kproj_grads_post_rotary_copy.shape)\n",
    "            # print(hf_kproj_grads_post_rotary_copy[:,:,0])\n",
    "            # Check hf ROPE \n",
    "            cos, sin = rotary_emb(hf_kproj_grads_post_rotary, seq_len=24)\n",
    "            cos = cos.cuda()\n",
    "            sin = sin.cuda()\n",
    "            # query_states:  torch.Size([1, 12, 24, 64])\n",
    "            # key_states:  torch.Size([1, 12, 24, 64])\n",
    "            # position_ids:  torch.Size([1, 24])\n",
    "            # tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "            #          18, 19, 20, 21, 22, 23]], device='cuda:0')\n",
    "            query_states = torch.zeros([1, 12, 24, 64]).cuda()\n",
    "            position_ids = torch.arange(24).unsqueeze(0).cuda()\n",
    "            query_states, hf_kproj_grads_post_rotary = apply_rotary_pos_emb(query_states, hf_kproj_grads_post_rotary, cos, sin, position_ids)\n",
    "            hf_kproj_grads_post_rotary = hf_kproj_grads_post_rotary.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "            # print(\"hf_kproj_grads_post_rotary: \", hf_kproj_grads_post_rotary.shape)\n",
    "            # print(hf_kproj_grads_post_rotary[:,:,0])\n",
    "            \n",
    "            hf_kproj_grads_before_rotary = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.identity_kv_before_rotary.go_0\"\n",
    "            hf_kproj_grads_before_rotary = torch.load(hf_kproj_grads_before_rotary)\n",
    "            hf_kproj_grads_before_rotary = hf_kproj_grads_before_rotary.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "            # print(\"hf_kproj_grads_before_rotary: \", hf_kproj_grads_before_rotary.shape)\n",
    "            # print(hf_kproj_grads_before_rotary[:,:,0])\n",
    "            # Compare HF rope with manual ROPE\n",
    "            assert(np.allclose(hf_kproj_grads_post_rotary, hf_kproj_grads_before_rotary, atol=1e-5))\n",
    "            # Compare HF Kproj with FF Kproj (before ROPE) \n",
    "            ff_kproj_pre = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_devkproj_pre\"\n",
    "            ff_kproj_pre = np.loadtxt(ff_kproj_pre, delimiter=',').reshape((num_tokens, qProjSize, num_heads), order = 'F')\n",
    "            # print(\"ff_kproj_pre: \", ff_kproj_pre.shape)\n",
    "            #print(ff_kproj_pre[:,:,0])\n",
    "            mismatches = np.where(~np.isclose(ff_kproj_pre, hf_kproj_grads_post_rotary_copy, atol=1e-5))\n",
    "            mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "            pct_mismatch = len(mismatches) / (ff_kproj_pre.shape[0] * ff_kproj_pre.shape[1] * ff_kproj_pre.shape[2])\n",
    "            print(f\"{pct_mismatch*100}% mismatch between HF and FF for kproj (before applying ROPE)\")\n",
    "            assert(pct_mismatch <= 0.05)\n",
    "            #assert(np.allclose(ff_kproj_pre, hf_kproj_grads_post_rotary_copy, atol=1e-5))\n",
    "            \n",
    "            ff_kproj = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_devkproj\"\n",
    "            ff_kproj = np.loadtxt(ff_kproj, delimiter=',').reshape((num_tokens, qProjSize, num_heads), order = 'F')\n",
    "            # print(\"ff_kproj: \", ff_kproj.shape)\n",
    "            #print(ff_kproj[:,:,0])\n",
    "            mismatches = np.where(~np.isclose(ff_kproj, hf_kproj_grads_before_rotary, atol=1e-5))\n",
    "            mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "            pct_mismatch = len(mismatches) / (ff_kproj.shape[0] * ff_kproj.shape[1] * ff_kproj.shape[2])\n",
    "            print(f\"{pct_mismatch*100}% mismatch between HF and FF for kproj (after applying ROPE)\")\n",
    "            assert(pct_mismatch <= 0.05)\n",
    "            #assert(np.allclose(ff_kproj, hf_kproj_grads_before_rotary, atol=1e-5))\n",
    "        \n",
    "        \n",
    "            #assert(np.allclose(hf_kproj_grads_post_rotary, hf_kproj_grads_before_rotary, atol=1e-2))\n",
    "            hf_kproj_grads = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.k_proj.go_0\"\n",
    "            hf_kproj_grads = torch.load(hf_kproj_grads).squeeze()\n",
    "            #print(\"hf_kproj_grads: \", hf_kproj_grads.shape)\n",
    "            #print(hf_kproj_grads[:,:64])\n",
    "            reshaped_tensor = hf_kproj_grads.view(24, 12, 64).transpose(1, 2).contiguous().detach().cpu().numpy()\n",
    "            #print(reshaped_tensor.shape)\n",
    "            assert(np.allclose(ff_kproj, reshaped_tensor, atol=1e-2))\n",
    "\n",
    "        ########################################## Qproj (with ROPE) ##########################################\n",
    "\n",
    "        # Compare QProj\n",
    "        hf_qproj_grads = f\"{hf_path}/bwd_step_0_layers.{i}.self_attn.q_proj.go_0\"\n",
    "        hf_qproj_grads = torch.load(hf_qproj_grads).squeeze()\n",
    "        # print(\"HF Qproj:\")\n",
    "        # print(hf_qproj_grads.shape)\n",
    "        reshaped_tensor = hf_qproj_grads.view(24, 12, 64).transpose(1, 2).contiguous().detach().cpu().numpy()\n",
    "        # print(\"\\t reshaped: \", reshaped_tensor.shape)\n",
    "        # print(reshaped_tensor[:,:,0])\n",
    "        ff_qproj = ff_path + f\"/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_devQKVPRojArray\"\n",
    "        ff_qproj = np.loadtxt(ff_qproj, delimiter=',').reshape((num_tokens, qProjSize, num_heads, 3), order = 'F')[:,:,:,0]\n",
    "        # print(\"FF Qproj:\")\n",
    "        # print(ff_qproj.shape)\n",
    "        # print(ff_qproj[:,:,0])\n",
    "        assert(np.allclose(ff_qproj, reshaped_tensor, atol=1e-2))\n",
    "\n",
    "    hf_attn_in = f\"{hf_path}/bwd_step_0_layers.{i}.input_layernorm.go_0\"\n",
    "    hf_attn_in = torch.load(hf_attn_in)\n",
    "    hf_attn_in = hf_attn_in.squeeze().T\n",
    "    hf_attn_in = hf_attn_in.detach().cpu().numpy()\n",
    "    print(\"hf_attn_in: \", hf_attn_in.shape)\n",
    "    print(hf_attn_in)\n",
    "\n",
    "    ff_attn_in = f\"{ff_path}/bwd_step_0_layers_{i}_layers_{i}_attention_shard_0_attn_final_grad_in\"\n",
    "    ff_attn_in = np.loadtxt(ff_attn_in, delimiter=',').reshape((768,num_tokens), order = 'F')\n",
    "    print(\"ff_attn_in: \", ff_attn_in.shape)\n",
    "    print(ff_attn_in)\n",
    "    #assert(np.allclose(ff_attn_in, hf_attn_in, atol=1e-2))\n",
    "\n",
    "    mismatches = np.where(~np.isclose(ff_attn_in, hf_attn_in))\n",
    "    mismatches = [(mismatches[0][i], mismatches[1][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (hf_attn_in.shape[0] * hf_attn_in.shape[1])\n",
    "    print(f\"{pct_mismatch*100}% mismatch in attention input grads\")\n",
    "    assert(pct_mismatch <= 0.1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01614726  0.01363804  0.01768043 ...  0.00724926 -0.00149747\n",
      " -0.01781223]\n"
     ]
    }
   ],
   "source": [
    "a = np.fromfile(\"/usr0/home/goliaro/.cache/flexflow/weights/goliaro/llama-160m-lora-full/full-precision/layers_11_feed_forward_w2_lora_A_weight\", dtype=np.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value states: torch.Size([1, 12, 24, 64])\n",
    "value_states=torch.from_numpy(hf_kproj_grads_post_rotary).permute(2,0,1).unsqueeze(0)\n",
    "key_states = value_states\n",
    "cos, sin = rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "# query_states:  torch.Size([1, 12, 24, 64])\n",
    "# key_states:  torch.Size([1, 12, 24, 64])\n",
    "# position_ids:  torch.Size([1, 24])\n",
    "# tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "#          18, 19, 20, 21, 22, 23]], device='cuda:0')\n",
    "query_states = torch.zeros([1, 12, 24, 64])\n",
    "position_ids = torch.arange(24).unsqueeze(0)\n",
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "key_states = key_states.squeeze()\n",
    "print(key_states.shape)\n",
    "print(key_states[0,:,:])\n",
    "print(hf_kproj_grads_before_rotary.shape)\n",
    "print(hf_kproj_grads_before_rotary[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 24, 24])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     ff_qkps \u001b[39m=\u001b[39m ff_qk_prods_softmax[:,:,head_idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39massert\u001b[39;00m(np\u001b[39m.\u001b[39mallclose(ff_qkps, hf_qkps, atol\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m hf_value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(hf_value_states)\u001b[39m#.squeeze().T.detach().cpu().numpy()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(hf_value_states\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_num = 11\n",
    "hf_qk_prods_softmax = f\"{hf_path}/fwd_step_0_layers.11.self_attn.qk_prods_softmax\"\n",
    "ff_qk_prods_softmax = f\"{ff_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "\n",
    "hf_value_states = f\"{hf_path}/fwd_step_0_layers.11.self_attn.value_states\"\n",
    "\n",
    "hf_qk_prods_softmax = torch.load(hf_qk_prods_softmax)#.squeeze().T.detach().cpu().numpy()\n",
    "ff_qk_prods_softmax = np.loadtxt(ff_qk_prods_softmax, delimiter=',').reshape((24, 24, 12), order = 'F')\n",
    "print(hf_qk_prods_softmax.shape)\n",
    "#print(ff_qk_prods_softmax.shape)\n",
    "#print(hf_qk_prods_softmax[:,:,0])\n",
    "#print()\n",
    "#print(ff_qk_prods_softmax[:,:,0])\n",
    "\n",
    "for head_idx in range(12):\n",
    "    hf_qkps = hf_qk_prods_softmax.squeeze()[head_idx, :, :].detach().cpu().numpy()\n",
    "    ff_qkps = ff_qk_prods_softmax[:,:,head_idx]\n",
    "    assert(np.allclose(ff_qkps, hf_qkps, atol=1e-5))\n",
    "\n",
    "\n",
    "hf_value_states = torch.load(hf_value_states)#.squeeze().T.detach().cpu().numpy()\n",
    "print(hf_value_states.shape)\n",
    "attn_output = torch.matmul(hf_qk_prods_softmax, hf_value_states)\n",
    "print()\n",
    "print(attn_output.shape)\n",
    "print(attn_output.transpose(1, 2).contiguous().shape)\n",
    "print(\"Hf attn heads\")\n",
    "print(torch.load(\"/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_layers.11.self_attn.o_proj.input_0\").shape)\n",
    "\n",
    "print(\"Attn heads grads:\")\n",
    "hf_attn_heads_grads = f\"{hf_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "print(torch.load(hf_attn_heads_grads).shape)\n",
    "print(\"HF value grads:\")\n",
    "vproj_grads = f\"{hf_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.gi_0\"\n",
    "print(torch.load(vproj_grads).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "print(a.shape)\n",
    "print(a.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [  27.8890,  -21.5089,   45.8214,  ...,    5.4010,  -10.8787,\n",
      "            39.7619],\n",
      "         [  19.2197,   27.4681,  -68.7141,  ...,  102.3280,   66.7925,\n",
      "          -160.8711],\n",
      "         ...,\n",
      "         [  63.9532,   17.4273,  -29.4416,  ...,  101.6105,   67.5937,\n",
      "          -198.4432],\n",
      "         [  31.2799,   13.0724,  -44.7179,  ...,  132.4898,   42.3135,\n",
      "          -194.4037],\n",
      "         [  42.3453,  -16.2693,  -55.7386,  ...,   90.5921,   52.2032,\n",
      "          -124.1802]]], device='cuda:0')\n",
      "tensor([[[-1.1845e+06, -6.7460e+05,  7.4494e+05,  ..., -9.1441e+05,\n",
      "          -1.4912e+05,  3.5769e+06],\n",
      "         [-7.3920e+01, -7.9389e+01,  1.1027e+02,  ..., -7.3020e+01,\n",
      "          -2.3540e+01,  3.4587e+02],\n",
      "         [-5.3885e+01, -1.7373e+01, -1.9780e+01,  ...,  4.1291e+01,\n",
      "           5.5099e+01,  5.5910e+01],\n",
      "         ...,\n",
      "         [-2.1948e+01, -3.2109e+01,  2.8364e+01,  ...,  3.4321e+01,\n",
      "           5.0713e+01,  5.6592e+01],\n",
      "         [-4.4339e+01, -2.8339e+01,  1.4070e+01,  ...,  6.2797e+01,\n",
      "           3.0760e+01,  6.1743e+01],\n",
      "         [-1.6287e+01, -5.0413e+01, -1.9940e+01,  ...,  4.3766e+01,\n",
      "           4.7833e+01,  4.7295e+01]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = \"./hf_peft_tensors/bwd_step_0_layers.11.post_attention_layernorm.gi_0\"\n",
    "b = \"./hf_peft_tensors/bwd_step_0_layers.11.self_attn.o_proj.go_0\"\n",
    "a = torch.load(a)\n",
    "b = torch.load(b)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "for layer_num in range(12):\n",
    "    hf_lora_A_weight_fp = f\"{hf_path}/layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp, tolerance=1e-5)\n",
    "    hf_lora_B_weight_fp = f\"{hf_path}/layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp, tolerance=1e-5)\n",
    "    hf_w1_weight = f\"{hf_path}/layers.{layer_num}.mlp.gate_proj.weight\"\n",
    "    ff_w1_weight = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w1_weight, ff_w1_weight, tolerance=1e-5)\n",
    "    hf_w3_weight = f\"{hf_path}/layers.{layer_num}.mlp.up_proj.weight\"\n",
    "    ff_w3_weight = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w3_weight, ff_w3_weight, tolerance=1e-5)\n",
    "    hf_w2_weight = f\"{hf_path}/layers.{layer_num}.mlp.down_proj.weight\"\n",
    "    ff_w2_weight = f\"{ff_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w2_weight, ff_w2_weight, tolerance=1e-5)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
