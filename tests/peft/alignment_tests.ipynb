{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_weight_base_path = \"/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors\"\n",
    "ff_weight_base_path = \"/usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors\"\n",
    "def compare_tensors(hf_tensor_filepath, ff_tensor_filepath, tolerance=1e-2):\n",
    "    assert(os.path.exists(hf_tensor_filepath) and os.path.exists(ff_tensor_filepath))\n",
    "    hf_tensor = torch.load(hf_tensor_filepath)\n",
    "    if type(hf_tensor) == tuple or type(hf_tensor) == list:\n",
    "        assert(len(hf_tensor) == 1)\n",
    "        hf_tensor = hf_tensor[0]\n",
    "    hf_tensor = torch.nan_to_num(hf_tensor)\n",
    "    hf_tensor = hf_tensor.flatten().detach().cpu().numpy()\n",
    "    ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "\n",
    "    len_hf_tensor = hf_tensor.shape[0]\n",
    "    ff_tensor = ff_tensor[:len_hf_tensor]\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor, hf_tensor, atol=tolerance):\n",
    "        print(f\"mismatch between {hf_tensor_filepath} and {ff_tensor_filepath}\")\n",
    "        print(f\"HF: {hf_tensor}\\nFF:{ff_tensor}\")\n",
    "        print(np.isclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor, hf_tensor, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "        #print(np.nonzero(hf_tensor)[0])\n",
    "        # print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\n",
    "        # print(ff_tensor[36], hf_tensor[36])\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len_hf_tensor)\n",
    "    print(\"Ok!\")\n",
    "def compare_tensors_difference(hf_tensor_filepath, ff_tensor1_filepath, ff_tensor2_filepath, tolerance=1e-2):\n",
    "    assert(os.path.exists(hf_tensor_filepath))\n",
    "    assert(os.path.exists(ff_tensor1_filepath))\n",
    "    assert(os.path.exists(ff_tensor2_filepath))\n",
    "    hf_tensor = torch.load(hf_tensor_filepath)\n",
    "    if type(hf_tensor) == tuple or type(hf_tensor) == list:\n",
    "        assert(len(hf_tensor) == 1)\n",
    "        hf_tensor = hf_tensor[0]\n",
    "    hf_tensor = torch.nan_to_num(hf_tensor)\n",
    "    hf_tensor = hf_tensor.flatten().detach().cpu().numpy()\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_filepath, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_filepath, delimiter=',')\n",
    "\n",
    "    len_hf_tensor = hf_tensor.shape[0]\n",
    "    ff_tensor1 = ff_tensor1[:len_hf_tensor]\n",
    "    ff_tensor2 = ff_tensor2[:len_hf_tensor]\n",
    "    ff_tensor = ff_tensor1 - ff_tensor2\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor, hf_tensor, atol=tolerance):\n",
    "        print(f\"mismatch between {hf_tensor_filepath} and {ff_tensor1_filepath} - {ff_tensor2_filepath}\")\n",
    "        print(f\"HF: {hf_tensor}\\nFF:{ff_tensor}\")\n",
    "        print(np.isclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor, hf_tensor, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "        #print(np.nonzero(hf_tensor)[0])\n",
    "        # print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\n",
    "        # print(ff_tensor[36], hf_tensor[36])\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len_hf_tensor)\n",
    "    print(\"Ok!\")\n",
    "def compare_hf_tensors(tensor1_fp, tensor2_fp):\n",
    "    assert(os.path.exists(tensor1_fp) and os.path.exists(tensor2_fp))\n",
    "    hf_tensor1 = torch.load(tensor1_fp)\n",
    "    hf_tensor2 = torch.load(tensor2_fp)\n",
    "    if type(hf_tensor1) == tuple or type(hf_tensor1) == list:\n",
    "        assert(len(hf_tensor1) == 1)\n",
    "        hf_tensor1 = hf_tensor1[0]\n",
    "    if type(hf_tensor2) == tuple or type(hf_tensor2) == list:\n",
    "        assert(len(hf_tensor2) == 1)\n",
    "        hf_tensor2 = hf_tensor2[0]\n",
    "    assert(torch.squeeze(hf_tensor1).shape == torch.squeeze(hf_tensor2).shape)\n",
    "    hf_tensor1 = torch.nan_to_num(hf_tensor1)\n",
    "    hf_tensor2 = torch.nan_to_num(hf_tensor2)\n",
    "    if not (np.allclose(hf_tensor1.detach().cpu().numpy(), hf_tensor2.detach().cpu().numpy())):\n",
    "        print(f\"mismatch between {tensor1_fp} and {tensor2_fp}\")\n",
    "        print(hf_tensor1)\n",
    "        print(hf_tensor2)\n",
    "        print(np.isclose(hf_tensor1.detach().cpu().numpy(), hf_tensor2.detach().cpu().numpy()))\n",
    "        mismatches = np.where(~np.isclose(hf_tensor1.detach().cpu().numpy(), hf_tensor2.detach().cpu().numpy()))[0]\n",
    "        print(mismatches)\n",
    "        assert(False)\n",
    "    print(\"Ok!\")\n",
    "\n",
    "def check_hf_sum_tensors(tensor_sum_fp, tensor1_fp, tensor2_fp):\n",
    "    assert(os.path.exists(tensor_sum_fp) and os.path.exists(tensor1_fp) and os.path.exists(tensor2_fp))\n",
    "    hf_tensor_sum = torch.load(tensor_sum_fp)\n",
    "    hf_tensor1 = torch.load(tensor1_fp)\n",
    "    hf_tensor2 = torch.load(tensor2_fp)\n",
    "    if type(hf_tensor_sum) == tuple or type(hf_tensor_sum) == list:\n",
    "        assert(len(hf_tensor_sum) == 1)\n",
    "        hf_tensor_sum = hf_tensor_sum[0]\n",
    "    if type(hf_tensor1) == tuple or type(hf_tensor1) == list:\n",
    "        assert(len(hf_tensor1) == 1)\n",
    "        hf_tensor1 = hf_tensor1[0]\n",
    "    if type(hf_tensor2) == tuple or type(hf_tensor2) == list:\n",
    "        assert(len(hf_tensor2) == 1)\n",
    "        hf_tensor2 = hf_tensor2[0]\n",
    "    assert(torch.squeeze(hf_tensor_sum).shape == torch.squeeze(hf_tensor1).shape)\n",
    "    assert(torch.squeeze(hf_tensor1).shape == torch.squeeze(hf_tensor2).shape)\n",
    "    hf_tensor1 = torch.nan_to_num(hf_tensor1)\n",
    "    hf_tensor2 = torch.nan_to_num(hf_tensor2)\n",
    "    hf_tensor_sum = torch.nan_to_num(hf_tensor_sum)\n",
    "    sum_check_tensor = hf_tensor1 + hf_tensor2\n",
    "    if not (np.allclose(sum_check_tensor.detach().cpu().numpy(), hf_tensor_sum.detach().cpu().numpy())):\n",
    "        print(f\"mismatch between {sum_check_tensor} and {tensor1_fp} + {tensor2_fp}\")\n",
    "        print(tensor_sum_fp)\n",
    "        print(sum_check_tensor)\n",
    "        print(hf_tensor1)\n",
    "        print(hf_tensor2)\n",
    "        print(np.isclose(sum_check_tensor.detach().cpu().numpy(), hf_tensor_sum.detach().cpu().numpy()))\n",
    "        mismatches = np.where(~np.isclose(sum_check_tensor.detach().cpu().numpy(), hf_tensor_sum.detach().cpu().numpy()))[0]\n",
    "        print(mismatches)\n",
    "        assert(False)\n",
    "    print(\"Ok!\")\n",
    "def check_hf_zero_tensor(hf_tensor_fp):\n",
    "    assert(os.path.exists(hf_tensor_fp))\n",
    "    hf_tensor1 = torch.load(hf_tensor_fp)\n",
    "    if type(hf_tensor1) == tuple or type(hf_tensor1) == list:\n",
    "        assert(len(hf_tensor1) == 1)\n",
    "        hf_tensor1 = hf_tensor1[0]\n",
    "    assert(torch.count_nonzero(torch.nan_to_num(hf_tensor1)).sum() == 0)\n",
    "def print_tensors(hf_tensor_filepath, ff_tensor_filepath, txt=\"\"):\n",
    "    assert(os.path.exists(hf_tensor_filepath) and os.path.exists(ff_tensor_filepath))\n",
    "    hf_tensor = torch.load(hf_tensor_filepath)\n",
    "    if type(hf_tensor) == tuple or type(hf_tensor) == list:\n",
    "        assert(len(hf_tensor) == 1)\n",
    "        hf_tensor = hf_tensor[0]\n",
    "    hf_tensor = torch.nan_to_num(hf_tensor)\n",
    "    hf_tensor = hf_tensor.flatten().detach().cpu().numpy()\n",
    "    ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "\n",
    "    len_hf_tensor = hf_tensor.shape[0]\n",
    "    ff_tensor = ff_tensor[:len_hf_tensor]\n",
    "\n",
    "    print(f\"{txt} - HF tensor:\")\n",
    "    print(hf_tensor)\n",
    "    print(f\"{txt} - FF tensor: \")\n",
    "    print(ff_tensor)\n",
    "def compare_flexflow_tensors(ff_tensor1_fp, ff_tensor2_fp, tolerance=1e-5, max_len=-1):\n",
    "    assert(os.path.exists(ff_tensor1_fp) and os.path.exists(ff_tensor2_fp))\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_fp, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_fp, delimiter=',')\n",
    "\n",
    "    if (ff_tensor1.shape != ff_tensor2.shape):\n",
    "        print(ff_tensor1.shape, ff_tensor2.shape)\n",
    "    assert(ff_tensor1.shape == ff_tensor2.shape)\n",
    "\n",
    "    if max_len > -1:\n",
    "        ff_tensor1 = ff_tensor1[:max_len]\n",
    "        ff_tensor2 = ff_tensor2[:max_len]\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor1, ff_tensor2, atol=tolerance):\n",
    "        print(f\"mismatch between {ff_tensor1_fp} and {ff_tensor2_fp}\")\n",
    "        print(f\"Tensor1: {ff_tensor1}\\nTensor2:{ff_tensor2}\")\n",
    "        print(np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len(ff_tensor1))\n",
    "    print(\"Ok!\")\n",
    "def compare_flexflow_tensors_shortest(ff_tensor1_fp, ff_tensor2_fp, tolerance=1e-5):\n",
    "    assert(os.path.exists(ff_tensor1_fp) and os.path.exists(ff_tensor2_fp))\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_fp, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_fp, delimiter=',')\n",
    "    minlen = min(ff_tensor1.shape[0], ff_tensor2.shape[0])\n",
    "    ff_tensor1 = ff_tensor1[:minlen]\n",
    "    ff_tensor2 = ff_tensor2[:minlen]\n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor1, ff_tensor2, atol=tolerance):\n",
    "        print(f\"mismatch between {ff_tensor1_fp} and {ff_tensor2_fp}\")\n",
    "        print(f\"Tensor1: {ff_tensor1}\\nTensor2:{ff_tensor2}\")\n",
    "        print(np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len(ff_tensor1))\n",
    "    print(\"Ok!\")\n",
    "def check_flexflow_tensors_sum(ff_tensor_sum_fp, ff_tensor1_fp, ff_tensor2_fp, tolerance=1e-5):\n",
    "    assert(os.path.exists(ff_tensor1_fp) and os.path.exists(ff_tensor2_fp))\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_fp, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_fp, delimiter=',')\n",
    "    ff_tensor_sum = np.loadtxt(ff_tensor_sum_fp, delimiter=',')\n",
    "    \n",
    "    ff_sum = ff_tensor1 + ff_tensor2\n",
    "    assert(ff_tensor1.shape == ff_tensor2.shape)\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor_sum, ff_sum, atol=tolerance):\n",
    "        print(f\"mismatch between {ff_tensor_sum_fp} and sum of {ff_tensor1_fp} + {ff_tensor2_fp}\")\n",
    "        print(f\"Tensor1: {ff_tensor1}\\nTensor2:{ff_tensor2}\")\n",
    "        print(f\"Sum Tensor: {ff_tensor_sum}\\nActual sum:{ff_sum}\")\n",
    "        print(np.isclose(ff_tensor_sum, ff_sum, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor_sum, ff_sum, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len(ff_tensor1))\n",
    "    print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "for layer_num in range(tot_num_layers):\n",
    "    hf_input_ln_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.input_layernorm.output_0\"\n",
    "    ff_input_ln_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_RMSNorm_shard-id_0_output_0\"\n",
    "    if layer_num > 0:\n",
    "        ff_input_ln_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_norm_shard-id_0_output_1\"\n",
    "    compare_tensors(hf_input_ln_out, ff_input_ln_out)\n",
    "    hf_attn_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.self_attn.o_proj.output_0\"\n",
    "    ff_attn_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_attn_out, ff_attn_out)\n",
    "    hf_ffn_norm_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.post_attention_layernorm.output_0\"\n",
    "    ff_ffn_norm_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_output_1\"\n",
    "    compare_tensors(hf_ffn_norm_out, ff_ffn_norm_out)\n",
    "    # w1\n",
    "    hf_gate_proj_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.gate_proj.output_0\"\n",
    "    ff_gate_proj_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_gate_proj_out, ff_gate_proj_out)\n",
    "    # w3\n",
    "    hf_up_proj_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.up_proj.output_0\" \n",
    "    ff_up_proj_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_up_proj_out, ff_up_proj_out)\n",
    "    # w2\n",
    "    hf_down_proj_in = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.input_0\"\n",
    "    hf_down_proj_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.output_0\"\n",
    "    ff_down_proj_in = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_input_0\"\n",
    "    ff_down_proj_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_down_proj_in, ff_down_proj_in)\n",
    "    # compare_tensors(hf_down_proj_out, ff_down_proj_out)\n",
    "    # LORA input\n",
    "    hf_lora_A_in = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.input_0\"\n",
    "    ff_lora_A_in = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_input_0\"\n",
    "    compare_hf_tensors(hf_down_proj_in, hf_lora_A_in)\n",
    "    compare_tensors(hf_lora_A_in, ff_lora_A_in)\n",
    "    # LORA weights\n",
    "    hf_lora_A_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp)\n",
    "    hf_lora_B_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp)\n",
    "    # LORA intermediate hf\n",
    "    hf_lora_A_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.output_0\"\n",
    "    hf_lora_B_in = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.input_0\"\n",
    "    compare_hf_tensors(hf_lora_A_out, hf_lora_B_in)\n",
    "    # LORA output\n",
    "    hf_lora_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.output_0\"\n",
    "    ff_lora_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_output_0\"\n",
    "    # compare_tensors(hf_lora_out, ff_lora_out)\n",
    "    # compare_flexflow_tensors(ff_down_proj_out, ff_lora_out)\n",
    "    # compare_tensors(hf_down_proj_out, ff_lora_out)\n",
    "    compare_tensors_difference(hf_lora_out, ff_lora_out, ff_down_proj_out)\n",
    "    \n",
    "\n",
    "# After last layer only\n",
    "hf_norm_out = f\"{hf_weight_base_path}/fwd_step_0_norm.output_0\"\n",
    "ff_norm_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_output_1\"\n",
    "compare_tensors(hf_norm_out, ff_norm_out)\n",
    "hf_lm_head_out = f\"{hf_weight_base_path}/fwd_step_0_base_model.model.lm_head.output_0\"\n",
    "ff_lm_head_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_output_0\"\n",
    "compare_tensors(hf_lm_head_out, ff_lm_head_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "\n",
    "ff_BWD_softmax_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_100_layer-name_Softmax_shard-id_0_input_0\"\n",
    "\n",
    "hf_BWD_lm_head_out = f\"{hf_weight_base_path}/bwd_step_0_base_model.model.lm_head.go_0\"\n",
    "ff_BWD_lm_head_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_output_0\"\n",
    "compare_tensors(hf_BWD_lm_head_out, ff_BWD_lm_head_out, tolerance=1e-5)\n",
    "# compare weights\n",
    "hf_lm_head_weight = f\"{hf_weight_base_path}/base_model.model.lm_head.weight\"\n",
    "ff_lm_head_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_weight_0\"\n",
    "compare_tensors(hf_lm_head_weight, ff_lm_head_weight, tolerance=1e-5)\n",
    "hf_BWD_lm_head_in = f\"{hf_weight_base_path}/bwd_step_0_base_model.model.lm_head.gi_0\"\n",
    "ff_BWD_lm_head_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_input_0\"\n",
    "compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in, tolerance=1e-5)\n",
    "# # Manually check the matmul\n",
    "# ff_tensor_out = np.loadtxt(ff_BWD_lm_head_out, delimiter=',')\n",
    "# ff_weight = np.loadtxt(ff_lm_head_weight, delimiter=',').reshape((4096,32000), order='F')\n",
    "# ff_tensor_out = ff_tensor_out[:32000*24].reshape((32000,24), order='F')\n",
    "# print(ff_tensor_out.shape)\n",
    "# print(ff_weight.shape)\n",
    "# print(np.matmul(ff_weight, ff_tensor_out))\n",
    "# compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in)\n",
    "# ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "\n",
    "hf_BWD_norm_out = f\"{hf_weight_base_path}/bwd_step_0_norm.go_0\"\n",
    "ff_BWD_norm_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_output_0\"\n",
    "compare_hf_tensors(hf_BWD_lm_head_in, hf_BWD_norm_out)\n",
    "compare_tensors(hf_BWD_norm_out, ff_BWD_norm_out)\n",
    "ff_BWD_norm_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_weight_0\"\n",
    "hf_FWD_norm_weight = f\"{hf_weight_base_path}/base_model.model.model.norm.weight\"\n",
    "compare_tensors(hf_FWD_norm_weight, ff_BWD_norm_weight, tolerance=1e-5)\n",
    "hf_BWD_norm_in = f\"{hf_weight_base_path}/bwd_step_0_norm.gi_0\"\n",
    "ff_BWD_norm_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_input_1\"\n",
    "compare_tensors(hf_BWD_norm_in, ff_BWD_norm_in, tolerance=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2] # first half\n",
    "    x2 = x[..., x.shape[-1] // 2 :] # second half\n",
    "    return torch.cat((x2, -x1), dim=-1)\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "head_dim = 64\n",
    "max_position_embeddings = 2048\n",
    "rope_theta=10_000\n",
    "kv_seq_len = 24\n",
    "rotary_emb = LlamaRotaryEmbedding(\n",
    "    head_dim,\n",
    "    max_position_embeddings=max_position_embeddings,\n",
    "    base=rope_theta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_bwd-step_0_layer-num_11_layer-name_SigmoidSiluMulti_shard-id_0_output_0\n",
      "HF: [ 6.4350547e+03 -6.4898600e+05  1.1761116e+05 ...  2.1410337e+01\n",
      "  1.2096541e+01  3.6424692e+00]\n",
      "FF:[ 6.43506250e+03 -6.48986000e+05  1.17611156e+05 ...  2.14103374e+01\n",
      "  1.20965424e+01  3.64246750e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[2394]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_bwd-step_0_layer-num_11_layer-name_layers_11_feed_forward_w2_shard-id_0_input_0\n",
      "HF: [ 6.4350547e+03 -6.4898600e+05  1.1761116e+05 ...  2.1410337e+01\n",
      "  1.2096541e+01  3.6424692e+00]\n",
      "FF:[ 6.43506250e+03 -6.48986000e+05  1.17611156e+05 ...  2.14103374e+01\n",
      "  1.20965424e+01  3.64246750e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[2394]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "4.383680555555555% mismatch in QK prods softmax out grad\n",
      "3.9116753472222223% mismatch between HF and FF for kproj (before applying ROPE)\n",
      "3.9008246527777777% mismatch between HF and FF for kproj (after applying ROPE)\n",
      "4.817708333333334% mismatch in attention input grads\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 353\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpct_mismatch\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% mismatch in attention input grads\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(pct_mismatch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "for layer_num in range(tot_num_layers-1, -1, -1):\n",
    "    # HuggingFace filepaths\n",
    "    hf_BWD_norm_in = f\"{hf_weight_base_path}/bwd_step_0_norm.gi_0\"\n",
    "    hf_BWD_loraB_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.go_0\"\n",
    "    hf_BWD_loraB_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.gi_0\"\n",
    "    hf_BWD_loraA_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.go_0\"\n",
    "    hf_BWD_loraA_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.gi_0\"\n",
    "    hf_loraA_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    hf_loraB_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    hf_BWD_lora_dropout_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_dropout.default.go_0\"\n",
    "    hf_BWD_lora_dropout_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_dropout.default.gi_0\"\n",
    "    hf_BWD_w2_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.go_0\"\n",
    "    hf_BWD_w2_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.gi_0\"\n",
    "    hf_w2_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.weight\"\n",
    "    hf_BWD_w3_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.up_proj.go_0\"\n",
    "    hf_BWD_w3_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.up_proj.gi_0\"\n",
    "    hf_BWD_w1_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.gate_proj.go_0\"\n",
    "    hf_BWD_w1_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.gate_proj.gi_0\"\n",
    "    hf_BWD_act_fn_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.act_fn.gi_0\"\n",
    "    hf_BWD_act_fn_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.act_fn.go_0\"\n",
    "    hf_BWD_ffn_norm_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.post_attention_layernorm.go_0\"\n",
    "    hf_BWD_ffn_norm_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.post_attention_layernorm.gi_0\"\n",
    "    hf_BWD_attn_out_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.go_0\"\n",
    "    hf_BWD_attn_q_in = f\"{hf_weight_base_path}/bwd_step_0_layers.11.self_attn.q_proj.gi_0\"\n",
    "    hf_FWD_w1_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.gate_proj.output_0\"\n",
    "    hf_FWD_w3_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.up_proj.output_0\"\n",
    "    hf_FWD_act_fn_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.act_fn.output_0\"\n",
    "    hf_BWD_attn_oproj_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "    hf_attn_qproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.q_proj.weight\"\n",
    "    hf_attn_kproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.k_proj.weight\"\n",
    "    hf_attn_vproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.v_proj.weight\"\n",
    "    hf_attn_oproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.o_proj.weight\"\n",
    "    # hf_BWD_attn_vproj_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.gi_0\"\n",
    "    # FlexFlow filepaths\n",
    "    ff_BWD_w2_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_output_0\"\n",
    "    ff_BWD_w2_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_input_0\"\n",
    "    ff_BWD_w2_in_pre = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_pre_input_0\"\n",
    "    ff_w2_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_weight_0\"\n",
    "    ff_BWD_ssm_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_output_0\"\n",
    "    ff_BWD_ssm_in1 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_input_0\"\n",
    "    ff_BWD_ssm_in2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_input_1\"\n",
    "    ff_BWD_w3_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_output_0\"\n",
    "    ff_BWD_w3_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_input_0\"\n",
    "    ff_BWD_lora_A_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_input_0\"\n",
    "    ff_BWD_lora_B_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_output_0\"\n",
    "    ff_lora_A_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    ff_lora_B_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    ff_BWD_w1_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_output_0\"\n",
    "    ff_BWD_w1_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_input_0\"\n",
    "    ff_BWD_w1_in_pre = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_pre_input_0\"\n",
    "    ff_w1_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_weight_0\"\n",
    "    ff_BWD_ffn_norm_in1 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_input_0\"\n",
    "    ff_BWD_ffn_norm_in2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_input_1\"\n",
    "    ff_BWD_ffn_norm_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_output_0\"\n",
    "    ff_BWD_attn_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_output_0\"\n",
    "    ff_BWD_attn_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_input_0\"\n",
    "    ff_BWD_ssm_cached_w1_input = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_cached_w1_output\"\n",
    "    ff_BWD_ssm_cached_w3_input = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_cached_w3_output\"\n",
    "    ff_FWD_w1_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_output_0\"\n",
    "    ff_FWD_w3_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_output_0\"\n",
    "    ff_FWD_act_fnc_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_act_fn_output\"\n",
    "    ff_BWD_attn_o_proj_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_o_proj_in_grad\"\n",
    "    # ff_BWD_attn_v_proj_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_v_proj_in_grad\"\n",
    "    ff_attn_oproj_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_11_layer-name_layers_11_attention_shard-id_0_weight_0\"\n",
    "    # ff_attn_qk_prods_softmax = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "\n",
    "    # xxx = torch.load(hf_BWD_attn_out_out)\n",
    "    # xxx.detach().cpu().numpy().tofile(f\"{hf_BWD_attn_out_out}.flexflow\")\n",
    "    # print(f\"{hf_BWD_attn_out_out}.flexflow\")\n",
    "    \n",
    "    # HuggingFace checks\n",
    "    print(\"\\nHuggingface checks:\")\n",
    "    if layer_num == tot_num_layers-1:\n",
    "        compare_hf_tensors(hf_BWD_norm_in, hf_BWD_loraB_out)\n",
    "        compare_hf_tensors(hf_BWD_norm_in, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_loraB_out, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_loraB_in, hf_BWD_loraA_out)\n",
    "    # compare_hf_tensors(hf_BWD_w3_out, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_act_fn_in, hf_BWD_w1_out)\n",
    "    check_hf_sum_tensors(hf_BWD_ffn_norm_out, hf_BWD_w1_in, hf_BWD_w3_in)\n",
    "    check_hf_sum_tensors(hf_BWD_attn_out_out, hf_BWD_ffn_norm_in, hf_BWD_norm_in)\n",
    "\n",
    "    # FlexFlow checks\n",
    "    print(\"\\nFlexFlow checks:\")\n",
    "    compare_flexflow_tensors(ff_BWD_w2_out, ff_BWD_lora_B_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w2_in_pre, ff_BWD_lora_A_in)\n",
    "    compare_flexflow_tensors(ff_BWD_w2_in, ff_BWD_ssm_out)\n",
    "    compare_flexflow_tensors(ff_BWD_ssm_in2, ff_BWD_w3_out)\n",
    "    compare_flexflow_tensors(ff_BWD_ssm_in1, ff_BWD_w1_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w1_in, ff_BWD_ffn_norm_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w1_in_pre, ff_BWD_w3_in)\n",
    "    compare_flexflow_tensors(ff_BWD_ffn_norm_in1, ff_BWD_ffn_norm_in2, max_len=24*768)\n",
    "    #compare_flexflow_tensors(ff_BWD_ffn_norm_in2, ff_BWD_attn_out, max_len=24*768) # should fail\n",
    "\n",
    "    # HF-FlexFlow checks\n",
    "    print(\"\\nHuggingface-FlexFlow checks:\")\n",
    "    compare_tensors(hf_BWD_w2_out, ff_BWD_w2_out, tolerance=1e-5)\n",
    "    compare_tensors(hf_w2_weight, ff_w2_weight, tolerance=1e-5)\n",
    "    #print(torch.load(hf_w2_weight).shape)\n",
    "    compare_tensors(hf_loraA_weight, ff_lora_A_weight, tolerance=1e-5)\n",
    "    compare_tensors(hf_loraB_weight, ff_lora_B_weight, tolerance=1e-5)\n",
    "\n",
    "    compare_tensors(hf_BWD_loraB_out, ff_BWD_lora_B_out)\n",
    "    compare_tensors(hf_BWD_loraA_in, ff_BWD_lora_A_in)\n",
    "\n",
    "    compare_tensors(hf_BWD_w2_in, ff_BWD_ssm_out)\n",
    "    compare_tensors(hf_BWD_w2_in, ff_BWD_w2_in)\n",
    "    compare_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "    compare_tensors_difference(hf_BWD_w1_in, ff_BWD_w1_in, ff_BWD_w1_in_pre)\n",
    "\n",
    "    compare_tensors(hf_FWD_w1_out, ff_FWD_w1_out)\n",
    "    compare_tensors(hf_FWD_w3_out, ff_FWD_w3_out)\n",
    "    compare_tensors(hf_BWD_w3_out, ff_BWD_w3_out)\n",
    "    compare_tensors(hf_BWD_w3_in, ff_BWD_w3_in)\n",
    "    compare_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "    # compare_tensors(hf_BWD_ffn_norm_out, ff_BWD_ffn_norm_out)\n",
    "    # compare_tensors(hf_BWD_ffn_norm_in, ff_BWD_ffn_norm_in2)\n",
    "    # compare_tensors(hf_BWD_attn_out_out, ff_BWD_ffn_norm_in2)\n",
    "    compare_tensors(hf_BWD_attn_out_out, ff_BWD_attn_out)\n",
    "\n",
    "    # compare attn weight tensors\n",
    "    hidden_size = 768\n",
    "    qProjSize = 64\n",
    "    num_heads = 12\n",
    "    num_new_tokens = num_tokens = 24\n",
    "    ff_attn_weight_tensor = np.loadtxt(ff_attn_oproj_weight, delimiter=',')\n",
    "    ff_attn_qproj_weight_tensor = ff_attn_weight_tensor[:hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "    ff_attn_kproj_weight_tensor = ff_attn_weight_tensor[hidden_size*qProjSize*num_heads:2*hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "    ff_attn_vproj_weight_tensor = ff_attn_weight_tensor[2*hidden_size*qProjSize*num_heads:3*hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "    ff_attn_oproj_weight_tensor = ff_attn_weight_tensor[3*hidden_size*qProjSize*num_heads:].reshape((qProjSize*num_heads,hidden_size), order='F')\n",
    "    \n",
    "    hf_attn_qproj_weight_tensor = torch.load(hf_attn_qproj_weight).T.detach().cpu().numpy()\n",
    "    hf_attn_kproj_weight_tensor = torch.load(hf_attn_kproj_weight).T.detach().cpu().numpy()\n",
    "    hf_attn_vproj_weight_tensor = torch.load(hf_attn_vproj_weight).T.detach().cpu().numpy()\n",
    "    hf_attn_oproj_weight_tensor = torch.load(hf_attn_oproj_weight).T.detach().cpu().numpy()\n",
    "    \n",
    "    assert(np.allclose(ff_attn_qproj_weight_tensor, hf_attn_qproj_weight_tensor, atol=1e-5))\n",
    "    assert(np.allclose(ff_attn_kproj_weight_tensor, hf_attn_kproj_weight_tensor, atol=1e-5))\n",
    "    assert(np.allclose(ff_attn_vproj_weight_tensor, hf_attn_vproj_weight_tensor, atol=1e-5))\n",
    "    assert(np.allclose(ff_attn_oproj_weight_tensor, hf_attn_oproj_weight_tensor, atol=1e-5))\n",
    "    \n",
    "    # Compare attn outproj grad in tensors\n",
    "    compare_tensors(hf_BWD_attn_oproj_in, ff_BWD_attn_o_proj_in)\n",
    "    \n",
    "    ########### Compare value projs grads ######################\n",
    "    # 1. compare qk prods softmax\n",
    "    hf_qk_prods_softmax = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.self_attn.qk_prods_softmax\"\n",
    "    ff_attn_qk_prods_softmax = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "    \n",
    "    hf_qk_prods_softmax = torch.load(hf_qk_prods_softmax)\n",
    "    ff_qk_prods_softmax = np.loadtxt(ff_attn_qk_prods_softmax, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "\n",
    "    for head_idx in range(num_heads):\n",
    "        hf_qkps = hf_qk_prods_softmax.squeeze()[head_idx, :, :].detach().cpu().numpy()\n",
    "        ff_qkps = ff_qk_prods_softmax[:,:,head_idx]\n",
    "        assert(np.allclose(ff_qkps, hf_qkps, atol=1e-5))\n",
    "    \n",
    "    # 2. compare attn heads grads\n",
    "    hf_attn_heads_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "    ff_attn_heads_grads = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_o_proj_in_grad\"\n",
    "\n",
    "    hf_attn_heads_grads = torch.load(hf_attn_heads_grads).T.squeeze().detach().cpu().numpy()\n",
    "    ff_attn_heads_grads = np.loadtxt(ff_attn_heads_grads, delimiter=',').reshape((qProjSize*num_heads, num_new_tokens), order = 'F')\n",
    "    assert(np.allclose(ff_attn_heads_grads, hf_attn_heads_grads, atol=1e-2))\n",
    "\n",
    "    # 3. vproj grads\n",
    "    hf_vproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.go_0\"\n",
    "    ff_vproj_grads = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_v_proj_in_grad\"\n",
    "\n",
    "    hf_vproj_grads = torch.load(hf_vproj_grads).squeeze().detach().cpu().numpy()\n",
    "    ff_vproj_grads = np.loadtxt(ff_vproj_grads, delimiter=',').reshape((num_tokens, qProjSize*num_heads), order='F')\n",
    "    assert(np.allclose(hf_vproj_grads, ff_vproj_grads, atol=1e-2))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################\n",
    "    hf_value_states = f\"{hf_weight_base_path}/fwd_step_0_layers.11.self_attn.value_states\"\n",
    "    hf_value_states = torch.load(hf_value_states).squeeze().permute(2,0,1).detach().cpu().numpy()\n",
    "    # print(hf_value_states.shape)\n",
    "    ff_value_states = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_vcache\"\n",
    "    ff_value_states = np.loadtxt(ff_value_states, delimiter=',').reshape((qProjSize, num_heads, num_tokens), order='F')\n",
    "    # print(ff_value_states.shape)\n",
    "    assert(np.allclose(hf_value_states, ff_value_states, atol=1e-2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########## Compare key and query projs grads ##################\n",
    "    ff_devQKVPRojArray = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devQKVPRojArray\"\n",
    "    ff_devQKVPRojArray = np.loadtxt(ff_devQKVPRojArray, delimiter=',').reshape((num_tokens, qProjSize*num_heads, 3), order = 'F')\n",
    "    ff_qProjGrads = ff_devQKVPRojArray[:,:,0]\n",
    "    ff_kProjGrads = ff_devQKVPRojArray[:,:,1]\n",
    "    ff_vProjGrads = ff_devQKVPRojArray[:,:,2]\n",
    "    assert(np.allclose(ff_vProjGrads, ff_vproj_grads, atol=1e-5))\n",
    "\n",
    "    # simulate qk_prods_softmax\n",
    "    ff_attn_heads_grads = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_o_proj_in_grad\"\n",
    "    ff_attn_heads_grads = np.loadtxt(ff_attn_heads_grads, delimiter=',').reshape((qProjSize,num_heads, num_new_tokens), order = 'F')\n",
    "    ff_attn_heads_grads = torch.from_numpy(ff_attn_heads_grads)\n",
    "    ff_attn_heads_grads = ff_attn_heads_grads.permute(1,2,0)\n",
    "    ff_value_states = torch.from_numpy(ff_value_states)\n",
    "    ff_value_states = ff_value_states.permute(1,0,2)\n",
    "    # print(ff_attn_heads_grads.shape)\n",
    "    # print(ff_value_states.shape)\n",
    "    simulated_qk_prods_softmax_grads = torch.matmul(ff_attn_heads_grads, ff_value_states)\n",
    "    #simulated_qk_prods_softmax_grads = simulated_qk_prods_softmax_grads\n",
    "    #print(\"Simulated QK prods grads:\")\n",
    "    #print(simulated_qk_prods_softmax_grads[0,:,:])\n",
    "\n",
    "    # qk prods softmax right before softmax\n",
    "    hf_qk_prods_softmax2 = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.softmax_op.go_0\"\n",
    "    hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "    ff_qk_prods_softmax2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax_grad\"\n",
    "    ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "    # assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "    mismatches = np.where(~np.isclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2))\n",
    "    mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (hf_qk_prods_softmax2.shape[0] * hf_qk_prods_softmax2.shape[1] * hf_qk_prods_softmax2.shape[2])\n",
    "    print(f\"{pct_mismatch*100}% mismatch in QK prods softmax out grad\")\n",
    "    assert(pct_mismatch <= 0.05)\n",
    "\n",
    "    # qk prods softmax right after softmax\n",
    "    hf_qk_prods_softmax2 = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.softmax_op.gi_0\"\n",
    "    hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "    ff_qk_prods_softmax2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax_grad_in\"\n",
    "    ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "    assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "    \n",
    "    # qk prods softmax after mask\n",
    "    hf_qk_prods_softmax2 = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.matmul_op.go_0\"\n",
    "    hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "    ff_qk_prods_softmax2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax_grad_in_masked\"\n",
    "    ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "    assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "\n",
    "    # Compare query activation\n",
    "    hf_query_activation = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.self_attn.query_activation\"\n",
    "    hf_query_activation = torch.load(hf_query_activation)\n",
    "    ff_query_activation = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_query_activation\"\n",
    "    ff_query_activation = np.loadtxt(ff_query_activation, delimiter=',').reshape((qProjSize, num_heads, num_new_tokens), order = 'F')\n",
    "    hf_query_activation = hf_query_activation.squeeze().permute(2,0,1).detach().cpu().numpy()\n",
    "    assert(np.allclose(ff_query_activation, hf_query_activation, atol=1e-2))\n",
    "    \n",
    "    ########################################## ROPE and Kproj ##########################################\n",
    "\n",
    "    # Compare FF kproj with intermediate kproj data from HF\n",
    "    hf_kproj_grads_post_rotary = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.identity_kv_post_rotary.go_0\"\n",
    "    hf_kproj_grads_post_rotary = torch.load(hf_kproj_grads_post_rotary)\n",
    "    hf_kproj_grads_post_rotary_copy = hf_kproj_grads_post_rotary.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    # print(\"hf_kproj_grads_post_rotary: \", hf_kproj_grads_post_rotary_copy.shape)\n",
    "    # print(hf_kproj_grads_post_rotary_copy[:,:,0])\n",
    "    # Check hf ROPE \n",
    "    cos, sin = rotary_emb(hf_kproj_grads_post_rotary, seq_len=24)\n",
    "    cos = cos.cuda()\n",
    "    sin = sin.cuda()\n",
    "    # query_states:  torch.Size([1, 12, 24, 64])\n",
    "    # key_states:  torch.Size([1, 12, 24, 64])\n",
    "    # position_ids:  torch.Size([1, 24])\n",
    "    # tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "    #          18, 19, 20, 21, 22, 23]], device='cuda:0')\n",
    "    query_states = torch.zeros([1, 12, 24, 64]).cuda()\n",
    "    position_ids = torch.arange(24).unsqueeze(0).cuda()\n",
    "    query_states, hf_kproj_grads_post_rotary = apply_rotary_pos_emb(query_states, hf_kproj_grads_post_rotary, cos, sin, position_ids)\n",
    "    hf_kproj_grads_post_rotary = hf_kproj_grads_post_rotary.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    # print(\"hf_kproj_grads_post_rotary: \", hf_kproj_grads_post_rotary.shape)\n",
    "    # print(hf_kproj_grads_post_rotary[:,:,0])\n",
    "    \n",
    "    hf_kproj_grads_before_rotary = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.identity_kv_before_rotary.go_0\"\n",
    "    hf_kproj_grads_before_rotary = torch.load(hf_kproj_grads_before_rotary)\n",
    "    hf_kproj_grads_before_rotary = hf_kproj_grads_before_rotary.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    # print(\"hf_kproj_grads_before_rotary: \", hf_kproj_grads_before_rotary.shape)\n",
    "    # print(hf_kproj_grads_before_rotary[:,:,0])\n",
    "    # Compare HF rope with manual ROPE\n",
    "    assert(np.allclose(hf_kproj_grads_post_rotary, hf_kproj_grads_before_rotary, atol=1e-5))\n",
    "    # Compare HF Kproj with FF Kproj (before ROPE) \n",
    "    ff_kproj_pre = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devkproj_pre\"\n",
    "    ff_kproj_pre = np.loadtxt(ff_kproj_pre, delimiter=',').reshape((num_tokens, qProjSize, num_heads), order = 'F')\n",
    "    # print(\"ff_kproj_pre: \", ff_kproj_pre.shape)\n",
    "    #print(ff_kproj_pre[:,:,0])\n",
    "    mismatches = np.where(~np.isclose(ff_kproj_pre, hf_kproj_grads_post_rotary_copy, atol=1e-5))\n",
    "    mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (ff_kproj_pre.shape[0] * ff_kproj_pre.shape[1] * ff_kproj_pre.shape[2])\n",
    "    print(f\"{pct_mismatch*100}% mismatch between HF and FF for kproj (before applying ROPE)\")\n",
    "    assert(pct_mismatch <= 0.05)\n",
    "    #assert(np.allclose(ff_kproj_pre, hf_kproj_grads_post_rotary_copy, atol=1e-5))\n",
    "    \n",
    "    ff_kproj = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devkproj\"\n",
    "    ff_kproj = np.loadtxt(ff_kproj, delimiter=',').reshape((num_tokens, qProjSize, num_heads), order = 'F')\n",
    "    # print(\"ff_kproj: \", ff_kproj.shape)\n",
    "    #print(ff_kproj[:,:,0])\n",
    "    mismatches = np.where(~np.isclose(ff_kproj, hf_kproj_grads_before_rotary, atol=1e-5))\n",
    "    mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (ff_kproj.shape[0] * ff_kproj.shape[1] * ff_kproj.shape[2])\n",
    "    print(f\"{pct_mismatch*100}% mismatch between HF and FF for kproj (after applying ROPE)\")\n",
    "    assert(pct_mismatch <= 0.05)\n",
    "    #assert(np.allclose(ff_kproj, hf_kproj_grads_before_rotary, atol=1e-5))\n",
    "    \n",
    "    \n",
    "    #assert(np.allclose(hf_kproj_grads_post_rotary, hf_kproj_grads_before_rotary, atol=1e-2))\n",
    "    hf_kproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.k_proj.go_0\"\n",
    "    hf_kproj_grads = torch.load(hf_kproj_grads).squeeze()\n",
    "    #print(\"hf_kproj_grads: \", hf_kproj_grads.shape)\n",
    "    #print(hf_kproj_grads[:,:64])\n",
    "    reshaped_tensor = hf_kproj_grads.view(24, 12, 64).transpose(1, 2).contiguous().detach().cpu().numpy()\n",
    "    #print(reshaped_tensor.shape)\n",
    "    assert(np.allclose(ff_kproj, reshaped_tensor, atol=1e-2))\n",
    "\n",
    "    ########################################## Qproj (with ROPE) ##########################################\n",
    "\n",
    "    # Compare QProj\n",
    "    hf_qproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.q_proj.go_0\"\n",
    "    hf_qproj_grads = torch.load(hf_qproj_grads).squeeze()\n",
    "    # print(\"HF Qproj:\")\n",
    "    # print(hf_qproj_grads.shape)\n",
    "    reshaped_tensor = hf_qproj_grads.view(24, 12, 64).transpose(1, 2).contiguous().detach().cpu().numpy()\n",
    "    # print(\"\\t reshaped: \", reshaped_tensor.shape)\n",
    "    # print(reshaped_tensor[:,:,0])\n",
    "    ff_qproj = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devQKVPRojArray\"\n",
    "    ff_qproj = np.loadtxt(ff_qproj, delimiter=',').reshape((num_tokens, qProjSize, num_heads, 3), order = 'F')[:,:,:,0]\n",
    "    # print(\"FF Qproj:\")\n",
    "    # print(ff_qproj.shape)\n",
    "    # print(ff_qproj[:,:,0])\n",
    "    assert(np.allclose(ff_qproj, reshaped_tensor, atol=1e-2))\n",
    "\n",
    "    hf_attn_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.input_layernorm.go_0\"\n",
    "    hf_attn_in = torch.load(hf_attn_in)\n",
    "    # print(\"hf_attn_in: \", hf_attn_in.shape)\n",
    "    hf_attn_in = hf_attn_in.squeeze().T\n",
    "    hf_attn_in = hf_attn_in.detach().cpu().numpy()\n",
    "    # print(\"hf_attn_in: \", hf_attn_in.shape)\n",
    "    # print(hf_attn_in)\n",
    "\n",
    "    ff_attn_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_attn_final_grad_in\"\n",
    "    ff_attn_in = np.loadtxt(ff_attn_in, delimiter=',').reshape((768,num_tokens), order = 'F')\n",
    "    # print(\"ff_attn_in: \", ff_attn_in.shape)\n",
    "    # print(ff_attn_in)\n",
    "    #assert(np.allclose(ff_attn_in, hf_attn_in, atol=1e-2))\n",
    "\n",
    "    mismatches = np.where(~np.isclose(ff_attn_in, hf_attn_in))\n",
    "    mismatches = [(mismatches[0][i], mismatches[1][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (hf_attn_in.shape[0] * hf_attn_in.shape[1])\n",
    "    print(f\"{pct_mismatch*100}% mismatch in attention input grads\")\n",
    "    assert(pct_mismatch <= 0.05)\n",
    "    \n",
    "\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 24, 64])\n",
      "tensor([[-1.5730e-02, -4.1161e-02,  3.0593e-02,  ...,  3.8630e-01,\n",
      "          3.2884e-01,  3.6067e-01],\n",
      "        [-2.8613e+01, -5.5872e+00,  2.9385e+01,  ...,  3.8782e+01,\n",
      "          9.6901e+01,  9.8470e+01],\n",
      "        [ 3.3027e+00,  1.8276e-01, -1.8497e+00,  ..., -4.4052e+01,\n",
      "         -2.0010e+01, -2.9788e+01],\n",
      "        ...,\n",
      "        [-7.6471e-02, -1.8892e-01,  3.6430e-01,  ..., -2.7493e-01,\n",
      "          5.7017e-01, -1.5986e-01],\n",
      "        [ 2.5780e+00, -1.8153e+00,  2.5088e+00,  ..., -1.0776e+01,\n",
      "          6.2167e-01,  8.3755e-01],\n",
      "        [-6.8324e-02,  1.7568e-01, -3.2311e-01,  ...,  3.1202e+00,\n",
      "         -2.6652e-01, -1.1917e+00]])\n",
      "(24, 64, 12)\n",
      "[[-1.5729919e-02 -4.1160699e-02  3.0592799e-02 ...  3.8629669e-01\n",
      "   3.2884139e-01  3.6066702e-01]\n",
      " [-2.8613457e+01 -5.5871558e+00  2.9384506e+01 ...  3.8781765e+01\n",
      "   9.6900581e+01  9.8469597e+01]\n",
      " [ 3.3027239e+00  1.8275940e-01 -1.8496730e+00 ... -4.4052174e+01\n",
      "  -2.0009745e+01 -2.9787930e+01]\n",
      " ...\n",
      " [-7.6470733e-02 -1.8891659e-01  3.6430117e-01 ... -2.7492592e-01\n",
      "   5.7017130e-01 -1.5985624e-01]\n",
      " [ 2.5780225e+00 -1.8152566e+00  2.5087588e+00 ... -1.0776262e+01\n",
      "   6.2166649e-01  8.3755457e-01]\n",
      " [-6.8324409e-02  1.7568478e-01 -3.2310838e-01 ...  3.1202292e+00\n",
      "  -2.6652411e-01 -1.1917179e+00]]\n"
     ]
    }
   ],
   "source": [
    "# value states: torch.Size([1, 12, 24, 64])\n",
    "value_states=torch.from_numpy(hf_kproj_grads_post_rotary).permute(2,0,1).unsqueeze(0)\n",
    "key_states = value_states\n",
    "cos, sin = rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "# query_states:  torch.Size([1, 12, 24, 64])\n",
    "# key_states:  torch.Size([1, 12, 24, 64])\n",
    "# position_ids:  torch.Size([1, 24])\n",
    "# tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "#          18, 19, 20, 21, 22, 23]], device='cuda:0')\n",
    "query_states = torch.zeros([1, 12, 24, 64])\n",
    "position_ids = torch.arange(24).unsqueeze(0)\n",
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "key_states = key_states.squeeze()\n",
    "print(key_states.shape)\n",
    "print(key_states[0,:,:])\n",
    "print(hf_kproj_grads_before_rotary.shape)\n",
    "print(hf_kproj_grads_before_rotary[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 24, 24])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     ff_qkps \u001b[39m=\u001b[39m ff_qk_prods_softmax[:,:,head_idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39massert\u001b[39;00m(np\u001b[39m.\u001b[39mallclose(ff_qkps, hf_qkps, atol\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m hf_value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(hf_value_states)\u001b[39m#.squeeze().T.detach().cpu().numpy()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(hf_value_states\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_num = 11\n",
    "hf_qk_prods_softmax = f\"{hf_weight_base_path}/fwd_step_0_layers.11.self_attn.qk_prods_softmax\"\n",
    "ff_qk_prods_softmax = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "\n",
    "hf_value_states = f\"{hf_weight_base_path}/fwd_step_0_layers.11.self_attn.value_states\"\n",
    "\n",
    "hf_qk_prods_softmax = torch.load(hf_qk_prods_softmax)#.squeeze().T.detach().cpu().numpy()\n",
    "ff_qk_prods_softmax = np.loadtxt(ff_qk_prods_softmax, delimiter=',').reshape((24, 24, 12), order = 'F')\n",
    "print(hf_qk_prods_softmax.shape)\n",
    "#print(ff_qk_prods_softmax.shape)\n",
    "#print(hf_qk_prods_softmax[:,:,0])\n",
    "#print()\n",
    "#print(ff_qk_prods_softmax[:,:,0])\n",
    "\n",
    "for head_idx in range(12):\n",
    "    hf_qkps = hf_qk_prods_softmax.squeeze()[head_idx, :, :].detach().cpu().numpy()\n",
    "    ff_qkps = ff_qk_prods_softmax[:,:,head_idx]\n",
    "    assert(np.allclose(ff_qkps, hf_qkps, atol=1e-5))\n",
    "\n",
    "\n",
    "hf_value_states = torch.load(hf_value_states)#.squeeze().T.detach().cpu().numpy()\n",
    "print(hf_value_states.shape)\n",
    "attn_output = torch.matmul(hf_qk_prods_softmax, hf_value_states)\n",
    "print()\n",
    "print(attn_output.shape)\n",
    "print(attn_output.transpose(1, 2).contiguous().shape)\n",
    "print(\"Hf attn heads\")\n",
    "print(torch.load(\"/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_layers.11.self_attn.o_proj.input_0\").shape)\n",
    "\n",
    "print(\"Attn heads grads:\")\n",
    "hf_attn_heads_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "print(torch.load(hf_attn_heads_grads).shape)\n",
    "print(\"HF value grads:\")\n",
    "vproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.gi_0\"\n",
    "print(torch.load(vproj_grads).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "print(a.shape)\n",
    "print(a.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [  27.8890,  -21.5089,   45.8214,  ...,    5.4010,  -10.8787,\n",
      "            39.7619],\n",
      "         [  19.2197,   27.4681,  -68.7141,  ...,  102.3280,   66.7925,\n",
      "          -160.8711],\n",
      "         ...,\n",
      "         [  63.9532,   17.4273,  -29.4416,  ...,  101.6105,   67.5937,\n",
      "          -198.4432],\n",
      "         [  31.2799,   13.0724,  -44.7179,  ...,  132.4898,   42.3135,\n",
      "          -194.4037],\n",
      "         [  42.3453,  -16.2693,  -55.7386,  ...,   90.5921,   52.2032,\n",
      "          -124.1802]]], device='cuda:0')\n",
      "tensor([[[-1.1845e+06, -6.7460e+05,  7.4494e+05,  ..., -9.1441e+05,\n",
      "          -1.4912e+05,  3.5769e+06],\n",
      "         [-7.3920e+01, -7.9389e+01,  1.1027e+02,  ..., -7.3020e+01,\n",
      "          -2.3540e+01,  3.4587e+02],\n",
      "         [-5.3885e+01, -1.7373e+01, -1.9780e+01,  ...,  4.1291e+01,\n",
      "           5.5099e+01,  5.5910e+01],\n",
      "         ...,\n",
      "         [-2.1948e+01, -3.2109e+01,  2.8364e+01,  ...,  3.4321e+01,\n",
      "           5.0713e+01,  5.6592e+01],\n",
      "         [-4.4339e+01, -2.8339e+01,  1.4070e+01,  ...,  6.2797e+01,\n",
      "           3.0760e+01,  6.1743e+01],\n",
      "         [-1.6287e+01, -5.0413e+01, -1.9940e+01,  ...,  4.3766e+01,\n",
      "           4.7833e+01,  4.7295e+01]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = \"./hf_peft_tensors/bwd_step_0_layers.11.post_attention_layernorm.gi_0\"\n",
    "b = \"./hf_peft_tensors/bwd_step_0_layers.11.self_attn.o_proj.go_0\"\n",
    "a = torch.load(a)\n",
    "b = torch.load(b)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual matmul checks\n",
    "# ff_w2_grad_out_tensor = np.loadtxt(ff_BWD_w2_out, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_w2_weight_tensor = np.loadtxt(ff_w2_weight, delimiter=',').reshape((3072,768), order='F')\n",
    "# ff_w2_gradin_tensor = np.matmul(ff_w2_weight_tensor, ff_w2_grad_out_tensor).reshape((3072,128), order='F')\n",
    "\n",
    "# ff_lora_gradout_tensor = np.loadtxt(ff_BWD_lora_B_out, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_lora_A_weight_tensor = np.loadtxt(ff_lora_A_weight, delimiter=',').reshape((3072,16), order='F')\n",
    "# ff_lora_B_weight_tensor = np.loadtxt(ff_lora_B_weight, delimiter=',').reshape((16,768), order='F')\n",
    "# ff_lora_int_grad_tensor = np.matmul(ff_lora_B_weight_tensor, ff_lora_gradout_tensor)\n",
    "# ff_lora_gradint_tensor = np.matmul(ff_lora_A_weight_tensor, ff_lora_int_grad_tensor)\n",
    "\n",
    "# # ff_w2_gradin_tensor = ff_w2_gradin_tensor + ff_lora_gradint_tensor\n",
    "# #print(ff_w2_gradin_tensor[:,:24])\n",
    "# print(\"calculated LORA grad in\")\n",
    "# print(ff_lora_gradint_tensor[:,:24])\n",
    "# # ff_BWD_w2_in_pre_tensor = np.loadtxt(ff_BWD_w2_in_pre, delimiter=',').reshape((3072,128), order='F')\n",
    "# ff_BWD_lora_A_in_tensor = np.loadtxt(ff_BWD_lora_A_in, delimiter=',').reshape((3072,128), order='F')\n",
    "# print(\"FlexFlow LORA grad in\")\n",
    "# print(ff_BWD_lora_A_in_tensor[:,:24])\n",
    "# # print(ff_BWD_w2_in_pre_tensor[:,:24])\n",
    "# print(\"HF lora grad in\")\n",
    "# print(torch.load(hf_BWD_loraA_in).squeeze().T.detach().cpu().numpy())\n",
    "# compare_tensors(hf_BWD_loraA_in, ff_BWD_lora_A_in)\n",
    "\n",
    "# simulate act_fn_grad\n",
    "# ssm_out_grad_tensor = np.loadtxt(ff_BWD_ssm_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# w3_fwd_out_tensor = np.loadtxt(ff_FWD_w3_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# #print(ssm_out_grad_tensor.shape, w3_fwd_out_tensor.shape)\n",
    "# act_fn_out_check = np.multiply(ssm_out_grad_tensor, w3_fwd_out_tensor)\n",
    "# print(\"simulated act fn out - simulated\")\n",
    "# print(act_fn_out_check[:,:24])\n",
    "# print(\"simulated act fn out - HF\")\n",
    "# print(torch.load(hf_BWD_act_fn_out).detach().cpu().numpy().squeeze().T)\n",
    "\n",
    "# Simulated w3_grad\n",
    "# ssm_out_grad_tensor = np.loadtxt(ff_BWD_ssm_out, delimiter=',').reshape((3072,128), order='F')[:,:24]\n",
    "# act_fnc_out_tensor = np.loadtxt(ff_FWD_act_fnc_out, delimiter=',').reshape((3072,24), order='F')\n",
    "# w3_out_gard_check = np.multiply(ssm_out_grad_tensor, act_fnc_out_tensor)\n",
    "# print(\"simulated w3 out - FF\")\n",
    "# print(w3_out_gard_check)\n",
    "# ff_BWD_w3_out_tensor = np.loadtxt(ff_BWD_w3_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# hf_BWD_w3_out_tensor = torch.load(hf_BWD_w3_out).detach().cpu().numpy().squeeze().T\n",
    "# print(\"w3 out, FF\")\n",
    "# print(ff_BWD_w3_out_tensor[:,:24])\n",
    "# print(\"w3 out, HF\")\n",
    "# print(hf_BWD_w3_out_tensor)\n",
    "\n",
    "# print_tensors(hf_BWD_w3_out, ff_BWD_w3_out, \"w3 out\")\n",
    "# assert False\n",
    "# print()\n",
    "# print()\n",
    "# print_tensors(hf_BWD_w3_out, ff_BWD_w3_out, \"w3 out\")\n",
    "# print_tensors(hf_BWD_w3_in, ff_BWD_w3_in, \"w3 in\")\n",
    "# print_tensors(hf_BWD_w1_out, ff_BWD_w1_out, \"w1 out\")\n",
    "# print_tensors(hf_BWD_w1_in, ff_BWD_w1_in, \"w1 in\")\n",
    "# print_tensors(hf_BWD_ffn_norm_out, ff_BWD_ffn_norm_out, \"ffn norm out\")\n",
    "# print_tensors(hf_BWD_ffn_norm_in, ff_BWD_ffn_norm_in2, \"ffn norm in\")\n",
    "# print()\n",
    "# ff_w1_out_tensor = np.loadtxt(ff_BWD_w1_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# ff_w1_in_tensor = np.loadtxt(ff_BWD_w1_in, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_w1_in_pre_tensor = np.loadtxt(ff_BWD_w1_in_pre, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_w1_only_in_tensor = ff_w1_in_tensor - ff_w1_in_pre_tensor\n",
    "# ff_w1_weight_tensor = np.loadtxt(ff_w1_weight, delimiter=',').reshape((768,3072), order='F')\n",
    "# ff_w1_in_check_tensor = np.matmul(ff_w1_weight_tensor, ff_w1_out_tensor)\n",
    "# print(\"W1 in (simulated):\")\n",
    "# print(ff_w1_in_check_tensor[:,:24])\n",
    "# print(\"W1 in (FF):\")\n",
    "# print(ff_w1_only_in_tensor[:,:24])\n",
    "# print(\"W1 in (HF):\")\n",
    "# print(torch.load(hf_BWD_w1_in).squeeze().T.detach().cpu().numpy())\n",
    "\n",
    "# compare_tensors_difference(hf_BWD_w2_in, ff_BWD_w2_in, ff_BWD_lora_A_in)\n",
    "# compare_tensors(hf_BWD_w3_out, ff_BWD_w3_out)\n",
    "#compare_hf_tensors(hf_BWD_ffn_norm_in, hf_BWD_attn_out_out)\n",
    "# print(\"\\nw1 out:\")\n",
    "\n",
    "# print_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "# print(\"\\nW1 in\\n\")\n",
    "# print_tensors(hf_BWD_w1_in, ff_BWD_w1_in)\n",
    "# compare_tensors(hf_BWD_w1_in, ff_BWD_w1_in)\n",
    "# print(\"\\nffn_norm\")\n",
    "# compare_tensors(hf_BWD_ffn_norm_out, ff_BWD_ffn_norm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "for layer_num in range(12):\n",
    "    hf_lora_A_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp, tolerance=1e-5)\n",
    "    hf_lora_B_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp, tolerance=1e-5)\n",
    "    hf_w1_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.gate_proj.weight\"\n",
    "    ff_w1_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w1_weight, ff_w1_weight, tolerance=1e-5)\n",
    "    hf_w3_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.up_proj.weight\"\n",
    "    ff_w3_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w3_weight, ff_w3_weight, tolerance=1e-5)\n",
    "    hf_w2_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.weight\"\n",
    "    ff_w2_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w2_weight, ff_w2_weight, tolerance=1e-5)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
