// THIS FILE WAS AUTO-GENERATED BY proj. DO NOT MODIFY IT!
// If you would like to modify this datatype, instead modify
// lib/op-attrs/include/op-attrs/ops/attention/multihead_attention_parallel_inputs.struct.toml
/* proj-data
{
  "generated_from": "7c434445707968123a361c038a337da2"
}
*/

#ifndef _FLEXFLOW_LIB_OP_ATTRS_INCLUDE_OP_ATTRS_OPS_ATTENTION_MULTIHEAD_ATTENTION_PARALLEL_INPUTS_DTG_H
#define _FLEXFLOW_LIB_OP_ATTRS_INCLUDE_OP_ATTRS_OPS_ATTENTION_MULTIHEAD_ATTENTION_PARALLEL_INPUTS_DTG_H

#include "fmt/format.h"
#include "nlohmann/json.hpp"
#include "op-attrs/datatype.dtg.h"
#include "op-attrs/parallel_tensor_shape/discard_copy_degree.dtg.h"
#include "op-attrs/parallel_tensor_shape/sum_degree.dtg.h"
#include "op-attrs/shard_parallel_dim.dtg.h"
#include "rapidcheck.h"
#include <cstddef>
#include <functional>
#include <ostream>
#include <tuple>

namespace FlexFlow {
struct MultiHeadAttentionParallelInputs {
  MultiHeadAttentionParallelInputs() = delete;
  MultiHeadAttentionParallelInputs(
      ::FlexFlow::ShardParallelDim const &batch_dim,
      ::FlexFlow::ShardParallelDim const &sequence_dim,
      ::FlexFlow::ShardParallelDim const &query_dim,
      ::FlexFlow::ShardParallelDim const &key_dim,
      ::FlexFlow::ShardParallelDim const &value_dim,
      ::FlexFlow::DiscardCopyDegree const &discard_copy_degree,
      ::FlexFlow::DataType const &datatype);

  bool operator==(MultiHeadAttentionParallelInputs const &) const;
  bool operator!=(MultiHeadAttentionParallelInputs const &) const;
  bool operator<(MultiHeadAttentionParallelInputs const &) const;
  bool operator>(MultiHeadAttentionParallelInputs const &) const;
  bool operator<=(MultiHeadAttentionParallelInputs const &) const;
  bool operator>=(MultiHeadAttentionParallelInputs const &) const;
  ::FlexFlow::ShardParallelDim batch_dim;
  ::FlexFlow::ShardParallelDim sequence_dim;
  ::FlexFlow::ShardParallelDim query_dim;
  ::FlexFlow::ShardParallelDim key_dim;
  ::FlexFlow::ShardParallelDim value_dim;
  ::FlexFlow::DiscardCopyDegree discard_copy_degree;
  ::FlexFlow::DataType datatype;
};
} // namespace FlexFlow

namespace std {
template <>
struct hash<FlexFlow::MultiHeadAttentionParallelInputs> {
  size_t operator()(FlexFlow::MultiHeadAttentionParallelInputs const &) const;
};
} // namespace std

namespace nlohmann {
template <>
struct adl_serializer<FlexFlow::MultiHeadAttentionParallelInputs> {
  static FlexFlow::MultiHeadAttentionParallelInputs from_json(json const &);
  static void to_json(json &,
                      FlexFlow::MultiHeadAttentionParallelInputs const &);
};
} // namespace nlohmann

namespace rc {
template <>
struct Arbitrary<FlexFlow::MultiHeadAttentionParallelInputs> {
  static Gen<FlexFlow::MultiHeadAttentionParallelInputs> arbitrary();
};
} // namespace rc

namespace FlexFlow {
std::string format_as(MultiHeadAttentionParallelInputs const &);
std::ostream &operator<<(std::ostream &,
                         MultiHeadAttentionParallelInputs const &);
} // namespace FlexFlow

#endif // _FLEXFLOW_LIB_OP_ATTRS_INCLUDE_OP_ATTRS_OPS_ATTENTION_MULTIHEAD_ATTENTION_PARALLEL_INPUTS_DTG_H
