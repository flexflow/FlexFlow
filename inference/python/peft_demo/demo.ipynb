{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlexFlow Co-Serving Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, random, subprocess, os\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from huggingface_hub import HfFolder\n",
    "import flexflow.serve as ff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(finetune_dataset_size=2, inference_file_path='inference_dataset.json', finetuning_file_path='finetuning_dataset.json'):\n",
    "    \"\"\"Creates the inference and finetuning datasets according to the data from https://huggingface.co/datasets/databricks/databricks-dolly-15k.\n",
    "    Only the 'open_qa' and 'closed_qa' prompts without context are kept.\n",
    "    The datasets are saved into the files given as arguments.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset_size -- the number of prompts to consider\n",
    "    inference_file_path -- the file in which to save the inference data\n",
    "    finetuning_file_path -- the file in which to save the finetuning data\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "    inference_data = []\n",
    "    finetuning_data = []\n",
    "    for row in dataset:\n",
    "        if len(finetuning_data) == finetune_dataset_size:\n",
    "            break\n",
    "        if (\"open_qa\" in row['category'] or \"closed_qa\" in row['category']) and len(row['context']) == 0:\n",
    "            inference_data.append(row['instruction'])\n",
    "            finetuning_data.append(row['instruction'] + \" \" + row['response'])\n",
    "    with open(inference_file_path, 'w') as file:\n",
    "        json.dump(inference_data[:1], file)\n",
    "    with open(finetuning_file_path, 'w') as file:\n",
    "        json.dump(finetuning_data[:1], file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    \"num_gpus\": 1,\n",
    "    \"memory_per_gpu\": 21000,\n",
    "    \"zero_copy_memory_per_node\": 40000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 1,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"max_requests_per_batch\": 1,\n",
    "    \"max_sequence_length\": 128,\n",
    "    \"max_tokens_per_batch\": 128,\n",
    "    \"max_training_steps\": 100,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"cache_path\": os.environ.get(\"FF_CACHE_PATH\", \"\"),\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": False,\n",
    "    # relative paths\n",
    "    \"inference_dataset\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"/usr/FlexFlow/inference/prompt/peft_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "generation_configs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"topp\": 0.8,\n",
    "    \"topk\": 1,\n",
    "}\n",
    "finetuning_configs = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"nesterov\": False,\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs_dict.update(generation_configs)\n",
    "configs_dict.update(finetuning_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(configs_dict[\"seed\"])\n",
    "\n",
    "configs = SimpleNamespace(**configs_dict)\n",
    "\n",
    "create_datasets(inference_file_path=configs_dict[\"inference_dataset\"], \n",
    "                finetuning_file_path=configs_dict[\"finetuning_dataset\"])\n",
    "\n",
    "# Clear output file\n",
    "with open(configs.output_file, 'w') as file:\n",
    "    file.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download base and peft inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora/config.json...\n",
      "Loading tokenizer...\n",
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora/config.json...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '../../utils/download_peft_model.py', 'goliaro/llama-3-8b-lora', '--base_model_name', 'meta-llama/Meta-Llama-3-8B'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = [configs.inference_peft_model_id, '--base_model_name', configs.base_model]\n",
    "subprocess.run(['python', '../../utils/download_peft_model.py'] + args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize FlexFlow runtime and LLM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - 7f4d49d21280]    0.672934 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f4d49d21280]    0.672995 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f4d49d21280]    0.673107 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f4d49d21280]    0.673118 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7f4d49d21280]    0.673124 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "workSpaceSize (128 MB)\n",
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora/config.json...\n",
      "Saving goliaro/llama-3-8b-lora configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora/config.json...\n",
      "Loading tokenizer...\n",
      "Adding layer layers.0.mlp.down_proj.lora\n",
      "Adding layer layers.1.mlp.down_proj.lora\n",
      "Adding layer layers.2.mlp.down_proj.lora\n",
      "Adding layer layers.3.mlp.down_proj.lora\n",
      "Adding layer layers.4.mlp.down_proj.lora\n",
      "Adding layer layers.5.mlp.down_proj.lora\n",
      "Adding layer layers.6.mlp.down_proj.lora\n",
      "Adding layer layers.7.mlp.down_proj.lora\n",
      "Adding layer layers.8.mlp.down_proj.lora\n",
      "Adding layer layers.9.mlp.down_proj.lora\n",
      "Adding layer layers.10.mlp.down_proj.lora\n",
      "Adding layer layers.11.mlp.down_proj.lora\n",
      "Adding layer layers.12.mlp.down_proj.lora\n",
      "Adding layer layers.13.mlp.down_proj.lora\n",
      "Adding layer layers.14.mlp.down_proj.lora\n",
      "Adding layer layers.15.mlp.down_proj.lora\n",
      "Adding layer layers.16.mlp.down_proj.lora\n",
      "Adding layer layers.17.mlp.down_proj.lora\n",
      "Adding layer layers.18.mlp.down_proj.lora\n",
      "Adding layer layers.19.mlp.down_proj.lora\n",
      "Adding layer layers.20.mlp.down_proj.lora\n",
      "Adding layer layers.21.mlp.down_proj.lora\n",
      "Adding layer layers.22.mlp.down_proj.lora\n",
      "Adding layer layers.23.mlp.down_proj.lora\n",
      "Adding layer layers.24.mlp.down_proj.lora\n",
      "Adding layer layers.25.mlp.down_proj.lora\n",
      "Adding layer layers.26.mlp.down_proj.lora\n",
      "Adding layer layers.27.mlp.down_proj.lora\n",
      "Adding layer layers.28.mlp.down_proj.lora\n",
      "Adding layer layers.29.mlp.down_proj.lora\n",
      "Adding layer layers.30.mlp.down_proj.lora\n",
      "Adding layer layers.31.mlp.down_proj.lora\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FlexFlow runtime. ff.init() takes a dictionary or the path to a JSON file with the configs\n",
    "ff.init(configs_dict)\n",
    "\n",
    "# Create the FlexFlow LLM\n",
    "ff_data_type = (\n",
    "    ff.DataType.DT_FLOAT if configs.full_precision else ff.DataType.DT_HALF\n",
    ")\n",
    "llm = ff.LLM(\n",
    "    configs.base_model,\n",
    "    data_type=ff_data_type,\n",
    "    cache_path=configs.cache_path,\n",
    "    refresh_cache=configs.refresh_cache,\n",
    "    output_file=configs.output_file,\n",
    ")\n",
    "# Add inference and/or finetuning lora\n",
    "lora_inference_config = None\n",
    "lora_finetuning_config = None\n",
    "if len(configs.inference_dataset) > 0:\n",
    "    lora_inference_config = ff.LoraLinearConfig(\n",
    "        llm.cache_path, \n",
    "        configs.inference_peft_model_id,\n",
    "        base_model_name_or_path=configs.base_model\n",
    "    )\n",
    "    llm.add_peft(lora_inference_config)\n",
    "if len(configs.finetuning_dataset) > 0:\n",
    "    lora_finetuning_config = ff.LoraLinearConfig(\n",
    "        llm.cache_path,\n",
    "        configs.finetuning_peft_model_id,\n",
    "        trainable=True,\n",
    "        init_lora_weights=False,\n",
    "        rank=16,\n",
    "        lora_alpha=16.0,\n",
    "        # target_modules = [\"down_proj\"],\n",
    "        base_model_name_or_path=configs.base_model,\n",
    "        optimizer_type=ff.OptimizerType.OPTIMIZER_TYPE_SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"learning_rate\": configs.learning_rate,\n",
    "            \"momentum\": configs.momentum,\n",
    "            \"weight_decay\": configs.weight_decay,\n",
    "            \"nesterov\": configs.nesterov,\n",
    "        },\n",
    "    )\n",
    "    llm.add_peft(lora_finetuning_config)\n",
    "\n",
    "# Compile the LLM for inference and load the weights into memory\n",
    "generation_config = ff.GenerationConfig(\n",
    "    do_sample=configs.do_sample,\n",
    "    temperature=configs.temperature,\n",
    "    topp=configs.topp,\n",
    "    topk=configs.topk\n",
    ")\n",
    "enable_peft_finetuning = len(configs.finetuning_dataset) > 0\n",
    "llm.compile(\n",
    "    generation_config,\n",
    "    enable_peft_finetuning=enable_peft_finetuning,\n",
    "    max_requests_per_batch=configs.max_requests_per_batch+int(enable_peft_finetuning),\n",
    "    max_seq_length=configs.max_sequence_length,\n",
    "    max_tokens_per_batch=configs.max_tokens_per_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the LLM Co-serving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background server started.\n",
      "2024-07-22 06:45:43 - ###PEFT DEBUGGING### Starting background serving task.\n",
      "2024-07-22 06:45:43 - ###PEFT DEBUGGING### Updated models' configuration.\n",
      "###PEFT DEBUGGING### LLM Model object exists.\n",
      "###PEFT DEBUGGING### Model object exists.\n",
      "###PEFT DEBUGGING### Model object still exists.\n",
      "###PEFT DEBUGGING### Entering compile_inference.\n",
      "###PEFT DEBUGGING### Configuration check passed: At least four CPU cores per node.\n"
     ]
    }
   ],
   "source": [
    "llm.start_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###PEFT DEBUGGING### Launching graph optimization task.\n",
      "[<flexflow.core.flexflow_cffi.Request object at 0x7f4ce8e13250>]\n",
      "num_nodes = 1 num_gpus_per_node = 1\n",
      "[0]10445\n",
      "[1]649\n",
      "[2]6730\n",
      "[3]2053\n",
      "[4]18167\n",
      "[5]369\n",
      "[6]1317\n",
      "[7]2085\n",
      "[8]3090\n",
      "[9]30\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7f4d49d21280]    1.600215 {3}{RequestManager}: [1000000]New request tokens: 128000 10445 649 6730 2053 18167 369 1317 2085 3090 30\n",
      "optimal_views.size = 262\n",
      "views.size() = 262\n",
      "###PEFT DEBUGGING### Operators reconstructed from optimized graph.\n",
      "###PEFT DEBUGGING### Starting inplace optimizations.\n",
      "###PEFT DEBUGGING### Mapping output tensors.\n",
      "ndim(1) dims[1 0 0 0]\n",
      "###PEFT DEBUGGING### Setting up NCCL communications.\n",
      "###PEFT DEBUGGING### compile_inference completed successfully.\n",
      "Loading weight file embed_tokens.weight\n",
      "Loading weight file layers.0.input_layernorm.weight\n",
      "Loading weight file layers.0.self_attn.q_proj.weight\n",
      "Loading weight file layers.0.self_attn.k_proj.weight\n",
      "Loading weight file layers.0.self_attn.v_proj.weight\n",
      "Loading weight file layers.0.self_attn.o_proj.weight\n",
      "Loading weight file layers.0.post_attention_layernorm.weight\n",
      "Loading weight file layers.0.mlp.gate_proj.weight\n",
      "Loading weight file layers.0.mlp.up_proj.weight\n",
      "Loading weight file layers.0.mlp.down_proj.weight\n",
      "Loading weight file layers.1.input_layernorm.weight\n",
      "Loading weight file layers.1.self_attn.q_proj.weight\n",
      "Loading weight file layers.1.self_attn.k_proj.weight\n",
      "Loading weight file layers.1.self_attn.v_proj.weight\n",
      "Loading weight file layers.1.self_attn.o_proj.weight\n",
      "Loading weight file layers.1.post_attention_layernorm.weight\n",
      "Loading weight file layers.1.mlp.gate_proj.weight\n",
      "Loading weight file layers.1.mlp.up_proj.weight\n",
      "Loading weight file layers.1.mlp.down_proj.weight\n",
      "Loading weight file layers.2.input_layernorm.weight\n",
      "Loading weight file layers.2.self_attn.q_proj.weight\n",
      "Loading weight file layers.2.self_attn.k_proj.weight\n",
      "Loading weight file layers.2.self_attn.v_proj.weight\n",
      "Loading weight file layers.2.self_attn.o_proj.weight\n",
      "Loading weight file layers.2.post_attention_layernorm.weight\n",
      "Loading weight file layers.2.mlp.gate_proj.weight\n",
      "Loading weight file layers.2.mlp.up_proj.weight\n",
      "Loading weight file layers.2.mlp.down_proj.weight\n",
      "Loading weight file layers.3.input_layernorm.weight\n",
      "Loading weight file layers.3.self_attn.q_proj.weight\n",
      "Loading weight file layers.3.self_attn.k_proj.weight\n",
      "Loading weight file layers.3.self_attn.v_proj.weight\n",
      "Loading weight file layers.3.self_attn.o_proj.weight\n",
      "Loading weight file layers.3.post_attention_layernorm.weight\n",
      "Loading weight file layers.3.mlp.gate_proj.weight\n",
      "Loading weight file layers.3.mlp.up_proj.weight\n",
      "Loading weight file layers.3.mlp.down_proj.weight\n",
      "Loading weight file layers.4.input_layernorm.weight\n",
      "Loading weight file layers.4.self_attn.q_proj.weight\n",
      "Loading weight file layers.4.self_attn.k_proj.weight\n",
      "Loading weight file layers.4.self_attn.v_proj.weight\n",
      "Loading weight file layers.4.self_attn.o_proj.weight\n",
      "Loading weight file layers.4.post_attention_layernorm.weight\n",
      "Loading weight file layers.4.mlp.gate_proj.weight\n",
      "Loading weight file layers.4.mlp.up_proj.weight\n",
      "Loading weight file layers.4.mlp.down_proj.weight\n",
      "Loading weight file layers.5.input_layernorm.weight\n",
      "Loading weight file layers.5.self_attn.q_proj.weight\n",
      "Loading weight file layers.5.self_attn.k_proj.weight\n",
      "Loading weight file layers.5.self_attn.v_proj.weight\n",
      "Loading weight file layers.5.self_attn.o_proj.weight\n",
      "Loading weight file layers.5.post_attention_layernorm.weight\n",
      "Loading weight file layers.5.mlp.gate_proj.weight\n",
      "Loading weight file layers.5.mlp.up_proj.weight\n",
      "Loading weight file layers.5.mlp.down_proj.weight\n",
      "Loading weight file layers.6.input_layernorm.weight\n",
      "Loading weight file layers.6.self_attn.q_proj.weight\n",
      "Loading weight file layers.6.self_attn.k_proj.weight\n",
      "Loading weight file layers.6.self_attn.v_proj.weight\n",
      "Loading weight file layers.6.self_attn.o_proj.weight\n",
      "Loading weight file layers.6.post_attention_layernorm.weight\n",
      "Loading weight file layers.6.mlp.gate_proj.weight\n",
      "Loading weight file layers.6.mlp.up_proj.weight\n",
      "Loading weight file layers.6.mlp.down_proj.weight\n",
      "Loading weight file layers.7.input_layernorm.weight\n",
      "Loading weight file layers.7.self_attn.q_proj.weight\n",
      "Loading weight file layers.7.self_attn.k_proj.weight\n",
      "Loading weight file layers.7.self_attn.v_proj.weight\n",
      "Loading weight file layers.7.self_attn.o_proj.weight\n",
      "Loading weight file layers.7.post_attention_layernorm.weight\n",
      "Loading weight file layers.7.mlp.gate_proj.weight\n",
      "Loading weight file layers.7.mlp.up_proj.weight\n",
      "Loading weight file layers.7.mlp.down_proj.weight\n",
      "Loading weight file layers.8.input_layernorm.weight\n",
      "Loading weight file layers.8.self_attn.q_proj.weight\n",
      "Loading weight file layers.8.self_attn.k_proj.weight\n",
      "Loading weight file layers.8.self_attn.v_proj.weight\n",
      "Loading weight file layers.8.self_attn.o_proj.weight\n",
      "Loading weight file layers.8.post_attention_layernorm.weight\n",
      "Loading weight file layers.8.mlp.gate_proj.weight\n",
      "Loading weight file layers.8.mlp.up_proj.weight\n",
      "Loading weight file layers.8.mlp.down_proj.weight\n",
      "Loading weight file layers.9.input_layernorm.weight\n",
      "Loading weight file layers.9.self_attn.q_proj.weight\n",
      "Loading weight file layers.9.self_attn.k_proj.weight\n",
      "Loading weight file layers.9.self_attn.v_proj.weight\n",
      "Loading weight file layers.9.self_attn.o_proj.weight\n",
      "Loading weight file layers.9.post_attention_layernorm.weight\n",
      "Loading weight file layers.9.mlp.gate_proj.weight\n",
      "Loading weight file layers.9.mlp.up_proj.weight\n",
      "Loading weight file layers.9.mlp.down_proj.weight\n",
      "Loading weight file layers.10.input_layernorm.weight\n",
      "Loading weight file layers.10.self_attn.q_proj.weight\n",
      "Loading weight file layers.10.self_attn.k_proj.weight\n",
      "Loading weight file layers.10.self_attn.v_proj.weight\n",
      "Loading weight file layers.10.self_attn.o_proj.weight\n",
      "Loading weight file layers.10.post_attention_layernorm.weight\n",
      "Loading weight file layers.10.mlp.gate_proj.weight\n",
      "Loading weight file layers.10.mlp.up_proj.weight\n",
      "Loading weight file layers.10.mlp.down_proj.weight\n",
      "Loading weight file layers.11.input_layernorm.weight\n",
      "Loading weight file layers.11.self_attn.q_proj.weight\n",
      "Loading weight file layers.11.self_attn.k_proj.weight\n",
      "Loading weight file layers.11.self_attn.v_proj.weight\n",
      "Loading weight file layers.11.self_attn.o_proj.weight\n",
      "Loading weight file layers.11.post_attention_layernorm.weight\n",
      "Loading weight file layers.11.mlp.gate_proj.weight\n",
      "Loading weight file layers.11.mlp.up_proj.weight\n",
      "Loading weight file layers.11.mlp.down_proj.weight\n",
      "Loading weight file layers.12.input_layernorm.weight\n",
      "Loading weight file layers.12.self_attn.q_proj.weight\n",
      "Loading weight file layers.12.self_attn.k_proj.weight\n",
      "Loading weight file layers.12.self_attn.v_proj.weight\n",
      "Loading weight file layers.12.self_attn.o_proj.weight\n",
      "Loading weight file layers.12.post_attention_layernorm.weight\n",
      "Loading weight file layers.12.mlp.gate_proj.weight\n",
      "Loading weight file layers.12.mlp.up_proj.weight\n",
      "Loading weight file layers.12.mlp.down_proj.weight\n",
      "Loading weight file layers.13.input_layernorm.weight\n",
      "Loading weight file layers.13.self_attn.q_proj.weight\n",
      "Loading weight file layers.13.self_attn.k_proj.weight\n",
      "Loading weight file layers.13.self_attn.v_proj.weight\n",
      "Loading weight file layers.13.self_attn.o_proj.weight\n",
      "Loading weight file layers.13.post_attention_layernorm.weight\n",
      "Loading weight file layers.13.mlp.gate_proj.weight\n",
      "Loading weight file layers.13.mlp.up_proj.weight\n",
      "Loading weight file layers.13.mlp.down_proj.weight\n",
      "Loading weight file layers.14.input_layernorm.weight\n",
      "Loading weight file layers.14.self_attn.q_proj.weight\n",
      "Loading weight file layers.14.self_attn.k_proj.weight\n",
      "Loading weight file layers.14.self_attn.v_proj.weight\n",
      "Loading weight file layers.14.self_attn.o_proj.weight\n",
      "Loading weight file layers.14.post_attention_layernorm.weight\n",
      "Loading weight file layers.14.mlp.gate_proj.weight\n",
      "Loading weight file layers.14.mlp.up_proj.weight\n",
      "Loading weight file layers.14.mlp.down_proj.weight\n",
      "Loading weight file layers.15.input_layernorm.weight\n",
      "Loading weight file layers.15.self_attn.q_proj.weight\n",
      "Loading weight file layers.15.self_attn.k_proj.weight\n",
      "Loading weight file layers.15.self_attn.v_proj.weight\n",
      "Loading weight file layers.15.self_attn.o_proj.weight\n",
      "Loading weight file layers.15.post_attention_layernorm.weight\n",
      "Loading weight file layers.15.mlp.gate_proj.weight\n",
      "Loading weight file layers.15.mlp.up_proj.weight\n",
      "Loading weight file layers.15.mlp.down_proj.weight\n",
      "Loading weight file layers.16.input_layernorm.weight\n",
      "Loading weight file layers.16.self_attn.q_proj.weight\n",
      "Loading weight file layers.16.self_attn.k_proj.weight\n",
      "Loading weight file layers.16.self_attn.v_proj.weight\n",
      "Loading weight file layers.16.self_attn.o_proj.weight\n",
      "Loading weight file layers.16.post_attention_layernorm.weight\n",
      "Loading weight file layers.16.mlp.gate_proj.weight\n",
      "Loading weight file layers.16.mlp.up_proj.weight\n",
      "Loading weight file layers.16.mlp.down_proj.weight\n",
      "Loading weight file layers.17.input_layernorm.weight\n",
      "Loading weight file layers.17.self_attn.q_proj.weight\n",
      "Loading weight file layers.17.self_attn.k_proj.weight\n",
      "Loading weight file layers.17.self_attn.v_proj.weight\n",
      "Loading weight file layers.17.self_attn.o_proj.weight\n",
      "Loading weight file layers.17.post_attention_layernorm.weight\n",
      "Loading weight file layers.17.mlp.gate_proj.weight\n",
      "Loading weight file layers.17.mlp.up_proj.weight\n",
      "Loading weight file layers.17.mlp.down_proj.weight\n",
      "Loading weight file layers.18.input_layernorm.weight\n",
      "Loading weight file layers.18.self_attn.q_proj.weight\n",
      "Loading weight file layers.18.self_attn.k_proj.weight\n",
      "Loading weight file layers.18.self_attn.v_proj.weight\n",
      "Loading weight file layers.18.self_attn.o_proj.weight\n",
      "Loading weight file layers.18.post_attention_layernorm.weight\n",
      "Loading weight file layers.18.mlp.gate_proj.weight\n",
      "Loading weight file layers.18.mlp.up_proj.weight\n",
      "Loading weight file layers.18.mlp.down_proj.weight\n",
      "Loading weight file layers.19.input_layernorm.weight\n",
      "Loading weight file layers.19.self_attn.q_proj.weight\n",
      "Loading weight file layers.19.self_attn.k_proj.weight\n",
      "Loading weight file layers.19.self_attn.v_proj.weight\n",
      "Loading weight file layers.19.self_attn.o_proj.weight\n",
      "Loading weight file layers.19.post_attention_layernorm.weight\n",
      "Loading weight file layers.19.mlp.gate_proj.weight\n",
      "Loading weight file layers.19.mlp.up_proj.weight\n",
      "Loading weight file layers.19.mlp.down_proj.weight\n",
      "Loading weight file layers.20.input_layernorm.weight\n",
      "Loading weight file layers.20.self_attn.q_proj.weight\n",
      "Loading weight file layers.20.self_attn.k_proj.weight\n",
      "Loading weight file layers.20.self_attn.v_proj.weight\n",
      "Loading weight file layers.20.self_attn.o_proj.weight\n",
      "Loading weight file layers.20.post_attention_layernorm.weight\n",
      "Loading weight file layers.20.mlp.gate_proj.weight\n",
      "Loading weight file layers.20.mlp.up_proj.weight\n",
      "Loading weight file layers.20.mlp.down_proj.weight\n",
      "Loading weight file layers.21.input_layernorm.weight\n",
      "Loading weight file layers.21.self_attn.q_proj.weight\n",
      "Loading weight file layers.21.self_attn.k_proj.weight\n",
      "Loading weight file layers.21.self_attn.v_proj.weight\n",
      "Loading weight file layers.21.self_attn.o_proj.weight\n",
      "Loading weight file layers.21.post_attention_layernorm.weight\n",
      "Loading weight file layers.21.mlp.gate_proj.weight\n",
      "Loading weight file layers.21.mlp.up_proj.weight\n",
      "Loading weight file layers.21.mlp.down_proj.weight\n",
      "Loading weight file layers.22.input_layernorm.weight\n",
      "Loading weight file layers.22.self_attn.q_proj.weight\n",
      "Loading weight file layers.22.self_attn.k_proj.weight\n",
      "Loading weight file layers.22.self_attn.v_proj.weight\n",
      "Loading weight file layers.22.self_attn.o_proj.weight\n",
      "Loading weight file layers.22.post_attention_layernorm.weight\n",
      "Loading weight file layers.22.mlp.gate_proj.weight\n",
      "Loading weight file layers.22.mlp.up_proj.weight\n",
      "Loading weight file layers.22.mlp.down_proj.weight\n",
      "Loading weight file layers.23.input_layernorm.weight\n",
      "Loading weight file layers.23.self_attn.q_proj.weight\n",
      "Loading weight file layers.23.self_attn.k_proj.weight\n",
      "Loading weight file layers.23.self_attn.v_proj.weight\n",
      "Loading weight file layers.23.self_attn.o_proj.weight\n",
      "Loading weight file layers.23.post_attention_layernorm.weight\n",
      "Loading weight file layers.23.mlp.gate_proj.weight\n",
      "Loading weight file layers.23.mlp.up_proj.weight\n",
      "Loading weight file layers.23.mlp.down_proj.weight\n",
      "Loading weight file layers.24.input_layernorm.weight\n",
      "Loading weight file layers.24.self_attn.q_proj.weight\n",
      "Loading weight file layers.24.self_attn.k_proj.weight\n",
      "Loading weight file layers.24.self_attn.v_proj.weight\n",
      "Loading weight file layers.24.self_attn.o_proj.weight\n",
      "Loading weight file layers.24.post_attention_layernorm.weight\n",
      "Loading weight file layers.24.mlp.gate_proj.weight\n",
      "Loading weight file layers.24.mlp.up_proj.weight\n",
      "Loading weight file layers.24.mlp.down_proj.weight\n",
      "Loading weight file layers.25.input_layernorm.weight\n",
      "Loading weight file layers.25.self_attn.q_proj.weight\n",
      "Loading weight file layers.25.self_attn.k_proj.weight\n",
      "Loading weight file layers.25.self_attn.v_proj.weight\n",
      "Loading weight file layers.25.self_attn.o_proj.weight\n",
      "Loading weight file layers.25.post_attention_layernorm.weight\n",
      "Loading weight file layers.25.mlp.gate_proj.weight\n",
      "Loading weight file layers.25.mlp.up_proj.weight\n",
      "Loading weight file layers.25.mlp.down_proj.weight\n",
      "Loading weight file layers.26.input_layernorm.weight\n",
      "Loading weight file layers.26.self_attn.q_proj.weight\n",
      "Loading weight file layers.26.self_attn.k_proj.weight\n",
      "Loading weight file layers.26.self_attn.v_proj.weight\n",
      "Loading weight file layers.26.self_attn.o_proj.weight\n",
      "Loading weight file layers.26.post_attention_layernorm.weight\n",
      "Loading weight file layers.26.mlp.gate_proj.weight\n",
      "Loading weight file layers.26.mlp.up_proj.weight\n",
      "Loading weight file layers.26.mlp.down_proj.weight\n",
      "Loading weight file layers.27.input_layernorm.weight\n",
      "Loading weight file layers.27.self_attn.q_proj.weight\n",
      "Loading weight file layers.27.self_attn.k_proj.weight\n",
      "Loading weight file layers.27.self_attn.v_proj.weight\n",
      "Loading weight file layers.27.self_attn.o_proj.weight\n",
      "Loading weight file layers.27.post_attention_layernorm.weight\n",
      "Loading weight file layers.27.mlp.gate_proj.weight\n",
      "Loading weight file layers.27.mlp.up_proj.weight\n",
      "Loading weight file layers.27.mlp.down_proj.weight\n",
      "Loading weight file layers.28.input_layernorm.weight\n",
      "Loading weight file layers.28.self_attn.q_proj.weight\n",
      "Loading weight file layers.28.self_attn.k_proj.weight\n",
      "Loading weight file layers.28.self_attn.v_proj.weight\n",
      "Loading weight file layers.28.self_attn.o_proj.weight\n",
      "Loading weight file layers.28.post_attention_layernorm.weight\n",
      "Loading weight file layers.28.mlp.gate_proj.weight\n",
      "Loading weight file layers.28.mlp.up_proj.weight\n",
      "Loading weight file layers.28.mlp.down_proj.weight\n",
      "Loading weight file layers.29.input_layernorm.weight\n",
      "Loading weight file layers.29.self_attn.q_proj.weight\n",
      "Loading weight file layers.29.self_attn.k_proj.weight\n",
      "Loading weight file layers.29.self_attn.v_proj.weight\n",
      "Loading weight file layers.29.self_attn.o_proj.weight\n",
      "Loading weight file layers.29.post_attention_layernorm.weight\n",
      "Loading weight file layers.29.mlp.gate_proj.weight\n",
      "Loading weight file layers.29.mlp.up_proj.weight\n",
      "Loading weight file layers.29.mlp.down_proj.weight\n",
      "Loading weight file layers.30.input_layernorm.weight\n",
      "Loading weight file layers.30.self_attn.q_proj.weight\n",
      "Loading weight file layers.30.self_attn.k_proj.weight\n",
      "Loading weight file layers.30.self_attn.v_proj.weight\n",
      "Loading weight file layers.30.self_attn.o_proj.weight\n",
      "Loading weight file layers.30.post_attention_layernorm.weight\n",
      "Loading weight file layers.30.mlp.gate_proj.weight\n",
      "Loading weight file layers.30.mlp.up_proj.weight\n",
      "Loading weight file layers.30.mlp.down_proj.weight\n",
      "Loading weight file layers.31.input_layernorm.weight\n",
      "Loading weight file layers.31.self_attn.q_proj.weight\n",
      "Loading weight file layers.31.self_attn.k_proj.weight\n",
      "Loading weight file layers.31.self_attn.v_proj.weight\n",
      "Loading weight file layers.31.self_attn.o_proj.weight\n",
      "Loading weight file layers.31.post_attention_layernorm.weight\n",
      "Loading weight file layers.31.mlp.gate_proj.weight\n",
      "Loading weight file layers.31.mlp.up_proj.weight\n",
      "Loading weight file layers.31.mlp.down_proj.weight\n",
      "Loading weight file norm.weight\n",
      "Loading weight file lm_head.weight\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "[0 - 7f4ce019c740]   24.015346 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0178740]   24.062661 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0190740]   24.128376 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0184740]   24.199797 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0178740]   24.255941 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0178740]   24.306545 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   24.357210 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0190740]   24.407958 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0178740]   24.459366 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0178740]   24.510618 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0178740]   24.560416 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0178740]   24.611335 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0178740]   24.663808 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0178740]   24.710965 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0178740]   24.756020 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0178740]   24.805719 {3}{RequestManager}: Output token is: 3090\n",
      "[0 - 7f4ce0178740]   24.858560 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7f4ce0184740]   24.910607 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0178740]   24.958879 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0184740]   25.002851 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   25.050780 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0178740]   25.104554 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0184740]   25.159509 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   25.211003 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0184740]   25.261411 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0190740]   25.312357 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0184740]   25.362253 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0184740]   25.412284 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0184740]   25.461502 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0184740]   25.513610 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0184740]   25.564433 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0184740]   25.613662 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0184740]   25.663786 {3}{RequestManager}: Output token is: 3090\n",
      "[0 - 7f4ce0184740]   25.712708 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7f4ce0184740]   25.762206 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0184740]   25.812755 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0184740]   25.863367 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0184740]   25.913378 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0184740]   25.965063 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0178740]   26.015739 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   26.065768 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0178740]   26.115556 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0184740]   26.166644 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0184740]   26.218528 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0178740]   26.269681 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0178740]   26.320250 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0178740]   26.371698 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0184740]   26.422587 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0178740]   26.474391 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0178740]   26.524817 {3}{RequestManager}: Output token is: 3090\n",
      "[0 - 7f4ce0190740]   26.575224 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7f4ce0178740]   26.627207 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0190740]   26.679366 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0178740]   26.729921 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   26.779766 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0178740]   26.832104 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0184740]   26.884087 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   26.935580 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0184740]   26.992909 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0184740]   27.043722 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0184740]   27.093960 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0178740]   27.144937 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0190740]   27.196991 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0178740]   27.248143 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0190740]   27.299549 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0190740]   27.351395 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0178740]   27.402975 {3}{RequestManager}: Output token is: 3090\n",
      "[0 - 7f4ce0190740]   27.453662 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7f4ce0178740]   27.504152 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0178740]   27.554072 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0184740]   27.605613 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0178740]   27.656807 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0190740]   27.707595 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0190740]   27.757815 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0190740]   27.809557 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0184740]   27.862148 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0190740]   27.914188 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0178740]   27.965942 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0184740]   28.017837 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0184740]   28.069997 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0184740]   28.122560 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0190740]   28.172513 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0190740]   28.224002 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0184740]   28.276536 {3}{RequestManager}: Output token is: 3090\n",
      "[0 - 7f4ce0184740]   28.327091 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7f4ce0184740]   28.377124 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0190740]   28.427226 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0190740]   28.477499 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0184740]   28.528489 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0178740]   28.580135 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0190740]   28.631761 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0190740]   28.683392 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0184740]   28.734001 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0190740]   28.783914 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0190740]   28.835832 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0184740]   28.885271 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0190740]   28.936179 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0190740]   28.987163 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0184740]   29.038264 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0184740]   29.084248 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0184740]   29.129864 {3}{RequestManager}: Output token is: 3090\n",
      "[0 - 7f4ce0184740]   29.175946 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7f4ce0184740]   29.226707 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7f4ce0184740]   29.277372 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7f4ce0184740]   29.329588 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0190740]   29.380856 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7f4ce0190740]   29.431483 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7f4ce0190740]   29.483399 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7f4ce0190740]   29.536268 {3}{RequestManager}: Output token is: 2144\n",
      "[0 - 7f4ce0190740]   29.588317 {3}{RequestManager}: Output token is: 430\n",
      "[0 - 7f4ce0184740]   29.638727 {3}{RequestManager}: Output token is: 6730\n",
      "[0 - 7f4ce0190740]   29.689708 {3}{RequestManager}: Output token is: 2053\n",
      "[0 - 7f4ce0190740]   29.740987 {3}{RequestManager}: Output token is: 649\n",
      "[0 - 7f4ce0178740]   29.791166 {3}{RequestManager}: Output token is: 18167\n",
      "[0 - 7f4ce0190740]   29.841776 {3}{RequestManager}: Output token is: 369\n",
      "[0 - 7f4ce0184740]   29.893514 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7f4ce0178740]   29.945509 {3}{RequestManager}: Output token is: 2085\n",
      "[0 - 7f4ce0178740]   29.945878 {3}{RequestManager}: [Done] guid(1000000) final_length(128)\n",
      "[0 - 7f4ce0178740]   29.945889 {3}{RequestManager}: Final output: <s> <|begin_of_text|>Why can camels survive for long without water? What is the reason behind the fact that camels can survive for long without water? What is the reason behind the fact that camels can survive for long without water? What is the reason behind the fact that camels can survive for long without water? What is the reason behind the fact that camels can survive for long without water? What is the reason behind the fact that camels can survive for long without water? What is the reason behind the fact that camels can survive for long without water? What is the reason behind the fact that camels can survive for long without\n",
      "[0 - 7f4ce0178740]   29.945900 {3}{RequestManager}: [Profile] guid(1000000) llm_decoding_steps(117) start(23696232.0) finish(29945893.0) latency(6249661.0) ttft(22415078.0)\n"
     ]
    }
   ],
   "source": [
    "prompts = [s for s in json.load(open(configs.inference_dataset))]\n",
    "inference_requests = [\n",
    "    ff.Request(\n",
    "        ff.RequestType.REQ_INFERENCE,\n",
    "        prompt=prompt,\n",
    "        max_sequence_length=configs.max_sequence_length,\n",
    "        peft_model_id=llm.get_ff_peft_id(lora_inference_config),\n",
    "    )\n",
    "    for prompt in prompts\n",
    "]\n",
    "inf_req_res_1 = llm.generate(inference_requests)\n",
    "with open(\"before_finetuning.txt\", \"w\") as file:\n",
    "    file.write(str(inf_req_res_1[0].output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Finetuning on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<flexflow.core.flexflow_cffi.Request object at 0x7f4ce3304c50>]\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7f4d49d21280]   29.957050 {3}{RequestManager}: [0] input: 128000 10445 649 6730 2053 18167 369 1317 2085 3090 30 8215 2053 1005 279 8834 304 872 305 12055 311 2567 1124 10409 449 4907 323 88000 369 1317 18852 315 892 13\n",
      "[0 - 7f4d49d21280]   29.957061 {3}{RequestManager}: [0] output:\n",
      "Loss: 2.6536\n",
      "Loss: 2.5942\n",
      "Loss: 2.5360\n",
      "Loss: 2.5083\n",
      "Loss: 2.4783\n",
      "Loss: 2.4570\n",
      "Loss: 2.4420\n",
      "Loss: 2.4194\n",
      "Loss: 2.4050\n",
      "Loss: 2.3949\n",
      "Loss: 2.3841\n",
      "Loss: 2.3764\n",
      "Loss: 2.3676\n",
      "Loss: 2.3535\n",
      "Loss: 2.3396\n",
      "Loss: 2.3299\n",
      "Loss: 2.3287\n",
      "Loss: 2.3215\n",
      "Loss: 2.3058\n",
      "Loss: 2.2978\n",
      "Loss: 2.2885\n",
      "Loss: 2.2852\n",
      "Loss: 2.2660\n",
      "Loss: 2.2619\n",
      "Loss: 2.2594\n",
      "Loss: 2.2479\n",
      "Loss: 2.2379\n",
      "Loss: 2.2243\n",
      "Loss: 2.2245\n",
      "Loss: 2.2057\n",
      "Loss: 2.2035\n",
      "Loss: 2.1891\n",
      "Loss: 2.1817\n",
      "Loss: 2.1703\n",
      "Loss: 2.1592\n",
      "Loss: 2.1548\n",
      "Loss: 2.1383\n",
      "Loss: 2.1321\n",
      "Loss: 2.1179\n",
      "Loss: 2.1138\n",
      "Loss: 2.1062\n",
      "Loss: 2.0934\n",
      "Loss: 2.0856\n",
      "Loss: 2.0758\n",
      "Loss: 2.0656\n",
      "Loss: 2.0532\n",
      "Loss: 2.0497\n",
      "Loss: 2.0410\n",
      "Loss: 2.0258\n",
      "Loss: 2.0161\n",
      "Loss: 2.0047\n",
      "Loss: 1.9940\n",
      "Loss: 1.9820\n",
      "Loss: 1.9737\n",
      "Loss: 1.9614\n",
      "Loss: 1.9486\n",
      "Loss: 1.9378\n",
      "Loss: 1.9281\n",
      "Loss: 1.9174\n",
      "Loss: 1.9047\n",
      "Loss: 1.8922\n",
      "Loss: 1.8798\n",
      "Loss: 1.8674\n",
      "Loss: 1.8574\n",
      "Loss: 1.8485\n",
      "Loss: 1.8301\n",
      "Loss: 1.8213\n",
      "Loss: 1.8091\n",
      "Loss: 1.8007\n",
      "Loss: 1.7850\n",
      "Loss: 1.7784\n",
      "Loss: 1.7606\n",
      "Loss: 1.7496\n",
      "Loss: 1.7320\n",
      "Loss: 1.7216\n",
      "Loss: 1.7067\n",
      "Loss: 1.6954\n",
      "Loss: 1.6781\n",
      "Loss: 1.6667\n",
      "Loss: 1.6551\n",
      "Loss: 1.6425\n",
      "Loss: 1.6272\n",
      "Loss: 1.6096\n",
      "Loss: 1.6030\n",
      "Loss: 1.5824\n",
      "Loss: 1.5724\n",
      "Loss: 1.5558\n",
      "Loss: 1.5399\n",
      "Loss: 1.5266\n",
      "Loss: 1.5109\n",
      "Loss: 1.4952\n",
      "Loss: 1.4829\n",
      "Loss: 1.4648\n",
      "Loss: 1.4496\n",
      "Loss: 1.4360\n",
      "Loss: 1.4154\n",
      "Loss: 1.4010\n",
      "Loss: 1.3958\n",
      "Loss: 1.3719\n",
      "Loss: 1.3562\n",
      "[0 - 7f4ce0190740]   38.933268 {3}{RequestManager}: [Finetuning] guid(1000001) completed_training_steps(100) processed_finetuning_tokens(3400) latency(38933176.0)\n"
     ]
    }
   ],
   "source": [
    "finetuning_request = ff.Request(\n",
    "    ff.RequestType.REQ_FINETUNING,\n",
    "    max_sequence_length=configs.max_sequence_length,\n",
    "    peft_model_id=llm.get_ff_peft_id(lora_finetuning_config),\n",
    "    dataset_filepath=os.path.join(os.getcwd(), configs.finetuning_dataset),\n",
    "    max_training_steps=configs.max_training_steps,\n",
    ")\n",
    "ft_res = llm.generate([finetuning_request])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = list(range(configs_dict[\"max_training_steps\"]))\n",
    "loss_values = ft_res[0].finetuning_losses\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss_values, marker='o', linestyle='-', color='b')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Loss Value vs. Number of Epochs')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save finetuned model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['python', '../../utils/upload_peft_model.py'] + f\"--peft-model-id {configs.finetuning_peft_model_id} --upload-peft-model-id {configs.finetuning_peft_model_id}-dolly\".split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop LLM Co-serving system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-22 06:46:20 - ###PEFT DEBUGGING### Background serving task completed.\n",
      "Background server stopped.\n"
     ]
    }
   ],
   "source": [
    "llm.stop_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference all over again with the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora-dolly configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora-dolly/config.json...\n",
      "Loading tokenizer...\n",
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora-dolly configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora-dolly/config.json...\n",
      "Loading tokenizer...\n",
      "[0 - 7ff1caf83280]    0.270628 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7ff1caf83280]    0.270673 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7ff1caf83280]    0.270699 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7ff1caf83280]    0.270744 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "[0 - 7ff1caf83280]    0.270753 {3}{Mapper}: Enabled Control Replication Optimizations.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "workSpaceSize (128 MB)\n",
      "Creating directory /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b (if it doesn't exist)...\n",
      "Saving meta-llama/Meta-Llama-3-8B configs to file /root/.cache/flexflow/configs/meta-llama/meta-llama-3-8b/config.json...\n",
      "Saving goliaro/llama-3-8b-lora-dolly configs to file /root/.cache/flexflow/configs/goliaro/llama-3-8b-lora-dolly/config.json...\n",
      "Loading tokenizer...\n",
      "Adding layer layers.0.mlp.down_proj.lora\n",
      "Adding layer layers.1.mlp.down_proj.lora\n",
      "Adding layer layers.2.mlp.down_proj.lora\n",
      "Adding layer layers.3.mlp.down_proj.lora\n",
      "Adding layer layers.4.mlp.down_proj.lora\n",
      "Adding layer layers.5.mlp.down_proj.lora\n",
      "Adding layer layers.6.mlp.down_proj.lora\n",
      "Adding layer layers.7.mlp.down_proj.lora\n",
      "Adding layer layers.8.mlp.down_proj.lora\n",
      "Adding layer layers.9.mlp.down_proj.lora\n",
      "Adding layer layers.10.mlp.down_proj.lora\n",
      "Adding layer layers.11.mlp.down_proj.lora\n",
      "Adding layer layers.12.mlp.down_proj.lora\n",
      "Adding layer layers.13.mlp.down_proj.lora\n",
      "Adding layer layers.14.mlp.down_proj.lora\n",
      "Adding layer layers.15.mlp.down_proj.lora\n",
      "Adding layer layers.16.mlp.down_proj.lora\n",
      "Adding layer layers.17.mlp.down_proj.lora\n",
      "Adding layer layers.18.mlp.down_proj.lora\n",
      "Adding layer layers.19.mlp.down_proj.lora\n",
      "Adding layer layers.20.mlp.down_proj.lora\n",
      "Adding layer layers.21.mlp.down_proj.lora\n",
      "Adding layer layers.22.mlp.down_proj.lora\n",
      "Adding layer layers.23.mlp.down_proj.lora\n",
      "Adding layer layers.24.mlp.down_proj.lora\n",
      "Adding layer layers.25.mlp.down_proj.lora\n",
      "Adding layer layers.26.mlp.down_proj.lora\n",
      "Adding layer layers.27.mlp.down_proj.lora\n",
      "Adding layer layers.28.mlp.down_proj.lora\n",
      "Adding layer layers.29.mlp.down_proj.lora\n",
      "Adding layer layers.30.mlp.down_proj.lora\n",
      "Adding layer layers.31.mlp.down_proj.lora\n",
      "Background server started.\n",
      "[<flexflow.core.flexflow_cffi.Request object at 0x7ff16b115bd0>]\n",
      "2024-07-22 06:42:43 - ###PEFT DEBUGGING### Starting background serving task.\n",
      "2024-07-22 06:42:43 - ###PEFT DEBUGGING### Updated models' configuration.\n",
      "###PEFT DEBUGGING### LLM Model object exists.\n",
      "###PEFT DEBUGGING### Model object exists.\n",
      "###PEFT DEBUGGING### Model object still exists.\n",
      "###PEFT DEBUGGING### Entering compile_inference.\n",
      "###PEFT DEBUGGING### Configuration check passed: At least four CPU cores per node.\n",
      "###PEFT DEBUGGING### Launching graph optimization task.\n",
      "num_nodes = 1 num_gpus_per_node = 1\n",
      "[0]10445\n",
      "[1]649\n",
      "[2]6730\n",
      "[3]2053\n",
      "[4]18167\n",
      "[5]369\n",
      "[6]1317\n",
      "[7]2085\n",
      "[8]3090\n",
      "[9]30\n",
      "No small speculative model registered, using incremental decoding.\n",
      "[0 - 7ff1caf83280]    1.100415 {3}{RequestManager}: [1000000]New request tokens: 128000 10445 649 6730 2053 18167 369 1317 2085 3090 30\n",
      "optimal_views.size = 262\n",
      "views.size() = 262\n",
      "###PEFT DEBUGGING### Operators reconstructed from optimized graph.\n",
      "###PEFT DEBUGGING### Starting inplace optimizations.\n",
      "###PEFT DEBUGGING### Mapping output tensors.\n",
      "ndim(1) dims[1 0 0 0]\n",
      "###PEFT DEBUGGING### Setting up NCCL communications.\n",
      "###PEFT DEBUGGING### compile_inference completed successfully.\n",
      "Loading weight file embed_tokens.weight\n",
      "Loading weight file layers.0.input_layernorm.weight\n",
      "Loading weight file layers.0.self_attn.q_proj.weight\n",
      "Loading weight file layers.0.self_attn.k_proj.weight\n",
      "Loading weight file layers.0.self_attn.v_proj.weight\n",
      "Loading weight file layers.0.self_attn.o_proj.weight\n",
      "Loading weight file layers.0.post_attention_layernorm.weight\n",
      "Loading weight file layers.0.mlp.gate_proj.weight\n",
      "Loading weight file layers.0.mlp.up_proj.weight\n",
      "Loading weight file layers.0.mlp.down_proj.weight\n",
      "Loading weight file layers.1.input_layernorm.weight\n",
      "Loading weight file layers.1.self_attn.q_proj.weight\n",
      "Loading weight file layers.1.self_attn.k_proj.weight\n",
      "Loading weight file layers.1.self_attn.v_proj.weight\n",
      "Loading weight file layers.1.self_attn.o_proj.weight\n",
      "Loading weight file layers.1.post_attention_layernorm.weight\n",
      "Loading weight file layers.1.mlp.gate_proj.weight\n",
      "Loading weight file layers.1.mlp.up_proj.weight\n",
      "Loading weight file layers.1.mlp.down_proj.weight\n",
      "Loading weight file layers.2.input_layernorm.weight\n",
      "Loading weight file layers.2.self_attn.q_proj.weight\n",
      "Loading weight file layers.2.self_attn.k_proj.weight\n",
      "Loading weight file layers.2.self_attn.v_proj.weight\n",
      "Loading weight file layers.2.self_attn.o_proj.weight\n",
      "Loading weight file layers.2.post_attention_layernorm.weight\n",
      "Loading weight file layers.2.mlp.gate_proj.weight\n",
      "Loading weight file layers.2.mlp.up_proj.weight\n",
      "Loading weight file layers.2.mlp.down_proj.weight\n",
      "Loading weight file layers.3.input_layernorm.weight\n",
      "Loading weight file layers.3.self_attn.q_proj.weight\n",
      "Loading weight file layers.3.self_attn.k_proj.weight\n",
      "Loading weight file layers.3.self_attn.v_proj.weight\n",
      "Loading weight file layers.3.self_attn.o_proj.weight\n",
      "Loading weight file layers.3.post_attention_layernorm.weight\n",
      "Loading weight file layers.3.mlp.gate_proj.weight\n",
      "Loading weight file layers.3.mlp.up_proj.weight\n",
      "Loading weight file layers.3.mlp.down_proj.weight\n",
      "Loading weight file layers.4.input_layernorm.weight\n",
      "Loading weight file layers.4.self_attn.q_proj.weight\n",
      "Loading weight file layers.4.self_attn.k_proj.weight\n",
      "Loading weight file layers.4.self_attn.v_proj.weight\n",
      "Loading weight file layers.4.self_attn.o_proj.weight\n",
      "Loading weight file layers.4.post_attention_layernorm.weight\n",
      "Loading weight file layers.4.mlp.gate_proj.weight\n",
      "Loading weight file layers.4.mlp.up_proj.weight\n",
      "Loading weight file layers.4.mlp.down_proj.weight\n",
      "Loading weight file layers.5.input_layernorm.weight\n",
      "Loading weight file layers.5.self_attn.q_proj.weight\n",
      "Loading weight file layers.5.self_attn.k_proj.weight\n",
      "Loading weight file layers.5.self_attn.v_proj.weight\n",
      "Loading weight file layers.5.self_attn.o_proj.weight\n",
      "Loading weight file layers.5.post_attention_layernorm.weight\n",
      "Loading weight file layers.5.mlp.gate_proj.weight\n",
      "Loading weight file layers.5.mlp.up_proj.weight\n",
      "Loading weight file layers.5.mlp.down_proj.weight\n",
      "Loading weight file layers.6.input_layernorm.weight\n",
      "Loading weight file layers.6.self_attn.q_proj.weight\n",
      "Loading weight file layers.6.self_attn.k_proj.weight\n",
      "Loading weight file layers.6.self_attn.v_proj.weight\n",
      "Loading weight file layers.6.self_attn.o_proj.weight\n",
      "Loading weight file layers.6.post_attention_layernorm.weight\n",
      "Loading weight file layers.6.mlp.gate_proj.weight\n",
      "Loading weight file layers.6.mlp.up_proj.weight\n",
      "Loading weight file layers.6.mlp.down_proj.weight\n",
      "Loading weight file layers.7.input_layernorm.weight\n",
      "Loading weight file layers.7.self_attn.q_proj.weight\n",
      "Loading weight file layers.7.self_attn.k_proj.weight\n",
      "Loading weight file layers.7.self_attn.v_proj.weight\n",
      "Loading weight file layers.7.self_attn.o_proj.weight\n",
      "Loading weight file layers.7.post_attention_layernorm.weight\n",
      "Loading weight file layers.7.mlp.gate_proj.weight\n",
      "Loading weight file layers.7.mlp.up_proj.weight\n",
      "Loading weight file layers.7.mlp.down_proj.weight\n",
      "Loading weight file layers.8.input_layernorm.weight\n",
      "Loading weight file layers.8.self_attn.q_proj.weight\n",
      "Loading weight file layers.8.self_attn.k_proj.weight\n",
      "Loading weight file layers.8.self_attn.v_proj.weight\n",
      "Loading weight file layers.8.self_attn.o_proj.weight\n",
      "Loading weight file layers.8.post_attention_layernorm.weight\n",
      "Loading weight file layers.8.mlp.gate_proj.weight\n",
      "Loading weight file layers.8.mlp.up_proj.weight\n",
      "Loading weight file layers.8.mlp.down_proj.weight\n",
      "Loading weight file layers.9.input_layernorm.weight\n",
      "Loading weight file layers.9.self_attn.q_proj.weight\n",
      "Loading weight file layers.9.self_attn.k_proj.weight\n",
      "Loading weight file layers.9.self_attn.v_proj.weight\n",
      "Loading weight file layers.9.self_attn.o_proj.weight\n",
      "Loading weight file layers.9.post_attention_layernorm.weight\n",
      "Loading weight file layers.9.mlp.gate_proj.weight\n",
      "Loading weight file layers.9.mlp.up_proj.weight\n",
      "Loading weight file layers.9.mlp.down_proj.weight\n",
      "Loading weight file layers.10.input_layernorm.weight\n",
      "Loading weight file layers.10.self_attn.q_proj.weight\n",
      "Loading weight file layers.10.self_attn.k_proj.weight\n",
      "Loading weight file layers.10.self_attn.v_proj.weight\n",
      "Loading weight file layers.10.self_attn.o_proj.weight\n",
      "Loading weight file layers.10.post_attention_layernorm.weight\n",
      "Loading weight file layers.10.mlp.gate_proj.weight\n",
      "Loading weight file layers.10.mlp.up_proj.weight\n",
      "Loading weight file layers.10.mlp.down_proj.weight\n",
      "Loading weight file layers.11.input_layernorm.weight\n",
      "Loading weight file layers.11.self_attn.q_proj.weight\n",
      "Loading weight file layers.11.self_attn.k_proj.weight\n",
      "Loading weight file layers.11.self_attn.v_proj.weight\n",
      "Loading weight file layers.11.self_attn.o_proj.weight\n",
      "Loading weight file layers.11.post_attention_layernorm.weight\n",
      "Loading weight file layers.11.mlp.gate_proj.weight\n",
      "Loading weight file layers.11.mlp.up_proj.weight\n",
      "Loading weight file layers.11.mlp.down_proj.weight\n",
      "Loading weight file layers.12.input_layernorm.weight\n",
      "Loading weight file layers.12.self_attn.q_proj.weight\n",
      "Loading weight file layers.12.self_attn.k_proj.weight\n",
      "Loading weight file layers.12.self_attn.v_proj.weight\n",
      "Loading weight file layers.12.self_attn.o_proj.weight\n",
      "Loading weight file layers.12.post_attention_layernorm.weight\n",
      "Loading weight file layers.12.mlp.gate_proj.weight\n",
      "Loading weight file layers.12.mlp.up_proj.weight\n",
      "Loading weight file layers.12.mlp.down_proj.weight\n",
      "Loading weight file layers.13.input_layernorm.weight\n",
      "Loading weight file layers.13.self_attn.q_proj.weight\n",
      "Loading weight file layers.13.self_attn.k_proj.weight\n",
      "Loading weight file layers.13.self_attn.v_proj.weight\n",
      "Loading weight file layers.13.self_attn.o_proj.weight\n",
      "Loading weight file layers.13.post_attention_layernorm.weight\n",
      "Loading weight file layers.13.mlp.gate_proj.weight\n",
      "Loading weight file layers.13.mlp.up_proj.weight\n",
      "Loading weight file layers.13.mlp.down_proj.weight\n",
      "Loading weight file layers.14.input_layernorm.weight\n",
      "Loading weight file layers.14.self_attn.q_proj.weight\n",
      "Loading weight file layers.14.self_attn.k_proj.weight\n",
      "Loading weight file layers.14.self_attn.v_proj.weight\n",
      "Loading weight file layers.14.self_attn.o_proj.weight\n",
      "Loading weight file layers.14.post_attention_layernorm.weight\n",
      "Loading weight file layers.14.mlp.gate_proj.weight\n",
      "Loading weight file layers.14.mlp.up_proj.weight\n",
      "Loading weight file layers.14.mlp.down_proj.weight\n",
      "Loading weight file layers.15.input_layernorm.weight\n",
      "Loading weight file layers.15.self_attn.q_proj.weight\n",
      "Loading weight file layers.15.self_attn.k_proj.weight\n",
      "Loading weight file layers.15.self_attn.v_proj.weight\n",
      "Loading weight file layers.15.self_attn.o_proj.weight\n",
      "Loading weight file layers.15.post_attention_layernorm.weight\n",
      "Loading weight file layers.15.mlp.gate_proj.weight\n",
      "Loading weight file layers.15.mlp.up_proj.weight\n",
      "Loading weight file layers.15.mlp.down_proj.weight\n",
      "Loading weight file layers.16.input_layernorm.weight\n",
      "Loading weight file layers.16.self_attn.q_proj.weight\n",
      "Loading weight file layers.16.self_attn.k_proj.weight\n",
      "Loading weight file layers.16.self_attn.v_proj.weight\n",
      "Loading weight file layers.16.self_attn.o_proj.weight\n",
      "Loading weight file layers.16.post_attention_layernorm.weight\n",
      "Loading weight file layers.16.mlp.gate_proj.weight\n",
      "Loading weight file layers.16.mlp.up_proj.weight\n",
      "Loading weight file layers.16.mlp.down_proj.weight\n",
      "Loading weight file layers.17.input_layernorm.weight\n",
      "Loading weight file layers.17.self_attn.q_proj.weight\n",
      "Loading weight file layers.17.self_attn.k_proj.weight\n",
      "Loading weight file layers.17.self_attn.v_proj.weight\n",
      "Loading weight file layers.17.self_attn.o_proj.weight\n",
      "Loading weight file layers.17.post_attention_layernorm.weight\n",
      "Loading weight file layers.17.mlp.gate_proj.weight\n",
      "Loading weight file layers.17.mlp.up_proj.weight\n",
      "Loading weight file layers.17.mlp.down_proj.weight\n",
      "Loading weight file layers.18.input_layernorm.weight\n",
      "Loading weight file layers.18.self_attn.q_proj.weight\n",
      "Loading weight file layers.18.self_attn.k_proj.weight\n",
      "Loading weight file layers.18.self_attn.v_proj.weight\n",
      "Loading weight file layers.18.self_attn.o_proj.weight\n",
      "Loading weight file layers.18.post_attention_layernorm.weight\n",
      "Loading weight file layers.18.mlp.gate_proj.weight\n",
      "Loading weight file layers.18.mlp.up_proj.weight\n",
      "Loading weight file layers.18.mlp.down_proj.weight\n",
      "Loading weight file layers.19.input_layernorm.weight\n",
      "Loading weight file layers.19.self_attn.q_proj.weight\n",
      "Loading weight file layers.19.self_attn.k_proj.weight\n",
      "Loading weight file layers.19.self_attn.v_proj.weight\n",
      "Loading weight file layers.19.self_attn.o_proj.weight\n",
      "Loading weight file layers.19.post_attention_layernorm.weight\n",
      "Loading weight file layers.19.mlp.gate_proj.weight\n",
      "Loading weight file layers.19.mlp.up_proj.weight\n",
      "Loading weight file layers.19.mlp.down_proj.weight\n",
      "Loading weight file layers.20.input_layernorm.weight\n",
      "Loading weight file layers.20.self_attn.q_proj.weight\n",
      "Loading weight file layers.20.self_attn.k_proj.weight\n",
      "Loading weight file layers.20.self_attn.v_proj.weight\n",
      "Loading weight file layers.20.self_attn.o_proj.weight\n",
      "Loading weight file layers.20.post_attention_layernorm.weight\n",
      "Loading weight file layers.20.mlp.gate_proj.weight\n",
      "Loading weight file layers.20.mlp.up_proj.weight\n",
      "Loading weight file layers.20.mlp.down_proj.weight\n",
      "Loading weight file layers.21.input_layernorm.weight\n",
      "Loading weight file layers.21.self_attn.q_proj.weight\n",
      "Loading weight file layers.21.self_attn.k_proj.weight\n",
      "Loading weight file layers.21.self_attn.v_proj.weight\n",
      "Loading weight file layers.21.self_attn.o_proj.weight\n",
      "Loading weight file layers.21.post_attention_layernorm.weight\n",
      "Loading weight file layers.21.mlp.gate_proj.weight\n",
      "Loading weight file layers.21.mlp.up_proj.weight\n",
      "Loading weight file layers.21.mlp.down_proj.weight\n",
      "Loading weight file layers.22.input_layernorm.weight\n",
      "Loading weight file layers.22.self_attn.q_proj.weight\n",
      "Loading weight file layers.22.self_attn.k_proj.weight\n",
      "Loading weight file layers.22.self_attn.v_proj.weight\n",
      "Loading weight file layers.22.self_attn.o_proj.weight\n",
      "Loading weight file layers.22.post_attention_layernorm.weight\n",
      "Loading weight file layers.22.mlp.gate_proj.weight\n",
      "Loading weight file layers.22.mlp.up_proj.weight\n",
      "Loading weight file layers.22.mlp.down_proj.weight\n",
      "Loading weight file layers.23.input_layernorm.weight\n",
      "Loading weight file layers.23.self_attn.q_proj.weight\n",
      "Loading weight file layers.23.self_attn.k_proj.weight\n",
      "Loading weight file layers.23.self_attn.v_proj.weight\n",
      "Loading weight file layers.23.self_attn.o_proj.weight\n",
      "Loading weight file layers.23.post_attention_layernorm.weight\n",
      "Loading weight file layers.23.mlp.gate_proj.weight\n",
      "Loading weight file layers.23.mlp.up_proj.weight\n",
      "Loading weight file layers.23.mlp.down_proj.weight\n",
      "Loading weight file layers.24.input_layernorm.weight\n",
      "Loading weight file layers.24.self_attn.q_proj.weight\n",
      "Loading weight file layers.24.self_attn.k_proj.weight\n",
      "Loading weight file layers.24.self_attn.v_proj.weight\n",
      "Loading weight file layers.24.self_attn.o_proj.weight\n",
      "Loading weight file layers.24.post_attention_layernorm.weight\n",
      "Loading weight file layers.24.mlp.gate_proj.weight\n",
      "Loading weight file layers.24.mlp.up_proj.weight\n",
      "Loading weight file layers.24.mlp.down_proj.weight\n",
      "Loading weight file layers.25.input_layernorm.weight\n",
      "Loading weight file layers.25.self_attn.q_proj.weight\n",
      "Loading weight file layers.25.self_attn.k_proj.weight\n",
      "Loading weight file layers.25.self_attn.v_proj.weight\n",
      "Loading weight file layers.25.self_attn.o_proj.weight\n",
      "Loading weight file layers.25.post_attention_layernorm.weight\n",
      "Loading weight file layers.25.mlp.gate_proj.weight\n",
      "Loading weight file layers.25.mlp.up_proj.weight\n",
      "Loading weight file layers.25.mlp.down_proj.weight\n",
      "Loading weight file layers.26.input_layernorm.weight\n",
      "Loading weight file layers.26.self_attn.q_proj.weight\n",
      "Loading weight file layers.26.self_attn.k_proj.weight\n",
      "Loading weight file layers.26.self_attn.v_proj.weight\n",
      "Loading weight file layers.26.self_attn.o_proj.weight\n",
      "Loading weight file layers.26.post_attention_layernorm.weight\n",
      "Loading weight file layers.26.mlp.gate_proj.weight\n",
      "Loading weight file layers.26.mlp.up_proj.weight\n",
      "Loading weight file layers.26.mlp.down_proj.weight\n",
      "Loading weight file layers.27.input_layernorm.weight\n",
      "Loading weight file layers.27.self_attn.q_proj.weight\n",
      "Loading weight file layers.27.self_attn.k_proj.weight\n",
      "Loading weight file layers.27.self_attn.v_proj.weight\n",
      "Loading weight file layers.27.self_attn.o_proj.weight\n",
      "Loading weight file layers.27.post_attention_layernorm.weight\n",
      "Loading weight file layers.27.mlp.gate_proj.weight\n",
      "Loading weight file layers.27.mlp.up_proj.weight\n",
      "Loading weight file layers.27.mlp.down_proj.weight\n",
      "Loading weight file layers.28.input_layernorm.weight\n",
      "Loading weight file layers.28.self_attn.q_proj.weight\n",
      "Loading weight file layers.28.self_attn.k_proj.weight\n",
      "Loading weight file layers.28.self_attn.v_proj.weight\n",
      "Loading weight file layers.28.self_attn.o_proj.weight\n",
      "Loading weight file layers.28.post_attention_layernorm.weight\n",
      "Loading weight file layers.28.mlp.gate_proj.weight\n",
      "Loading weight file layers.28.mlp.up_proj.weight\n",
      "Loading weight file layers.28.mlp.down_proj.weight\n",
      "Loading weight file layers.29.input_layernorm.weight\n",
      "Loading weight file layers.29.self_attn.q_proj.weight\n",
      "Loading weight file layers.29.self_attn.k_proj.weight\n",
      "Loading weight file layers.29.self_attn.v_proj.weight\n",
      "Loading weight file layers.29.self_attn.o_proj.weight\n",
      "Loading weight file layers.29.post_attention_layernorm.weight\n",
      "Loading weight file layers.29.mlp.gate_proj.weight\n",
      "Loading weight file layers.29.mlp.up_proj.weight\n",
      "Loading weight file layers.29.mlp.down_proj.weight\n",
      "Loading weight file layers.30.input_layernorm.weight\n",
      "Loading weight file layers.30.self_attn.q_proj.weight\n",
      "Loading weight file layers.30.self_attn.k_proj.weight\n",
      "Loading weight file layers.30.self_attn.v_proj.weight\n",
      "Loading weight file layers.30.self_attn.o_proj.weight\n",
      "Loading weight file layers.30.post_attention_layernorm.weight\n",
      "Loading weight file layers.30.mlp.gate_proj.weight\n",
      "Loading weight file layers.30.mlp.up_proj.weight\n",
      "Loading weight file layers.30.mlp.down_proj.weight\n",
      "Loading weight file layers.31.input_layernorm.weight\n",
      "Loading weight file layers.31.self_attn.q_proj.weight\n",
      "Loading weight file layers.31.self_attn.k_proj.weight\n",
      "Loading weight file layers.31.self_attn.v_proj.weight\n",
      "Loading weight file layers.31.self_attn.o_proj.weight\n",
      "Loading weight file layers.31.post_attention_layernorm.weight\n",
      "Loading weight file layers.31.mlp.gate_proj.weight\n",
      "Loading weight file layers.31.mlp.up_proj.weight\n",
      "Loading weight file layers.31.mlp.down_proj.weight\n",
      "Loading weight file norm.weight\n",
      "Loading weight file lm_head.weight\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.0.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.1.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.2.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.3.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.4.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.5.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.6.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.7.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.8.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.9.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.10.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.11.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.12.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.13.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.14.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.15.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.16.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.17.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.18.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.19.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.20.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.21.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.22.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.23.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.24.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.25.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.26.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.27.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.28.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.29.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.30.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_A.weight, num_rows: 14336, num_cols: 16, num_shards: 1, shard_id: 0\n",
      "Loading LORA weight layers.31.mlp.down_proj.lora_B.weight, num_rows: 16, num_cols: 4096, num_shards: 1, shard_id: 0\n",
      "[0 - 7ff1680b6740]   16.224181 {3}{RequestManager}: Output token is: 3639\n",
      "[0 - 7ff1680b6740]   16.321885 {3}{RequestManager}: Output token is: 374\n",
      "[0 - 7ff168092740]   16.407712 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7ff1680b6740]   16.492788 {3}{RequestManager}: Output token is: 2944\n",
      "[0 - 7ff168092740]   16.563500 {3}{RequestManager}: Output token is: 4920\n",
      "[0 - 7ff168092740]   16.624616 {3}{RequestManager}: Output token is: 279\n",
      "[0 - 7ff168092740]   16.675778 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   16.725625 {3}{RequestManager}: Output token is: 13272\n",
      "[0 - 7ff168092740]   16.776205 {3}{RequestManager}: Output token is: 315\n",
      "[0 - 7ff168092740]   16.827883 {3}{RequestManager}: Output token is: 41389\n",
      "[0 - 7ff168092740]   16.878348 {3}{RequestManager}: Output token is: 2715\n",
      "[0 - 7ff168092740]   16.929025 {3}{RequestManager}: Output token is: 288\n",
      "[0 - 7ff168092740]   16.979287 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff1680b6740]   17.029879 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff1680b6740]   17.078696 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   17.127942 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff1680b6740]   17.177796 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff1680b6740]   17.227023 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff1680b6740]   17.277136 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff1680b6740]   17.328143 {3}{RequestManager}: Output token is: 64614\n",
      "[0 - 7ff1680b6740]   17.378508 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   17.430618 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   17.482129 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff168092740]   17.533479 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   17.584503 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   17.634591 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   17.685727 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   17.736768 {3}{RequestManager}: Output token is: 14535\n",
      "[0 - 7ff168092740]   17.785909 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   17.836515 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   17.886526 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   17.936502 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   17.986222 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   18.037888 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   18.088468 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   18.138261 {3}{RequestManager}: Output token is: 25212\n",
      "[0 - 7ff168092740]   18.187102 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   18.237270 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   18.289979 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff168092740]   18.340895 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   18.391145 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   18.441155 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   18.499716 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff1680b6740]   18.552423 {3}{RequestManager}: Output token is: 97814\n",
      "[0 - 7ff168092740]   18.603261 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   18.654986 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   18.706227 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff168092740]   18.756543 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   18.807690 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff1680b6740]   18.857508 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   18.907649 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   18.958208 {3}{RequestManager}: Output token is: 41759\n",
      "[0 - 7ff168092740]   19.009971 {3}{RequestManager}: Output token is: 388\n",
      "[0 - 7ff168092740]   19.060626 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   19.112370 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   19.161425 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff168092740]   19.206435 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   19.254004 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   19.306102 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   19.356853 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   19.408861 {3}{RequestManager}: Output token is: 89435\n",
      "[0 - 7ff1680b6740]   19.460391 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff1680b6740]   19.511207 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff1680b6740]   19.565692 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   19.617057 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff1680b6740]   19.669739 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff1680b6740]   19.722325 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff1680b6740]   19.773583 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff1680b6740]   19.824646 {3}{RequestManager}: Output token is: 68550\n",
      "[0 - 7ff1680b6740]   19.876650 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff1680b6740]   19.926939 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff1680b6740]   19.977325 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   20.028247 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff1680b6740]   20.078419 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   20.128614 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   20.179748 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   20.230542 {3}{RequestManager}: Output token is: 18311\n",
      "[0 - 7ff1680b6740]   20.281634 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   20.330089 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   20.375491 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   20.422220 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   20.475078 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   20.526058 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   20.577651 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   20.628505 {3}{RequestManager}: Output token is: 7013\n",
      "[0 - 7ff168092740]   20.681354 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   20.734160 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   20.786299 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   20.837268 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   20.888265 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   20.939708 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   20.990707 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   21.041260 {3}{RequestManager}: Output token is: 18742\n",
      "[0 - 7ff1680b6740]   21.091386 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   21.145432 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff168092740]   21.197149 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff168092740]   21.249242 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   21.301514 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   21.352632 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   21.404018 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   21.455101 {3}{RequestManager}: Output token is: 56994\n",
      "[0 - 7ff1680b6740]   21.506371 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   21.559369 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff1680b6740]   21.611370 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff168092740]   21.663655 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff1680b6740]   21.715270 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff168092740]   21.766481 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff168092740]   21.818563 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff168092740]   21.872108 {3}{RequestManager}: Output token is: 29505\n",
      "[0 - 7ff168092740]   21.922670 {3}{RequestManager}: Output token is: 30\n",
      "[0 - 7ff168092740]   21.973973 {3}{RequestManager}: Output token is: 8595\n",
      "[0 - 7ff1680b6740]   22.024297 {3}{RequestManager}: Output token is: 656\n",
      "[0 - 7ff1680b6740]   22.076266 {3}{RequestManager}: Output token is: 1063\n",
      "[0 - 7ff168092740]   22.127594 {3}{RequestManager}: Output token is: 10099\n",
      "[0 - 7ff1680b6740]   22.179008 {3}{RequestManager}: Output token is: 617\n",
      "[0 - 7ff1680b6740]   22.230414 {3}{RequestManager}: Output token is: 1317\n",
      "[0 - 7ff1680b6740]   22.281805 {3}{RequestManager}: Output token is: 993\n",
      "[0 - 7ff1680b6740]   22.282235 {3}{RequestManager}: [Done] guid(1000000) final_length(128)\n",
      "[0 - 7ff1680b6740]   22.282243 {3}{RequestManager}: Final output: <s> <|begin_of_text|>Why can camels survive for long without water? What is the reason behind the long neck of giraffes? Why do some animals have long tails? Why do some animals have long legs? Why do some animals have long ears? Why do some animals have long noses? Why do some animals have long whiskers? Why do some animals have long tongues? Why do some animals have long claws? Why do some animals have long teeth? Why do some animals have long hair? Why do some animals have long fur? Why do some animals have long feathers? Why do some animals have long scales? Why do some animals have long sp\n",
      "[0 - 7ff1680b6740]   22.282250 {3}{RequestManager}: [Profile] guid(1000000) llm_decoding_steps(117) start(15892528.0) finish(22282245.0) latency(6389717.0) ttft(15123707.0)\n",
      "2024-07-22 06:43:05 - ###PEFT DEBUGGING### Background serving task completed.\n",
      "Background server stopped.\n"
     ]
    }
   ],
   "source": [
    "import json, random, subprocess, os\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from huggingface_hub import HfFolder\n",
    "import flexflow.serve as ff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "configs_dict = {\n",
    "    \"num_gpus\": 1,\n",
    "    \"memory_per_gpu\": 21000,\n",
    "    \"zero_copy_memory_per_node\": 40000,\n",
    "    \"num_cpus\": 4,\n",
    "    \"legion_utility_processors\": 4,\n",
    "    \"data_parallelism_degree\": 1,\n",
    "    \"tensor_parallelism_degree\": 1,\n",
    "    \"pipeline_parallelism_degree\": 1,\n",
    "    \"offload\": False,\n",
    "    \"offload_reserve_space_size\": 8 * 1024,  # 8GB\n",
    "    \"use_4bit_quantization\": False,\n",
    "    \"use_8bit_quantization\": False,\n",
    "    \"enable_peft\": True,\n",
    "    \"peft_activation_reserve_space_size\": 1024,  # 1GB\n",
    "    \"peft_weight_reserve_space_size\": 1024,  # 1GB\n",
    "    \"profiling\": False,\n",
    "    \"inference_debugging\": False,\n",
    "    \"fusion\": False,\n",
    "    \"max_requests_per_batch\": 1,\n",
    "    \"max_sequence_length\": 128,\n",
    "    \"max_tokens_per_batch\": 128,\n",
    "    \"max_training_steps\": 100,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "model_configs = {\n",
    "    \"base_model\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"inference_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"finetuning_peft_model_id\": \"goliaro/llama-3-8b-lora\",\n",
    "    \"cache_path\": os.environ.get(\"FF_CACHE_PATH\", \"\"),\n",
    "    \"refresh_cache\": False,\n",
    "    \"full_precision\": False,\n",
    "    # relative paths\n",
    "    \"inference_dataset\": \"inference_dataset.json\",\n",
    "    \"finetuning_dataset\": \"/usr/FlexFlow/inference/prompt/peft_dataset.json\",\n",
    "    \"output_file\": \"peft_demo.txt\",\n",
    "}\n",
    "generation_configs = {\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.9,\n",
    "    \"topp\": 0.8,\n",
    "    \"topk\": 1,\n",
    "}\n",
    "finetuning_configs = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"nesterov\": False,\n",
    "}\n",
    "# Merge dictionaries\n",
    "configs_dict.update(model_configs)\n",
    "configs_dict.update(generation_configs)\n",
    "configs_dict.update(finetuning_configs)\n",
    "\n",
    "configs = SimpleNamespace(**configs_dict)\n",
    "\n",
    "\n",
    "args = [configs.finetuning_peft_model_id+\"-dolly\", '--base_model_name', configs.base_model]\n",
    "subprocess.run(['python', '../../utils/download_peft_model.py'] + args)\n",
    "\n",
    "# Initialize the FlexFlow runtime. ff.init() takes a dictionary or the path to a JSON file with the configs\n",
    "ff.init(configs_dict)\n",
    "\n",
    "# Create the FlexFlow LLM\n",
    "ff_data_type = (\n",
    "    ff.DataType.DT_FLOAT if configs.full_precision else ff.DataType.DT_HALF\n",
    ")\n",
    "llm = ff.LLM(\n",
    "    configs.base_model,\n",
    "    data_type=ff_data_type,\n",
    "    cache_path=configs.cache_path,\n",
    "    refresh_cache=configs.refresh_cache,\n",
    "    output_file=configs.output_file,\n",
    ")\n",
    "\n",
    "lora_inference_config2 = ff.LoraLinearConfig(\n",
    "    llm.cache_path, \n",
    "    configs.finetuning_peft_model_id+\"-dolly\",\n",
    "    base_model_name_or_path=configs.base_model\n",
    ")\n",
    "llm.add_peft(lora_inference_config2)\n",
    "\n",
    "\n",
    "# Compile the LLM for inference and load the weights into memory\n",
    "generation_config = ff.GenerationConfig(\n",
    "    do_sample=configs.do_sample,\n",
    "    temperature=configs.temperature,\n",
    "    topp=configs.topp,\n",
    "    topk=configs.topk\n",
    ")\n",
    "llm.compile(\n",
    "    generation_config,\n",
    "    max_requests_per_batch=configs.max_requests_per_batch,\n",
    "    max_seq_length=configs.max_sequence_length,\n",
    "    max_tokens_per_batch=configs.max_tokens_per_batch,\n",
    ")\n",
    "\n",
    "llm.start_server()\n",
    "\n",
    "prompts = [s for s in json.load(open(configs.inference_dataset))]\n",
    "inference_requests = [\n",
    "    ff.Request(\n",
    "        ff.RequestType.REQ_INFERENCE,\n",
    "        prompt=prompt,\n",
    "        max_sequence_length=configs.max_sequence_length,\n",
    "        peft_model_id=llm.get_ff_peft_id(lora_inference_config2),\n",
    "    )\n",
    "    for prompt in prompts\n",
    "]\n",
    "inf_req_res_2 = llm.generate(inference_requests)\n",
    "\n",
    "llm.stop_server()\n",
    "\n",
    "with open(\"after_finetuning.txt\", \"w\") as file:\n",
    "    file.write(str(inf_req_res_2[0].output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
